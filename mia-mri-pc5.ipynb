{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1299795,"sourceType":"datasetVersion","datasetId":751906},{"sourceId":9939279,"sourceType":"datasetVersion","datasetId":6110747},{"sourceType":"datasetVersion","sourceId":10013317,"datasetId":6164842}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, LayerNormalization, Conv3D, UpSampling3D, Concatenate, Add\nimport numpy as np\nimport nibabel as nib\nimport glob\nimport matplotlib.pyplot as plt\nfrom tifffile import imsave\nfrom tqdm import tqdm\nimport os\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom skimage.transform import resize\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T01:21:36.426819Z","iopub.execute_input":"2024-11-26T01:21:36.427793Z","iopub.status.idle":"2024-11-26T01:21:36.434042Z","shell.execute_reply.started":"2024-11-26T01:21:36.427753Z","shell.execute_reply":"2024-11-26T01:21:36.432772Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Load the NIfTI file\nfile_path = \"/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_355/W39_1998.09.19_Segm.nii\"  # Replace with your file path\nnii_image = nib.load(file_path)\n\n# Get the data as a NumPy array\nimage_data = nii_image.get_fdata()\n\n# Check the shape of the image\nprint(\"Image shape:\", image_data.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T01:21:36.436131Z","iopub.execute_input":"2024-11-26T01:21:36.437149Z","iopub.status.idle":"2024-11-26T01:21:36.610189Z","shell.execute_reply.started":"2024-11-26T01:21:36.437100Z","shell.execute_reply":"2024-11-26T01:21:36.609054Z"}},"outputs":[{"name":"stdout","text":"Image shape: (240, 240, 155)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"seg_list = sorted(glob.glob('/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/*/*seg.nii'))\nt1_list = sorted(glob.glob('/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/*/*t1.nii'))\nt2_list = sorted(glob.glob('/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/*/*t2.nii'))\nt1ce_list = sorted(glob.glob('/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/*/*t1ce.nii'))\nflair_list = sorted(glob.glob('/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/*/*flair.nii'))\nseg_list.insert(354,\"/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_355/W39_1998.09.19_Segm.nii\")\n#seg_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T01:21:36.612095Z","iopub.execute_input":"2024-11-26T01:21:36.612458Z","iopub.status.idle":"2024-11-26T01:21:39.733728Z","shell.execute_reply.started":"2024-11-26T01:21:36.612424Z","shell.execute_reply":"2024-11-26T01:21:39.732647Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"target_size = (128, 128, 64)  # Desired output dimensions\n\ndef resize_nii_image(nii_img, target_size, is_mask=False):\n    \"\"\"\n    Resizes a NIfTI image to the target size, updating the affine matrix.\n\n    Parameters:\n    - nii_img: nib.Nifti1Image, input NIfTI image.\n    - target_size: tuple, desired output shape (x, y, z).\n    - is_mask: bool, whether the input is a binary mask.\n\n    Returns:\n    - nib.Nifti1Image, resized NIfTI image.\n    \"\"\"\n    image_data = nii_img.get_fdata(dtype=np.float16)\n    original_shape = image_data.shape\n    affine = nii_img.affine\n\n    # Compute scaling factors\n    scaling_factors = np.array(target_size) / np.array(original_shape)\n\n    # Resize the image\n    resized_image = resize(\n        image_data,\n        target_size,\n        mode='constant',\n        anti_aliasing=not is_mask,  # Avoid anti-aliasing for masks\n        preserve_range=True,  # Preserve intensity range\n    )\n\n    # Update the affine matrix\n    new_affine = affine.copy()\n    new_affine[:3, :3] = affine[:3, :3] * scaling_factors.reshape(3, 1)\n\n    # Create a new NIfTI image\n    resized_nii_img = nib.Nifti1Image(resized_image, new_affine)\n    del image_data, original_shape, affine, resized_image, new_affine, scaling_factors\n    gc.collect()\n\n    return resized_nii_img\n\ndef preprocess_and_resize(image_path_list, mask_path_list, target_size):\n    images = []\n    masks = []\n\n    for img_path, mask_path in zip(image_path_list, mask_path_list):\n        # Load image and mask\n        img_t1 = nib.load(img_path[0])  # T1 image path\n        img_t2 = nib.load(img_path[1])  # T2 image path\n        img_t1ce = nib.load(img_path[2])  # T1CE image path\n        img_flair = nib.load(img_path[3])  # FLAIR image path\n        mask = nib.load(mask_path)  # Mask path\n\n        # Resize images\n        # Ensure get_fdata() returns float32 directly\n        img_t1_resized = resize_nii_image(img_t1, target_size, is_mask=False).get_fdata(dtype=np.float16)\n        img_t2_resized = resize_nii_image(img_t2, target_size, is_mask=False).get_fdata(dtype=np.float16)\n        img_t1ce_resized = resize_nii_image(img_t1ce, target_size, is_mask=False).get_fdata(dtype=np.float16)\n        img_flair_resized = resize_nii_image(img_flair, target_size, is_mask=False).get_fdata(dtype=np.float16)\n\n        mask_resized = resize_nii_image(mask, target_size, is_mask=True).get_fdata()\n        mask_resized = mask_resized.astype(np.uint8)\n        mask_resized[mask_resized==4]=3\n\n        # Normalize images to range [0, 1]\n        img_t1_resized = (img_t1_resized - img_t1_resized.min()) / (img_t1_resized.max() - img_t1_resized.min())\n        img_t2_resized = (img_t2_resized - img_t2_resized.min()) / (img_t2_resized.max() - img_t2_resized.min())\n        img_t1ce_resized = (img_t1ce_resized - img_t1ce_resized.min()) / (img_t1ce_resized.max() - img_t1ce_resized.min())\n        img_flair_resized = (img_flair_resized - img_flair_resized.min()) / (img_flair_resized.max() - img_flair_resized.min())\n\n        # Convert mask to categorical\n        mask_resized = tf.keras.utils.to_categorical(mask_resized, num_classes=4)\n\n        # Stack modalities into a single 4D array\n        image = np.stack([img_t1_resized, img_t2_resized, img_t1ce_resized, img_flair_resized], axis=-1)\n        \n        # Append to the dataset lists\n        images.append(image)\n        masks.append(mask_resized)\n        del img_t1, img_t2, img_t1ce, img_flair, mask, img_t1_resized, img_t2_resized, img_t1ce_resized, img_flair_resized, mask_resized, image\n        gc.collect()\n\n    return np.array(images), np.array(masks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T01:21:39.735190Z","iopub.execute_input":"2024-11-26T01:21:39.735507Z","iopub.status.idle":"2024-11-26T01:21:39.749665Z","shell.execute_reply.started":"2024-11-26T01:21:39.735475Z","shell.execute_reply":"2024-11-26T01:21:39.748635Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def preprocess_and_resize(image_path_list, mask_path_list, target_size):\n    \"\"\"\n    Process and resize images one at a time, saving them to disk.\n    \"\"\"\n    os.makedirs(\"/kaggle/working/data\", exist_ok=True)\n    for idx, (img_paths, mask_path) in enumerate(zip(image_path_list, mask_path_list)):\n        # Load image and mask\n        img_t1 = nib.load(img_paths[0]).get_fdata()\n        img_t2 = nib.load(img_paths[1]).get_fdata()\n        img_t1ce = nib.load(img_paths[2]).get_fdata()\n        img_flair = nib.load(img_paths[3]).get_fdata()\n        mask = nib.load(mask_path).get_fdata()\n\n        # Resize images\n        img_t1_resized = resize(img_t1, target_size, mode='constant', preserve_range=True)\n        img_t2_resized = resize(img_t2, target_size, mode='constant', preserve_range=True)\n        img_t1ce_resized = resize(img_t1ce, target_size, mode='constant', preserve_range=True)\n        img_flair_resized = resize(img_flair, target_size, mode='constant', preserve_range=True)\n        mask_resized = resize(mask, target_size, mode='constant', preserve_range=True).astype(np.uint8)\n        mask_resized[mask_resized == 4] = 3\n        print(mask_resized.dtype)\n\n        # Normalize images\n        img_t1_resized = (img_t1_resized - img_t1_resized.min()) / (img_t1_resized.max() - img_t1_resized.min())\n        img_t2_resized = (img_t2_resized - img_t2_resized.min()) / (img_t2_resized.max() - img_t2_resized.min())\n        img_t1ce_resized = (img_t1ce_resized - img_t1ce_resized.min()) / (img_t1ce_resized.max() - img_t1ce_resized.min())\n        img_flair_resized = (img_flair_resized - img_flair_resized.min()) / (img_flair_resized.max() - img_flair_resized.min())\n\n        img_t1_resized = img_t1_resized.astype(np.float16)\n        img_t2_resized = img_t2_resized.astype(np.float16)\n        img_t1ce_resized = img_t1ce_resized.astype(np.float16)\n        img_flair_resized = img_flair_resized.astype(np.float16)\n\n        # Convert mask to categorical\n        mask_resized = tf.keras.utils.to_categorical(mask_resized, num_classes=4)\n\n        # Stack modalities into a single array\n        image = np.stack([img_t1_resized, img_t2_resized, img_t1ce_resized, img_flair_resized], axis=-1)\n\n        # Save to disk\n        #np.save(f\"/kaggle/working/data/image_{idx}.npy\", image)\n        #np.save(f\"/kaggle/working/data/mask_{idx}.npy\", mask_resized)\n        print(mask_resized.dtype)\n        del img_t1, img_t2, img_t1ce, img_flair, mask, img_t1_resized, img_t2_resized, img_t1ce_resized, img_flair_resized, mask_resized, image\n        gc.collect()\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T01:21:39.752188Z","iopub.execute_input":"2024-11-26T01:21:39.752553Z","iopub.status.idle":"2024-11-26T01:21:39.768006Z","shell.execute_reply.started":"2024-11-26T01:21:39.752501Z","shell.execute_reply":"2024-11-26T01:21:39.767114Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Combine paths for multi-modality inputs\nimage_paths = list(zip(t1_list, t2_list, t1ce_list, flair_list))\n\n# Resize and preprocess images\npreprocess_and_resize(image_paths, seg_list, target_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T01:21:39.769175Z","iopub.execute_input":"2024-11-26T01:21:39.769506Z","iopub.status.idle":"2024-11-26T01:21:42.703589Z","shell.execute_reply.started":"2024-11-26T01:21:39.769474Z","shell.execute_reply":"2024-11-26T01:21:42.702456Z"}},"outputs":[{"name":"stdout","text":"uint8\nfloat64\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom scipy.ndimage import label\n\n# Set paths\ndata_folder = \"/kaggle/input/processed-brats2020-image-mask-npy/data/\"\n\n# Load all image and mask pairs\ndef load_dataset(folder):\n    images = []\n    masks = []\n    for file in sorted(os.listdir(folder)):\n        if file.startswith(\"image_\"):\n            images.append(np.load(os.path.join(folder, file)))\n        elif file.startswith(\"mask_\"):\n            masks.append(np.load(os.path.join(folder, file)))\n    return np.array(images), np.array(masks)\n\n# Split the dataset into train, validation, and test sets\ndef split_dataset(images, masks, train_ratio=0.65, val_ratio=0.15, test_ratio=0.2):\n    X_train, X_temp, y_train, y_temp = train_test_split(images, masks, test_size=(1 - train_ratio), random_state=42)\n    val_size = test_ratio / (val_ratio + test_ratio)\n    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_size, random_state=42)\n    return X_train, X_val, X_test, y_train, y_val, y_test\n\n# Extract voxel features (intensity + coordinates)\ndef extract_voxel_features(images):\n    voxel_features = []\n    for image in images:\n        shape = image.shape[:-1]\n        coords = np.indices(shape).reshape(3, -1).T  # Coordinates\n        intensities = image.reshape(-1, 4)  # Flatten 4D intensity\n        voxel_features.append(np.hstack([coords, intensities]))\n    return np.vstack(voxel_features)\n\n# Generate class labels from the one-hot encoded masks\ndef generate_labels(masks):\n    labels = []\n    for mask in masks:\n        labels.append(np.argmax(mask, axis=-1).flatten())  # Convert one-hot to class labels\n    return np.hstack(labels)\n\n# Focal Loss\ndef focal_loss(true, pred, gamma=2.0, alpha=0.25):\n    eps = 1e-6  # To avoid log(0)\n    true = true.astype(np.float32)\n    pred = np.clip(pred, eps, 1.0 - eps)\n    cross_entropy = -true * np.log(pred)\n    weight = alpha * (1 - pred) ** gamma\n    loss = weight * cross_entropy\n    return np.mean(loss)\n\n# Dice Loss\ndef dice_loss(true, pred):\n    eps = 1e-6  # To avoid division by zero\n    intersection = np.sum(true * pred)\n    union = np.sum(true) + np.sum(pred)\n    loss = 1 - (2 * intersection + eps) / (union + eps)\n    return loss\n\n# Combined Loss\ndef combined_loss(true, pred):\n    return focal_loss(true, pred) + dice_loss(true, pred)\n\n# Hyperparameter tuning\ndef tune_hyperparameters(train_features, train_labels, val_features, val_labels, val_masks):\n    best_model = None\n    best_loss = float('inf')\n    best_params = None\n\n    # Define hyperparameter grid\n    n_estimators_list = [50, 100, 150, 200]\n    max_depth_list = [10, 20, None]\n    for n_estimators in n_estimators_list:\n        for max_depth in max_depth_list:\n            print(f\"Training Random Forest with n_estimators={n_estimators}, max_depth={max_depth}\")\n            clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n            clf.fit(train_features, train_labels)\n\n            # Validate the model\n            val_predictions = clf.predict(val_features)\n            val_predictions = val_predictions.reshape(val_masks.shape[:-1])\n            val_loss = 0\n            for i in range(len(val_masks)):\n                val_pred = val_predictions[i]\n                val_true = np.argmax(val_masks[i], axis=-1)\n                val_loss += combined_loss(val_true, val_pred)\n\n            val_loss /= len(val_masks)\n            print(f\"Validation Loss: {val_loss}\")\n\n            if val_loss < best_loss:\n                best_model = clf\n                best_loss = val_loss\n                best_params = {\"n_estimators\": n_estimators, \"max_depth\": max_depth}\n\n    print(f\"Best Hyperparameters: {best_params}\")\n    return best_model\n\n# Main Pipeline\nimages, masks = load_dataset(data_folder)\nprint(\"data load complete\")\nX_train, X_val, X_test, y_train, y_val, y_test = split_dataset(images, masks)\n\n# Prepare training and validation data\ntrain_features = extract_voxel_features(X_train)\ntrain_labels = generate_labels(y_train)\n\nval_features = extract_voxel_features(X_val)\n\n# Tune hyperparameters using combined focal + dice loss on validation set\nclf = tune_hyperparameters(train_features, train_labels, val_features, X_val)\n\n# Predict on test set\ntest_features = extract_voxel_features(X_test)\npredicted_labels = clf.predict(test_features)\n\n# Reshape predicted labels back to 3D format\npredicted_labels = predicted_labels.reshape(X_test.shape[:-1])\n\n# Evaluate on the test set\niou_scores = []\naccuracy_scores = []\nfor i in range(len(X_test)):\n    test_pred = predicted_labels[i]\n    test_gt = np.argmax(y_test[i], axis=-1)\n    iou = iou_score(test_pred, test_gt)\n    accuracy = pixel_accuracy(test_pred, test_gt)\n    iou_scores.append(iou)\n    accuracy_scores.append(accuracy)\n\n# Output metrics\nprint(\"Test IoU Scores (Per Class):\", np.mean(iou_scores, axis=0))\nprint(\"Test Pixel Accuracy:\", np.mean(accuracy_scores))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T01:21:42.704946Z","iopub.execute_input":"2024-11-26T01:21:42.705297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nimport nibabel as nib\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import jaccard_score\nimport matplotlib.pyplot as plt\n\n# Load NIfTI images and masks\ndef load_nifti_images(data_folder, num_images, suffix='image'):\n    images = []\n    masks = []\n    for i in range(1, num_images+1):\n        image = nib.load(os.path.join(data_folder, f'{suffix}_{i}.nii.gz')).get_fdata()\n        images.append(image)\n        mask = nib.load(os.path.join(data_folder, f'mask_{i}.nii.gz')).get_fdata()\n        masks.append(mask)\n    return np.array(images), np.array(masks)\n\n# Split the data into train, validation, and test sets\ndef train_val_test_split(images, masks, train_size=0.65, val_size=0.15, test_size=0.20):\n    X_train, X_temp, y_train, y_temp = train_test_split(images, masks, train_size=train_size, random_state=42)\n    val_size_adjusted = val_size / (val_size + test_size)\n    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, train_size=val_size_adjusted, random_state=42)\n    return X_train, X_val, X_test, y_train, y_val, y_test\n\ndef extract_seed_coordinates(mask):\n    seed_coords = []\n    \n    # Iterate over all classes (assuming mask has shape (128, 128, 64, num_classes))\n    for class_idx in range(mask.shape[-1]):\n        # Find non-zero voxels for the current class\n        class_mask = mask[..., class_idx]  # Extract the binary mask for this class\n        if np.sum(class_mask) > 0:\n            # Compute the center of mass (seed) for the current class\n            center_of_mass = measurements.center_of_mass(class_mask)\n            seed_coords.append(center_of_mass)\n    \n    return np.array(seed_coords) \n\n# CNN Model for seed generation\ndef create_seed_model(input_shape=(128, 128, 64, 4), num_classes=4):\n    model = models.Sequential([\n        layers.Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=input_shape),\n        layers.MaxPooling3D(pool_size=(2, 2, 2)),\n        layers.Conv3D(64, kernel_size=(3, 3, 3), activation='relu'),\n        layers.MaxPooling3D(pool_size=(2, 2, 2)),\n        layers.Conv3D(128, kernel_size=(3, 3, 3), activation='relu'),\n        layers.MaxPooling3D(pool_size=(2, 2, 2)),\n        layers.Flatten(),\n        layers.Dense(512, activation='relu'),\n        layers.Dense(num_classes, activation='softmax')  # Output 4 classes for seed prediction\n    ])\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Region Growing Algorithm with Shape Constraints\ndef hybrid_region_growing(image, seeds, thresholds, max_size=5000):\n    segmented = np.zeros_like(image)\n    for seed, threshold in zip(seeds, thresholds):\n        stack = [seed]\n        region_size = 0\n        while stack and region_size < max_size:\n            x, y, z = stack.pop()\n            if segmented[x, y, z] == 0:\n                segmented[x, y, z] = 1\n                region_size += 1\n                for dx, dy, dz in [(-1, 0, 0), (1, 0, 0), (0, -1, 0), (0, 1, 0), (0, 0, -1), (0, 0, 1)]:\n                    nx, ny, nz = x + dx, y + dy, z + dz\n                    if 0 <= nx < image.shape[0] and 0 <= ny < image.shape[1] and 0 <= nz < image.shape[2]:\n                        if abs(image[nx, ny, nz] - image[x, y, z]) <= threshold:\n                            stack.append((nx, ny, nz))\n    return segmented\n\n# IoU and Pixel-Level Accuracy\ndef iou(true_mask, predicted_mask, num_classes=4):\n    iou_scores = []\n    for i in range(num_classes):\n        intersection = np.sum((true_mask == i) & (predicted_mask == i))\n        union = np.sum((true_mask == i) | (predicted_mask == i))\n        iou_scores.append(intersection / union)\n    return np.mean(iou_scores)\n\ndef pixel_accuracy(true_mask, predicted_mask):\n    return np.sum(true_mask == predicted_mask) / true_mask.size\n\n# Visualize 3D segmentation\ndef visualize_3d_segmentation(segmentation):\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    x, y, z = np.nonzero(segmentation)\n    ax.scatter(x, y, z, c=z, cmap='jet')\n    plt.show()\n\n# Main Script\ndef main(data_folder, num_images=100):\n    # Load the data\n    images, masks = load_nifti_images(data_folder, num_images)\n\n    # Split into train, validation, and test\n    X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(images, masks)\n\n    # Create and train the CNN model for seed generation\n    seed_model = create_seed_model(input_shape=(128, 128, 64, 4), num_classes=4)\n    seed_model.summary()\n\n    # Train the seed model\n    seed_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=4, \n                   callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)])\n\n    # Generate seeds for region growing using the CNN model\n    seed_predictions = seed_model.predict(X_test)\n    seeds = np.argmax(seed_predictions, axis=-1)  # Get the most probable class for each voxel\n\n    # Perform Region Growing with Shape Constraints\n    thresholds = [0.2, 0.3, 0.4, 0.5]  # Example thresholds for each class\n    segmented_images = []\n    for i in range(X_test.shape[0]):\n        segmented = hybrid_region_growing(X_test[i], seeds[i], thresholds)\n        segmented_images.append(segmented)\n    segmented_images = np.array(segmented_images)\n\n    # Evaluate using IoU and Pixel-Level Accuracy\n    iou_score = iou(y_test, segmented_images)\n    pixel_acc = pixel_accuracy(y_test, segmented_images)\n    print(f\"IoU Score: {iou_score}\")\n    print(f\"Pixel-Level Accuracy: {pixel_acc}\")\n\n    # Visualize the segmentation for the first test image\n    visualize_3d_segmentation(segmented_images[0])\n\n\ndata_folder = '/kaggle/input/processed-brats2020-image-mask-npy/data'  # Update with your data folder path\nmain(data_folder)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport os\nfrom sklearn.tree import DecisionTreeRegressor\nimport tensorflow.keras.backend as K\n\n# Data Generator Class\nclass DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, data_dir, batch_size=32, image_size=(128, 128, 64, 4), num_classes=4, shuffle=True, drop_last=True):\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.image_size = image_size\n        self.num_classes = num_classes\n        self.shuffle = shuffle\n        self.drop_last = drop_last\n        self.image_files = sorted([f for f in os.listdir(data_dir) if f.startswith('image_') and f.endswith('.npy')])\n        self.mask_files = sorted([f for f in os.listdir(data_dir) if f.startswith('mask_') and f.endswith('.npy')])\n        \n        self.indices = np.arange(len(self.image_files))\n        self.on_epoch_end()\n\n    def __getitem__(self, index):\n        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n        if len(batch_indices) < self.batch_size and self.drop_last:\n            return None  # Or raise an error, depending on your needs\n        batch_images = []\n        batch_masks = []\n\n        for idx in batch_indices:\n            image_path = os.path.join(self.data_dir, self.image_files[idx])\n            mask_path = os.path.join(self.data_dir, self.mask_files[idx])\n            image = np.load(image_path)\n            mask = np.load(mask_path)\n            batch_images.append(image)\n            batch_masks.append(mask)\n\n        return np.array(batch_images), np.array(batch_masks)\n\n    def __len__(self):\n        # Calculate number of batches (excluding the last batch if drop_last=True)\n        return len(self.image_files) // self.batch_size if self.drop_last else (len(self.image_files) + self.batch_size - 1) // self.batch_size\n\n\n# Load data directory\ndata_dir = '/kaggle/input/processed-brats2020-image-mask-npy/data/'  # Folder containing both image and mask .npy files\n\n# Initialize Data Generator\nbatch_size = 8\ntrain_gen = DataGenerator(data_dir, batch_size=batch_size)\nval_gen = DataGenerator(data_dir, batch_size=batch_size)\n\n# Train a decision tree model for each class\ndef train_decision_tree(image_data, mask_data):\n    X = image_data.reshape(-1, image_data.shape[-1])  # Flatten the images to (num_voxels, num_modalities)\n    y = mask_data.reshape(-1, mask_data.shape[-1])    # Flatten the mask to (num_voxels, num_classes)\n    \n    models = []\n    for class_idx in range(mask_data.shape[-1]):  # For each class\n        y_class = np.where(y[:, class_idx] == 1)[0]  # Get indices where the class is present\n        coords = np.array([(i // (image_data.shape[1] * image_data.shape[2]),\n                            (i // image_data.shape[2]) % image_data.shape[1],\n                            i % image_data.shape[2]) for i in y_class])  # Get 3D coordinates\n        \n        model = DecisionTreeRegressor(max_depth=10)  # You can adjust max depth\n        model.fit(X[y_class], coords)  # Train decision tree to predict (x, y, z) coordinates\n        models.append(model)\n    \n    return models\n\n# Predict seed coordinates for each class\ndef predict_seeds(models, image_data):\n    X = image_data.reshape(-1, image_data.shape[-1])\n    predicted_seed_coords = []\n    for class_idx, model in enumerate(models):\n        predicted_seeds = model.predict(X)\n        predicted_seed_coords.append(predicted_seeds)\n    return predicted_seed_coords\n\n# Define Region Growing with Hybrid Thresholding and Shape Constraints\ndef get_neighbors(seed, shape):\n    neighbors = []\n    directions = [(-1, 0, 0), (1, 0, 0), (0, -1, 0), (0, 1, 0), (0, 0, -1), (0, 0, 1)]\n    for direction in directions:\n        neighbor = tuple(np.add(seed, direction))\n        if all(0 <= n < dim for n, dim in zip(neighbor, shape)):\n            neighbors.append(neighbor)\n    return neighbors\n\ndef region_growing(image, seed_coords, threshold=0.5, min_size=10, max_size=2000):\n    grown_mask = np.zeros_like(image, dtype=np.uint8)\n    grown_mask[tuple(seed_coords)] = 1\n    \n    size = 0\n    regions = [seed_coords]\n    \n    while regions:\n        new_regions = []\n        for region in regions:\n            neighbors = get_neighbors(region, image.shape)\n            for neighbor in neighbors:\n                if grown_mask[neighbor] == 0 and image[neighbor] > threshold:  # Apply threshold\n                    grown_mask[neighbor] = 1\n                    new_regions.append(neighbor)\n                    size += 1\n        regions = new_regions\n        if size > max_size or size < min_size:\n            break\n    return grown_mask\n\n# Focal Loss and Dice Loss functions\ndef focal_loss(y_true, y_pred, alpha=0.25, gamma=2.0):\n    cross_entropy = -y_true * K.log(y_pred)\n    loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n    return K.sum(loss, axis=-1)\n\ndef dice_loss(y_true, y_pred, smooth=1e-6):\n    intersection = K.sum(K.abs(y_true * y_pred), axis=(1, 2, 3))\n    sum_ = K.sum(y_true, axis=(1, 2, 3)) + K.sum(y_pred, axis=(1, 2, 3))\n    return 1 - (2 * intersection + smooth) / (sum_ + smooth)\n\ndef combined_loss(y_true, y_pred):\n    return dice_loss(y_true, y_pred) + focal_loss(y_true, y_pred)\n\n# Custom Training Step\n@tf.function\ndef custom_train_step(model, image, mask, models):\n    with tf.GradientTape() as tape:\n        seed_coords = predict_seeds(models, image)  # Get seed coordinates from the decision tree\n        grown_mask = region_growing(image, seed_coords)  # Apply region growing using the seeds\n        loss = combined_loss(mask, grown_mask)  # Calculate combined loss (Dice + Focal Loss)\n    \n    gradients = tape.gradient(loss, model.trainable_variables)\n    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    return loss\n\n# Evaluation Metrics\ndef compute_iou(pred, mask, num_classes=4):\n    iou_scores = []\n    for i in range(num_classes):\n        intersection = np.sum((pred == i) & (mask == i))\n        union = np.sum((pred == i) | (mask == i))\n        iou_scores.append(intersection / union if union != 0 else 0)\n    return np.mean(iou_scores)\n\ndef compute_pixel_accuracy(pred, mask):\n    return np.sum(pred == mask) / np.size(mask)\n\n# Train decision tree models to predict seed coordinates\nmodels = []\ncount =0\nfor image_batch, mask_batch in train_gen:\n    print(count)\n    count+=1\n    models = train_decision_tree(image_batch, mask_batch)\n\n# Model Training Loop\nepochs = 10\nfor epoch in range(epochs):\n    for step, (image_batch, mask_batch) in enumerate(train_gen):\n        loss = custom_train_step(seed_model, image_batch, mask_batch, models)  # Train the model using the custom train step\n        if step % 100 == 0:\n            print(f\"Epoch {epoch + 1}, Step {step}, Loss: {loss.numpy()}\")\n\n    # Evaluate on validation set\n    val_iou = []\n    val_pixel_acc = []\n    for image_batch, mask_batch in val_gen:\n        pred_mask = region_growing(image_batch, predict_seeds(models, image_batch))\n        val_iou.append(compute_iou(pred_mask, mask_batch))\n        val_pixel_acc.append(compute_pixel_accuracy(pred_mask, mask_batch))\n\n    print(f\"Validation IoU: {np.mean(val_iou)}, Pixel Accuracy: {np.mean(val_pixel_acc)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def data_generator(image_files, mask_files, batch_size):\n    while True:  # Infinite loop to keep generating batches\n        for i in range(0, len(image_files), batch_size):\n            image_batch = []\n            mask_batch = []\n            for j in range(i, min(i + batch_size, len(image_files))):\n                image = np.load(image_files[j])\n                mask = np.load(mask_files[j])\n                image_batch.append(image)\n                mask_batch.append(mask)\n            yield np.array(image_batch), np.array(mask_batch)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith('image_')])\nmask_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith('mask_')])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"c=0\nfor image_batch, mask_batch in data_generator(image_files, mask_files, batch_size=8):\n    print(c,image_batch.shape, mask_batch.shape)\n    c+=1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import Sequence\nfrom sklearn.metrics import confusion_matrix\nimport multiprocessing\nimport random\n\n# Data generator class\nclass DataGenerator(Sequence):\n    def __init__(self, data_dir, batch_size=8, image_size=(128, 128, 64, 4), num_classes=4, shuffle=True):\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.image_size = image_size\n        self.num_classes = num_classes\n        self.shuffle = shuffle\n\n        self.image_files = sorted([f for f in os.listdir(data_dir) if f.startswith('image_') and f.endswith('.npy')])\n        self.mask_files = sorted([f for f in os.listdir(data_dir) if f.startswith('mask_') and f.endswith('.npy')])\n\n        self.indices = np.arange(len(self.image_files))\n        self.on_epoch_end()\n\n    def __getitem__(self, index):\n        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n        batch_images = []\n        batch_masks = []\n\n        for idx in batch_indices:\n            image_path = os.path.join(self.data_dir, self.image_files[idx])\n            mask_path = os.path.join(self.data_dir, self.mask_files[idx])\n\n            image = np.load(image_path)\n            mask = np.load(mask_path)\n            batch_images.append(image)\n            batch_masks.append(mask)\n\n        return np.array(batch_images), np.array(batch_masks)\n\n    def __len__(self):\n        return len(self.image_files) // self.batch_size\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.shuffle(self.indices)\n\n# Region Growing Algorithm\ndef region_growing(image, seed_coords, threshold=0.1):\n    grown_mask = np.zeros_like(image)\n    for seed in seed_coords:\n        x, y, z = seed\n        grown_mask[x, y, z] = 1  # Set seed point in the mask\n\n        # Perform region growing around seed (simplified)\n        for i in range(max(0, x-1), min(image.shape[0], x+2)):\n            for j in range(max(0, y-1), min(image.shape[1], y+2)):\n                for k in range(max(0, z-1), min(image.shape[2], z+2)):\n                    if np.abs(image[i, j, k] - image[x, y, z]) < threshold:\n                        grown_mask[i, j, k] = 1\n    return grown_mask\n\n# Loss Functions\ndef dice_loss(y_true, y_pred):\n    intersection = K.sum(K.abs(y_true * y_pred), axis=(1, 2, 3))\n    sum_ = K.sum(y_true, axis=(1, 2, 3)) + K.sum(y_pred, axis=(1, 2, 3))\n    return 1 - (2 * intersection + 1e-6) / (sum_ + 1e-6)\n\ndef focal_loss(y_true, y_pred, gamma=2, alpha=0.25):\n    cross_entropy = -y_true * K.log(y_pred)\n    loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n    return K.sum(loss, axis=-1)\n\ndef hybrid_loss(y_true, y_pred):\n    return dice_loss(y_true, y_pred) + focal_loss(y_true, y_pred)\n\n# Train a Decision Tree model to generate seed coordinates\ndef train_decision_tree(image_data, mask_data):\n    # Flatten the image and mask for decision tree input\n    X = image_data.reshape(-1, image_data.shape[-1])  # Flatten image (voxels, channels)\n    y = mask_data.argmax(axis=-1).reshape(-1)  # Flatten mask (voxels, classes)\n\n    clf = DecisionTreeClassifier(max_depth=5)  # Decision Tree Classifier\n    clf.fit(X, y)\n    \n    return clf\n\n# Function to get seed coordinates from Decision Tree\ndef get_seed_coordinates(clf, image_data, num_classes=4):\n    seed_coords = []\n    for class_idx in range(num_classes):\n        # For each class, get the coordinates where the classifier predicts that class\n        predicted_mask = clf.predict(image_data.reshape(-1, image_data.shape[-1])).reshape(image_data.shape[:-1])\n        class_coords = np.argwhere(predicted_mask == class_idx)\n        if class_coords.size > 0:\n            seed_coords.append(random.choice(class_coords))  # Randomly select a seed point\n    return seed_coords\n\n# Train and evaluate the model\ndata_dir = \"/kaggle/input/processed-brats2020-image-mask-npy/data\"\ntrain_gen = DataGenerator(data_dir, batch_size=8)\n\n# Load a small subset of data for decision tree training (you can adjust as needed)\ntrain_images, train_masks = [], []\nfor i in range(10):  # Just using 10 samples to train the decision tree for simplicity\n    image = np.load(os.path.join(data_dir, f'image_{i:03d}.npy'))\n    mask = np.load(os.path.join(data_dir, f'mask_{i:03d}.npy'))\n    train_images.append(image)\n    train_masks.append(mask)\n\ntrain_images = np.array(train_images)\ntrain_masks = np.array(train_masks)\n\n# Train decision tree on the training data\nclf = train_decision_tree(train_images, train_masks)\n\n# Get seed coordinates for region growing\nseed_coords = get_seed_coordinates(clf, train_images[0])  # Example for the first image\n\n# Apply hybrid region growing and calculate the loss\ngenerated_mask = region_growing(train_images[0], seed_coords)\nhybrid_loss_value = hybrid_loss(train_masks[0], generated_mask)\n\nprint(\"Hybrid Loss:\", hybrid_loss_value)\n\n# You can use the hybrid_loss_value as part of your training loop\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport os\nimage_files = [f for f in os.listdir('/kaggle/input/processed-brats2020-image-mask-npy/data/') if f.startswith('image')]\nmask_files = [f for f in os.listdir('/kaggle/input/processed-brats2020-image-mask-npy/data/') if f.startswith('mask')]\n\nprint(f\"Number of image files: {len(image_files)}\")\nprint(f\"Number of mask files: {len(mask_files)}\")\n\n# Ensure image and mask files are of the same length\nif len(image_files) != len(mask_files):\n    raise ValueError(\"Mismatch in number of image and mask files.\")\n\n# Split data into 80% training and 20% testing\ntrain_image_files, test_image_files, train_mask_files, test_mask_files = train_test_split(\n    image_files, mask_files, test_size=0.2, random_state=42\n)\n\nprint(f\"Training samples: {len(train_image_files)}\")\nprint(f\"Testing samples: {len(test_image_files)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, LayerNormalization, Conv3D, UpSampling3D, Concatenate, Add\nimport numpy as np\nimport nibabel as nib\nimport glob\nimport matplotlib.pyplot as plt\nfrom tifffile import imsave\nfrom tqdm import tqdm\nimport os\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom skimage.transform import resize\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nimport gc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for available GPUs\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Allocate memory growth for the GPUs\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n            print(\"Device\")\n    except RuntimeError as e:\n        print(e)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def swin_transformer_block(x, embed_dim, num_heads, window_size, shift=False):\n    \"\"\"\n    Swin Transformer Block with residual connections.\n    \"\"\"\n    ln1 = LayerNormalization()(x)\n    w_msa = shifted_attention(ln1, embed_dim, num_heads, window_size, shift=False)\n    skip1 = Add()([x, w_msa])  # Residual connection\n\n    ln2 = LayerNormalization()(skip1)\n    mlp1 = Dense(embed_dim * 4, activation=\"gelu\")(ln2)\n    mlp1 = Dense(embed_dim)(mlp1)\n    skip2 = Add()([skip1, mlp1])  # Residual connection\n\n    ln3 = LayerNormalization()(skip2)\n    sw_msa = shifted_attention(ln3, embed_dim, num_heads, window_size, shift=True)\n    skip3 = Add()([skip2, sw_msa])  # Residual connection\n\n    ln4 = LayerNormalization()(skip3)\n    mlp2 = Dense(embed_dim * 4, activation=\"gelu\")(ln4)\n    mlp2 = Dense(embed_dim)(mlp2)\n    skip4 = Add()([skip3, mlp2])  # Residual connection\n\n    return skip4\n\n\ndef shifted_attention(x, embed_dim, num_heads, window_size, shift=False):\n    \"\"\"\n    Computes Shifted Window Multi-Head Self-Attention (SW-MSA) or W-MSA.\n    \"\"\"\n    if shift:\n        shift_dim = window_size // 2\n        shifted_x = Concatenate(axis=1)([x[:, -shift_dim:], x[:, :-shift_dim]])\n    else:\n        shifted_x = x\n\n    attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(shifted_x, shifted_x)\n    return attention\n\n\ndef patch_embedding(x, embed_dim):\n    \"\"\"\n    Patch partitioning for downsampling.\n    \"\"\"\n    x = Conv3D(embed_dim, kernel_size=3, strides=2, padding=\"same\")(x)\n    return x\n\n\ndef decoder_block(x, skip, embed_dim, is_bottleneck=False):\n    \"\"\"\n    Decoder block with upsampling and skip connections.\n    If `is_bottleneck=True`, no concatenation is done, only conv layers.\n    \"\"\"\n    # First, upsample `x` to match the spatial size of `skip`\n    if x.shape[1] != skip.shape[1] or x.shape[2] != skip.shape[2] or x.shape[3] != skip.shape[3]:\n        x = UpSampling3D(size=(2, 2, 2))(x)  # Upsample by a factor of 2\n    \n    # Ensure the dimensions match before concatenation (if necessary, apply Conv3D)\n    if x.shape[1] != skip.shape[1] or x.shape[2] != skip.shape[2] or x.shape[3] != skip.shape[3]:\n        x = Conv3D(embed_dim, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n\n    # Concatenate the upsampled `x` with the `skip` connection\n    x = Concatenate()([x, skip])\n\n    # Apply a Conv3D layer after concatenation to refine the features\n    x = Conv3D(embed_dim, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n    \n    return x\n\n\ndef swin_3d_unetr(input_shape, num_classes):\n    \"\"\"\n    Swin 3D UNETR model with hierarchical Swin Transformer embedding blocks and decoder.\n    \"\"\"\n    inputs = Input(shape=(128, 128, 64, 4))  # Fix spatial dimensions and number of channels\n\n    # Encoder\n    enc1 = inputs  # Level 1: Input image\n    embed_1 = patch_embedding(enc1, embed_dim=48)  # Level 2\n    swin_1 = swin_transformer_block(embed_1, embed_dim=48, num_heads=3, window_size=7, shift=False)  # Level 3\n\n    embed_2 = patch_embedding(swin_1, embed_dim=96)  # Level 4\n    swin_2 = swin_transformer_block(embed_2, embed_dim=96, num_heads=6, window_size=7, shift=True)  # Level 5\n\n    embed_3 = patch_embedding(swin_2, embed_dim=192)  # Level 6\n    swin_3 = swin_transformer_block(embed_3, embed_dim=192, num_heads=12, window_size=7, shift=False)  # Level 7\n\n    embed_4 = patch_embedding(swin_3, embed_dim=384)  # Level 8\n    swin_4 = swin_transformer_block(embed_4, embed_dim=384, num_heads=24, window_size=7, shift=True)  # Level 9\n\n    embed_5 = patch_embedding(swin_4, embed_dim=768)  # Level 10 (Bottleneck)\n    swin_5 = swin_transformer_block(embed_5, embed_dim=768, num_heads=48, window_size=7, shift=False)  # Bottleneck\n\n    # Decoder\n    dec_6 = decoder_block(swin_5, swin_4, embed_dim=768, is_bottleneck=True)  # Bottleneck decoder (no concat)\n    dec_5 = decoder_block(dec_6, swin_3, embed_dim=384)\n    dec_4 = decoder_block(dec_5, swin_2, embed_dim=192)\n    dec_3 = decoder_block(dec_4, swin_1, embed_dim=96)\n    dec_2 = decoder_block(dec_3, embed_1, embed_dim=48)\n    dec_1 = decoder_block(dec_2, enc1, embed_dim=48)  # Level 1\n\n    # Segmentation Head\n    # Output shape is (128, 128, 64) with num_classes\n    outputs = Conv3D(num_classes, kernel_size=1, activation=\"softmax\")(dec_1)\n\n    return tf.keras.Model(inputs, outputs)\n\n\n# Define input shape and number of classes\ninput_shape = (128, 128, 64, 4)  # New input dimensions\nnum_classes = 4  # Adjust for the number of classes\n\n# Instantiate the model\nmodel_1 = swin_3d_unetr(input_shape, num_classes)\nprint(model_1.summary())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Layer Type\", f\"Output Shape\", f\"Number of Parameters\")\n\n\nfor layer in model.layers:\n    try:\n        print(f\"Layer Type: {layer.__class__.__name__}\", end=\"\")\n    except:\n        print(\" \"*10, end=\"\")\n    try:\n        print(f\"Output Shape: {layer.output_shape}\", end=\"\")\n    except:\n        print(\" \"*10, end=\"\")\n    try:\n        print(f\"Number of Parameters: {layer.count_params()}\")\n    except:\n        print(\" \"*10)\n    print(\"-\"*50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import LayerNormalization, Dense, Conv3D, UpSampling3D, Input, Add, Concatenate, Lambda\nfrom tensorflow.keras.models import Model\nfrom functools import partial\n\ndef decoder_block(x, skip, embed_dim, is_bottleneck=False):\n    \"\"\"\n    Decoder block with upsampling and skip connections.\n    Ensures dimensional consistency for the decoder layers.\n    \"\"\"\n    # First, upsample `x` to match the spatial size of `skip`\n    if x.shape[1] != skip.shape[1] or x.shape[2] != skip.shape[2] or x.shape[3] != skip.shape[3]:\n        x = UpSampling3D(size=(2, 2, 2))(x)  # Upsample by a factor of 2\n    \n    # If spatial dimensions still don't match, adjust `skip` using Conv3D to match `x`\n    if x.shape[1] != skip.shape[1] or x.shape[2] != skip.shape[2] or x.shape[3] != skip.shape[3]:\n        skip = Conv3D(embed_dim, kernel_size=3, padding=\"same\", activation=\"relu\")(skip)\n\n    # Ensure that `skip` and `x` have the same number of channels (depth)\n    if x.shape[-1] != skip.shape[-1]:\n        skip = Conv3D(x.shape[-1], kernel_size=1, padding=\"same\")(skip)\n\n    # Concatenate the upsampled `x` with the `skip` connection\n    x = Concatenate()([x, skip])\n\n    # Apply a Conv3D layer after concatenation to refine the features\n    x = Conv3D(embed_dim, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n\n    return x\n\n\ndef swin_3d_unetr(input_shape, num_classes):\n    \"\"\"\n    Swin 3D UNETR model with hierarchical Swin Transformer embedding blocks and decoder.\n    Ensures batch size and spatial dimensions consistency at each layer.\n    \"\"\"\n    inputs = tf.keras.Input(input_shape)\n\n    # Encoder\n    enc1 = inputs  # Level 1: Input image\n    embed_1 = patch_embedding(enc1, embed_dim=48)  # Level 2\n    swin_1 = swin_transformer_block(embed_1, embed_dim=48, num_heads=3, window_size=7, shift=False)  # Level 3\n\n    embed_2 = patch_embedding(swin_1, embed_dim=96)  # Level 4\n    swin_2 = swin_transformer_block(embed_2, embed_dim=96, num_heads=6, window_size=7, shift=True)  # Level 5\n\n    embed_3 = patch_embedding(swin_2, embed_dim=192)  # Level 6\n    swin_3 = swin_transformer_block(embed_3, embed_dim=192, num_heads=12, window_size=7, shift=False)  # Level 7\n\n    embed_4 = patch_embedding(swin_3, embed_dim=384)  # Level 8\n    swin_4 = swin_transformer_block(embed_4, embed_dim=384, num_heads=24, window_size=7, shift=True)  # Level 9\n\n    embed_5 = patch_embedding(swin_4, embed_dim=768)  # Level 10 (Bottleneck)\n    swin_5 = swin_transformer_block(embed_5, embed_dim=768, num_heads=48, window_size=7, shift=False)  # Bottleneck\n\n    # Decoder\n    dec_6 = decoder_block(swin_5, swin_4, embed_dim=768, is_bottleneck=True)  # Bottleneck decoder (no concat)\n    dec_5 = decoder_block(dec_6, swin_3, embed_dim=384)\n    dec_4 = decoder_block(dec_5, swin_2, embed_dim=192)\n    dec_3 = decoder_block(dec_4, swin_1, embed_dim=96)\n    dec_2 = decoder_block(dec_3, embed_1, embed_dim=48)\n    dec_1 = decoder_block(dec_2, enc1, embed_dim=48)  # Level 1\n\n    # Segmentation Head\n    # Output shape is (128, 128, 64) with num_classes\n    outputs = Conv3D(num_classes, kernel_size=1, activation=\"softmax\")(dec_1)\n\n    return tf.keras.Model(inputs, outputs)\n\ndef patch_embedding(x, embed_dim):\n    \"\"\"\n    Patch partitioning for downsampling.\n    \"\"\"\n    x = Conv3D(embed_dim, kernel_size=3, strides=2, padding=\"same\")(x)\n    return x\n\ndef swin_transformer_block(x, embed_dim, num_heads, window_size, shift=False):\n    \"\"\"\n    Swin Transformer Block with residual connections.\n    \"\"\"\n    ln1 = tf.keras.layers.LayerNormalization()(x)\n    w_msa = shifted_attention(ln1, embed_dim, num_heads, window_size, shift=False)\n    skip1 = tf.keras.layers.Add()([x, w_msa])  # Residual connection\n\n    ln2 = tf.keras.layers.LayerNormalization()(skip1)\n    mlp1 = tf.keras.layers.Dense(embed_dim * 4, activation=\"gelu\")(ln2)\n    mlp1 = tf.keras.layers.Dense(embed_dim)(mlp1)\n    skip2 = tf.keras.layers.Add()([skip1, mlp1])  # Residual connection\n\n    ln3 = tf.keras.layers.LayerNormalization()(skip2)\n    sw_msa = shifted_attention(ln3, embed_dim, num_heads, window_size, shift=True)\n    skip3 = tf.keras.layers.Add()([skip2, sw_msa])  # Residual connection\n\n    ln4 = tf.keras.layers.LayerNormalization()(skip3)\n    mlp2 = tf.keras.layers.Dense(embed_dim * 4, activation=\"gelu\")(ln4)\n    mlp2 = tf.keras.layers.Dense(embed_dim)(mlp2)\n    skip4 = tf.keras.layers.Add()([skip3, mlp2])  # Residual connection\n\n    return skip4\n\n\ndef shifted_attention(x, embed_dim, num_heads, window_size, shift=False):\n    \"\"\"\n    Computes Shifted Window Multi-Head Self-Attention (SW-MSA) or W-MSA with dynamic shape handling.\n    \"\"\"\n    def attention_with_dynamic_shape(inputs, window_size, shift):\n        x = inputs\n        batch_size = tf.shape(x)[0]\n        height = tf.shape(x)[1]\n        width = tf.shape(x)[2]\n        depth = tf.shape(x)[3]\n\n        if shift:\n            shift_dims = [window_size // 2] * 3  # Half-window shift\n            x = tf.roll(x, shift=shift_dims, axis=[1, 2, 3])  # Circular shift along spatial axes\n\n        # Partition into windows\n        x = tf.reshape(\n            x,\n            [batch_size,\n             height // window_size, window_size,\n             width // window_size, window_size,\n             depth // window_size, window_size,\n             embed_dim],\n        )\n        x = tf.transpose(x, perm=[0, 1, 3, 5, 2, 4, 6, 7])  # Permute dimensions for attention\n        x = tf.reshape(\n            x,\n            [batch_size * (height // window_size) * (width // window_size) * (depth // window_size),\n             window_size ** 3,\n             embed_dim],\n        )\n\n        # Multi-head attention\n        attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x)\n\n        # Restore shape\n        x = tf.reshape(attention, [\n            batch_size,\n            height // window_size, width // window_size, depth // window_size,\n            window_size, window_size, window_size, embed_dim,\n        ])\n        x = tf.transpose(x, perm=[0, 1, 4, 2, 5, 3, 6, 7])  # Reverse transpose\n        x = tf.reshape(x, [batch_size, height, width, depth, embed_dim])\n\n        if shift:\n            x = tf.roll(x, shift=[-s for s in shift_dims], axis=[1, 2, 3])  # Reverse shift\n\n        return x\n\n    # Use `partial` to fix `window_size` and `shift` as arguments for `Lambda`\n    attention_fn = tf.keras.layers.Lambda(lambda x: attention_with_dynamic_shape(x, window_size, shift))\n    x = attention_fn(x)\n\n    return x\n\n# Define input shape and number of classes\ninput_shape = (128, 128, 64, 4)  # New input dimensions\nnum_classes = 4  # Adjust for the number of classes\n\n# Instantiate the model\nmodel_2 = swin_3d_unetr(input_shape, num_classes)\nmodel_2.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from functools import partial\n\ndef shifted_attention(x, embed_dim, num_heads, window_size, shift=False):\n    \"\"\"\n    Computes Shifted Window Multi-Head Self-Attention (SW-MSA) or W-MSA with dynamic shape handling.\n    \"\"\"\n    def attention_with_dynamic_shape(inputs, window_size, shift):\n        x = inputs\n        batch_size = tf.shape(x)[0]\n        height = tf.shape(x)[1]\n        width = tf.shape(x)[2]\n        depth = tf.shape(x)[3]\n\n        if shift:\n            shift_dims = [window_size // 2] * 3  # Half-window shift\n            x = tf.roll(x, shift=shift_dims, axis=[1, 2, 3])  # Circular shift along spatial axes\n\n        # Partition into windows\n        x = tf.reshape(\n            x,\n            [batch_size,\n             height // window_size, window_size,\n             width // window_size, window_size,\n             depth // window_size, window_size,\n             embed_dim],\n        )\n        x = tf.transpose(x, perm=[0, 1, 3, 5, 2, 4, 6, 7])  # Permute dimensions for attention\n        x = tf.reshape(\n            x,\n            [batch_size * (height // window_size) * (width // window_size) * (depth // window_size),\n             window_size ** 3,\n             embed_dim],\n        )\n\n        # Multi-head attention\n        attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x)\n\n        # Restore shape\n        x = tf.reshape(attention, [\n            batch_size,\n            height // window_size, width // window_size, depth // window_size,\n            window_size, window_size, window_size, embed_dim,\n        ])\n        x = tf.transpose(x, perm=[0, 1, 4, 2, 5, 3, 6, 7])  # Reverse transpose\n        x = tf.reshape(x, [batch_size, height, width, depth, embed_dim])\n\n        if shift:\n            x = tf.roll(x, shift=[-s for s in shift_dims], axis=[1, 2, 3])  # Reverse shift\n\n        return x\n\n    # Use `partial` to fix `window_size` and `shift` as arguments for `Lambda`\n    attention_fn = partial(attention_with_dynamic_shape, window_size=window_size, shift=shift)\n    x = Lambda(attention_fn)(x)\n\n    return x\n\n\ndef swin_transformer_block(x, embed_dim, num_heads, window_size, shift=False):\n    \"\"\"\n    Swin Transformer Block with residual connections.\n    \"\"\"\n    ln1 = LayerNormalization()(x)\n    w_msa = shifted_attention(ln1, embed_dim, num_heads, window_size, shift=False)\n    skip1 = Add()([x, w_msa])  # Residual connection\n\n    ln2 = LayerNormalization()(skip1)\n    mlp1 = Dense(embed_dim * 4, activation=\"gelu\")(ln2)\n    mlp1 = Dense(embed_dim)(mlp1)\n    skip2 = Add()([skip1, mlp1])  # Residual connection\n\n    ln3 = LayerNormalization()(skip2)\n    sw_msa = shifted_attention(ln3, embed_dim, num_heads, window_size, shift=True)\n    skip3 = Add()([skip2, sw_msa])  # Residual connection\n\n    ln4 = LayerNormalization()(skip3)\n    mlp2 = Dense(embed_dim * 4, activation=\"gelu\")(ln4)\n    mlp2 = Dense(embed_dim)(mlp2)\n    skip4 = Add()([skip3, mlp2])  # Residual connection\n\n    return skip4\n\ndef patch_embedding(x, embed_dim):\n    \"\"\"\n    Patch partitioning for downsampling.\n    \"\"\"\n    x = Conv3D(embed_dim, kernel_size=3, strides=2, padding=\"same\")(x)\n    return x\n\n\ndef decoder_block(x, skip, embed_dim, is_bottleneck=False):\n    \"\"\"\n    Decoder block with upsampling and skip connections.\n    If `is_bottleneck=True`, no concatenation is done, only conv layers.\n    \"\"\"\n    # First, upsample `x` to match the spatial size of `skip`\n    if x.shape[1] != skip.shape[1] or x.shape[2] != skip.shape[2] or x.shape[3] != skip.shape[3]:\n        x = UpSampling3D(size=(2, 2, 2))(x)  # Upsample by a factor of 2\n    \n    # Ensure the dimensions match before concatenation (if necessary, apply Conv3D)\n    if x.shape[1] != skip.shape[1] or x.shape[2] != skip.shape[2] or x.shape[3] != skip.shape[3]:\n        x = Conv3D(embed_dim, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n\n    # Concatenate the upsampled `x` with the `skip` connection\n    x = Concatenate()([x, skip])\n\n    # Apply a Conv3D layer after concatenation to refine the features\n    x = Conv3D(embed_dim, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n    \n    return x\n\n\ndef swin_3d_unetr(input_shape, num_classes):\n    \"\"\"\n    Swin 3D UNETR model with hierarchical Swin Transformer embedding blocks and decoder.\n    \"\"\"\n    inputs = Input(input_shape)\n\n    # Encoder\n    enc1 = inputs  # Level 1: Input image\n    embed_1 = patch_embedding(enc1, embed_dim=48)  # Level 2\n    swin_1 = swin_transformer_block(embed_1, embed_dim=48, num_heads=3, window_size=7, shift=False)  # Level 3\n\n    embed_2 = patch_embedding(swin_1, embed_dim=96)  # Level 4\n    swin_2 = swin_transformer_block(embed_2, embed_dim=96, num_heads=6, window_size=7, shift=True)  # Level 5\n\n    embed_3 = patch_embedding(swin_2, embed_dim=192)  # Level 6\n    swin_3 = swin_transformer_block(embed_3, embed_dim=192, num_heads=12, window_size=7, shift=False)  # Level 7\n\n    embed_4 = patch_embedding(swin_3, embed_dim=384)  # Level 8\n    swin_4 = swin_transformer_block(embed_4, embed_dim=384, num_heads=24, window_size=7, shift=True)  # Level 9\n\n    embed_5 = patch_embedding(swin_4, embed_dim=768)  # Level 10 (Bottleneck)\n    swin_5 = swin_transformer_block(embed_5, embed_dim=768, num_heads=48, window_size=7, shift=False)  # Bottleneck\n\n    # Decoder\n    dec_6 = decoder_block(swin_5, swin_4, embed_dim=768, is_bottleneck=True)  # Bottleneck decoder (no concat)\n    dec_5 = decoder_block(dec_6, swin_3, embed_dim=384)\n    dec_4 = decoder_block(dec_5, swin_2, embed_dim=192)\n    dec_3 = decoder_block(dec_4, swin_1, embed_dim=96)\n    dec_2 = decoder_block(dec_3, embed_1, embed_dim=48)\n    dec_1 = decoder_block(dec_2, enc1, embed_dim=48)  # Level 1\n\n    # Segmentation Head\n    # Output shape is (128, 128, 64) with num_classes\n    outputs = Conv3D(num_classes, kernel_size=1, activation=\"softmax\")(dec_1)\n\n    return tf.keras.Model(inputs, outputs)\n\n\n# Define input shape and number of classes\ninput_shape = (128, 128, 64, 4)  # New input dimensions\nnum_classes = 4  # Adjust for the number of classes\n\n# Instantiate the model\nmodel_3 = swin_3d_unetr(input_shape, num_classes)\nprint(model_3.summary())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dice_loss(y_true, y_pred):\n    \"\"\"\n    Calculate Dice Loss.\n    \"\"\"\n    smooth = 1e-6\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    intersection = tf.reduce_sum(y_true * y_pred)\n    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n    dice = (2. * intersection + smooth) / (union + smooth)\n    return 1 - dice\n\ndef focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n    \"\"\"\n    Calculate Focal Loss.\n    \"\"\"\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n    loss = -y_true * (alpha * tf.pow(1 - y_pred, gamma) * tf.math.log(y_pred))\n    return tf.reduce_sum(loss)\n\ndef combined_loss(y_true, y_pred):\n    \"\"\"\n    Combine Dice Loss and Focal Loss.\n    \"\"\"\n    return dice_loss(y_true, y_pred) + focal_loss(y_true, y_pred)\n\ndef iou_metric(y_true, y_pred):\n    \"\"\"\n    Calculate Intersection over Union (IoU).\n    \"\"\"\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n    intersection = tf.reduce_sum(y_true * y_pred)\n    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n    return intersection / union\n\ndef pixel_accuracy(y_true, y_pred):\n    \"\"\"\n    Calculate pixel-wise accuracy.\n    \"\"\"\n    y_true = tf.argmax(y_true, axis=-1)\n    y_pred = tf.argmax(y_pred, axis=-1)\n    return tf.reduce_mean(tf.cast(y_true == y_pred, tf.float32))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom sklearn.model_selection import train_test_split\n\n# List all files\nfolder = \"/kaggle/input/processed-brats2020-image-mask-npy/data\"\nfiles = sorted([f for f in os.listdir(folder) if f.endswith('.npy')])\n\n# Separate image and mask files\nimage_files = sorted([f for f in files if f.startswith('image_')])\nmask_files = sorted([f for f in files if f.startswith('mask_')])\nprint(len(image_files))\nprint(len(mask_files))\n# Train-test split based on file paths\nimage_train, image_test, mask_train, mask_test = train_test_split(\n    image_files, mask_files, test_size=0.2, random_state=42\n)\n\n# # Split dataset into train, validation, and test sets\n# X_train, X_temp, y_train, y_temp = train_test_split(image_files, mask_files, test_size=0.35, random_state=42)\n# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.57, random_state=42)  # 15% val, 20% test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_npy_files_in_batches(file_paths, folder, batch_size=16, image_shape=(128, 128, 64, 4)):\n    \"\"\"\n    Loads .npy files in batches, reshapes them to the specified image_shape, and normalizes image data.\n    \"\"\"\n    for i in range(0, len(file_paths), batch_size):\n        batch_files = file_paths[i:i + batch_size]\n\n        images = []\n        masks = []\n\n        for img_file, mask_file in batch_files:\n            img_path = os.path.join(folder, img_file)\n            mask_path = os.path.join(folder, mask_file)\n\n            # Load the files\n            image = np.load(img_path).astype(np.float32)  # Ensure dtype is float32\n            image = np.resize(image, image_shape)        # Resize to fixed shape\n            image /= 255.0                               # Normalize to [0, 1]\n\n            mask = np.load(mask_path).astype(np.uint8)   # Ensure dtype is uint8\n            mask = np.resize(mask, image_shape[:3] + (1,))  # Resize and add channel dimension\n\n            images.append(image)\n            masks.append(mask)\n\n        yield np.array(images), np.array(masks, dtype=np.uint8)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_npy_files_in_batches(file_paths, folder, batch_size=16):\n    \"\"\"\n    Generator to load .npy files in batches.\n\n    Parameters:\n    - file_paths: list, list of file names to load.\n    - folder: str, folder containing the files.\n    - batch_size: int, number of files to load per batch.\n\n    Yields:\n    - images: numpy array, batch of images.\n    - masks: numpy array, batch of masks.\n    \"\"\"\n    for i in range(0, len(file_paths), batch_size):\n        batch_files = file_paths[i:i + batch_size]\n\n        images = []\n        masks = []\n\n        for img_file, mask_file in batch_files:\n            img_path = os.path.join(folder, img_file)\n            mask_path = os.path.join(folder, mask_file)\n\n            # Load the files\n            image = np.load(img_path)\n            mask = np.load(mask_path)\n\n            images.append(image)\n            masks.append(mask)\n\n        yield np.array(images), np.array(masks, dtype=np.uint8)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_npy_files_in_batches(file_paths, batch_size=4, image_shape=(128, 128, 64, 4)):\n    for i in range(0, len(file_paths), batch_size):\n        batch_files = file_paths[i:i + batch_size]\n\n        images = []\n        masks = []\n\n        for img_path, mask_path in batch_files:\n            # Load the files\n            image = np.load(img_path).astype(np.float32)\n            image = np.resize(image, image_shape)\n            image /= 255.0\n\n            mask = np.load(mask_path).astype(np.uint8)\n            mask = np.resize(mask, image_shape[:3] + (1,))\n\n            images.append(image)\n            masks.append(mask)\n\n        yield np.array(images), np.array(masks, dtype=np.uint8)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_npy_files_in_batches(file_paths, batch_size=16, image_shape=(128, 128, 64, 4)):\n    for i in range(0, len(file_paths), batch_size):\n        batch_files = file_paths[i:i + batch_size]\n\n        images = []\n        masks = []\n\n        for img_path, mask_path in batch_files:\n            # Load the files\n            image = np.load(img_path).astype(np.float32)\n            image = np.resize(image, image_shape)\n            image /= 255.0\n\n            mask = np.load(mask_path).astype(np.uint8)\n            mask = np.resize(mask, image_shape[:3] + (1,))\n\n            images.append(image)\n            masks.append(mask)\n\n        yield np.array(images), np.array(masks, dtype=np.uint8)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_dataset(image_files, mask_files, folder, batch_size=4):\n    def generator():\n        for img, mask in load_npy_files_in_batches(zip(image_files, mask_files), folder, batch_size):\n            yield img, mask\n\n    # Define output signature\n    output_signature = (\n        tf.TensorSpec(shape=(128, 128, 64, 4), dtype=tf.float32),  # Adjust shape as per your data\n        tf.TensorSpec(shape=(128, 128, 64, 4), dtype=tf.uint8),   # Adjust shape as per your data\n    )\n\n    return tf.data.Dataset.from_generator(generator, output_signature=output_signature)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_dataset(image_files, mask_files, batch_size=4, image_shape=(128, 128, 64, 4)):\n    def generator():\n        for img, mask in load_npy_files_in_batches(zip(image_files, mask_files), batch_size, image_shape):\n            yield img, mask\n\n    # Define output signature with fixed shapes\n    output_signature = (\n        tf.TensorSpec(shape=(batch_size,) + image_shape, dtype=tf.float32),  # Fixed image shape\n        tf.TensorSpec(shape=(batch_size,) + image_shape[:3] + (1,), dtype=tf.uint8),  # Fixed mask shape\n    )\n\n    return tf.data.Dataset.from_generator(generator, output_signature=output_signature)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_dataset(image_files, mask_files, batch_size=4, image_shape=(128, 128, 64, 4)):\n    def generator():\n        # Convert zip object to a list before passing\n        file_pairs = list(zip(image_files, mask_files))\n        for img, mask in load_npy_files_in_batches(file_pairs, batch_size, image_shape):\n            yield img, mask\n\n    # Define output signature\n    output_signature = (\n        tf.TensorSpec(shape=(None,) + image_shape, dtype=tf.float32),  # Batch dimension is None\n        tf.TensorSpec(shape=(None,) + image_shape[:3] + (1,), dtype=tf.uint8),  # Adjust for mask shape\n    )\n\n    return tf.data.Dataset.from_generator(generator, output_signature=output_signature)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = create_dataset(image_train, mask_train, batch_size=4, image_shape=(128, 128, 64, 4))\ntest_dataset = create_dataset(image_test, mask_test, batch_size=4, image_shape=(128, 128, 64, 4))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = create_dataset(image_train, mask_train, batch_size=4, image_shape=(128, 128, 64, 4))\nfor img_batch, mask_batch in train_dataset.take(1):\n    print(\"Image batch shape:\", img_batch.shape)\n    print(\"Mask batch shape:\", mask_batch.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compile the model\nmodel_1.compile(optimizer='adamw', loss=combined_loss, metrics=[iou_metric, pixel_accuracy])\n\n\n# Set up EarlyStopping and ReduceLROnPlateau\nearly_stopping = EarlyStopping(\n    monitor='val_loss',  # Monitor validation loss\n    patience=25,         # Number of epochs with no improvement after which training will be stopped\n    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss', \n    factor=0.5, \n    patience=10, \n    min_lr=1e-6,\n    verbose=1\n)\nmodel_1.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import layers, models\n\n# Set path to your .npy files\nfolder = '/kaggle/input/processed-brats2020-image-mask-npy/data'\n\n# Get the image and mask file paths\nimage_files = sorted([f for f in os.listdir(folder) if f.startswith('image_')])\nmask_files = sorted([f for f in os.listdir(folder) if f.startswith('mask_')])\n\n# Ensure the number of image files and mask files match\nassert len(image_files) == len(mask_files), \"Mismatch between image and mask file counts.\"\n\n# Get full paths to the files\nimage_paths = [os.path.join(folder, f) for f in image_files]\nmask_paths = [os.path.join(folder, f) for f in mask_files]\n\n# Split the dataset into train and test sets (e.g., 80% training, 20% testing)\ntrain_image_paths, test_image_paths, train_mask_paths, test_mask_paths = train_test_split(\n    image_paths, mask_paths, test_size=0.2, random_state=42\n)\n\n# Memory-mapping function to load images and masks\ndef load_images_and_masks(image_paths, mask_paths):\n    images = [np.load(img_path, mmap_mode='r') for img_path in image_paths]\n    masks = [np.load(mask_path, mmap_mode='r') for mask_path in mask_paths]\n    return images, masks\n\n# Load memory-mapped images and masks for both train and test sets\ntrain_images, train_masks = load_images_and_masks(train_image_paths, train_mask_paths)\ntest_images, test_masks = load_images_and_masks(test_image_paths, test_mask_paths)\n\n# Data generator to yield batches of images and masks\ndef data_generator(images, masks, batch_size=4):\n    num_samples = len(images)\n    while True:  # Loop forever\n        for i in range(0, num_samples, batch_size):\n            batch_images = images[i:i + batch_size]\n            batch_masks = masks[i:i + batch_size]\n            yield np.array(batch_images), np.array(batch_masks)\n\n\n# Train the model using the data generator\ntrain_gen = data_generator(train_images, train_masks, batch_size=16)\ntest_gen = data_generator(test_images, test_masks, batch_size=16)\n\n# # Fit the model\n# model.fit(\n#     train_gen, \n#     steps_per_epoch=len(train_images) // 16,  # Number of batches per epoch\n#     epochs=10, \n#     validation_data=test_gen, \n#     validation_steps=len(test_images) // 16  # Number of validation batches\n# )\n\n# # Evaluate the model on the test set\n# test_loss, test_acc = model.evaluate(test_gen, steps=len(test_images) // 16)\n# print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import layers, models\n\n# Path to the directory where .npy files are stored\nfolder = '/kaggle/input/processed-brats2020-image-mask-npy/data'\n\n# Get the image and mask file paths\nimage_files = sorted([f for f in os.listdir(folder) if f.startswith('image_')])\nmask_files = sorted([f for f in os.listdir(folder) if f.startswith('mask_')])\n\n# Ensure the number of image files and mask files match\nassert len(image_files) == len(mask_files), \"Mismatch between image and mask file counts.\"\n\n# Get full paths to the files\nimage_paths = [os.path.join(folder, f) for f in image_files]\nmask_paths = [os.path.join(folder, f) for f in mask_files]\n\n# Split the dataset into train and test sets (e.g., 80% training, 20% testing)\ntrain_image_paths, test_image_paths, train_mask_paths, test_mask_paths = train_test_split(\n    image_paths, mask_paths, test_size=0.2, random_state=42\n)\n\n# Memory-mapping function to load images and masks (using mmap_mode='r' for efficient memory loading)\ndef load_mmap_images_and_masks(image_paths, mask_paths):\n    # Load images and masks as memory-mapped arrays\n    images = [np.load(img_path, mmap_mode='r') for img_path in image_paths]\n    masks = [np.load(mask_path, mmap_mode='r') for mask_path in mask_paths]\n    return images, masks\n\n# Generator function (loading images in batches)\ndef data_generator(image_paths, mask_paths, batch_size=4):\n    def generator():\n        images, masks = load_mmap_images_and_masks(image_paths, mask_paths)\n        for i in range(0, len(images), batch_size):\n            batch_images = images[i:i + batch_size]\n            batch_masks = masks[i:i + batch_size]\n            batch_images = np.array(batch_images).reshape(-1, 128, 128, 64, 4)\n            batch_masks = np.array(batch_masks).reshape(-1, 128, 128, 64,4)\n            yield batch_images, batch_masks\n\n    return tf.data.Dataset.from_generator(generator, \n                                           output_signature=(\n                                               tf.TensorSpec(shape=(None, 128, 128, 64, 4), dtype=tf.float32),\n                                               tf.TensorSpec(shape=(None, 128, 128, 64,4), dtype=tf.uint8)\n                                           ))\n\n# Assuming `train_image_paths`, `train_mask_paths`, `test_image_paths`, `test_mask_paths`\ntrain_dataset = data_generator(train_image_paths, train_mask_paths, batch_size=4)\ntest_dataset = data_generator(test_image_paths, test_mask_paths, batch_size=4)\n\ntrain_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\ntest_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n\n# # Model architecture (simple example)\n# def build_model(input_shape, num_classes):\n#     inputs = layers.Input(shape=input_shape)\n#     x = layers.Conv3D(32, (3, 3, 3), activation='relu', padding='same')(inputs)\n#     x = layers.MaxPooling3D((2, 2, 2))(x)\n#     x = layers.Conv3D(64, (3, 3, 3), activation='relu', padding='same')(x)\n#     x = layers.MaxPooling3D((2, 2, 2))(x)\n#     x = layers.Conv3D(128, (3, 3, 3), activation='relu', padding='same')(x)\n#     x = layers.GlobalAveragePooling3D()(x)\n#     x = layers.Dense(64, activation='relu')(x)\n#     outputs = layers.Dense(num_classes, activation='softmax')(x)\n\n#     model = models.Model(inputs, outputs)\n#     return model\n\n# # Define input shape and number of classes\n# input_shape = (128, 128, 64, 4)  # (Height, Width, Depth, Channels)\n# num_classes = 4  # Number of segmentation classes\n\n# # Build the model\n# model = build_model(input_shape, num_classes)\n\n# # Compile the model\n# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# # Train the model using the data generator\n# model.fit(\n#     train_dataset,\n#     epochs=10,\n#     validation_data=test_dataset\n# )\n\n# # Evaluate the model on the test set\n# test_loss, test_acc = model.evaluate(test_dataset)\n# print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test the generator directly before passing it to the model\ndef test_generator(image_paths, mask_paths, batch_size=4):\n    images, masks = load_mmap_images_and_masks(image_paths, mask_paths)\n    \n    for i in range(0, len(images), batch_size):\n        batch_images = images[i:i + batch_size]\n        batch_masks = masks[i:i + batch_size]\n\n        # Debugging: Check the shape of the current batch\n        print(f\"Batch shape (images): {np.array(batch_images).shape}, Batch shape (masks): {np.array(batch_masks).shape}\")\n\n        batch_images = np.array(batch_images).reshape(-1, 128, 128, 64, 4)\n        batch_masks = np.array(batch_masks).reshape(-1, 128, 128, 64,4)\n\n        print(f\"Reshaped batch shape (images): {batch_images.shape}, Reshaped batch shape (masks): {batch_masks.shape}\")\n        break  # Run just for the first batch\n\n# Test a few batches from the train set\ntest_generator(train_image_paths, train_mask_paths, batch_size=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nimport tensorflow as tf\n\ndef data_generator(image_files, mask_files, folder, batch_size=4):\n    \"\"\"\n    A generator that loads and yields batches of images and masks from .npy files.\n    This ensures that only the current batch is in memory at any time, helping with memory usage.\n    \"\"\"\n    while True:  # Loop indefinitely so the generator can be used in training\n        # Shuffle the image-mask pairs at the start of each epoch\n        indices = np.random.permutation(len(image_files))\n        image_files = [image_files[i] for i in indices]\n        mask_files = [mask_files[i] for i in indices]\n        \n        for start in range(0, len(image_files), batch_size):\n            end = min(start + batch_size, len(image_files))\n            \n            batch_images = []\n            batch_masks = []\n            \n            # Load the batch of images and masks\n            for i in range(start, end):\n                img_path = os.path.join(folder, image_files[i])\n                mask_path = os.path.join(folder, mask_files[i])\n                \n                # Load the .npy files\n                image = np.load(img_path)\n                mask = np.load(mask_path)\n                \n                batch_images.append(image)\n                batch_masks.append(mask)\n            \n            # Convert the batch list to numpy arrays\n            yield np.array(batch_images), np.array(batch_masks)\n\n# Example: loading data in batches using this generator\ndef load_data(folder, batch_size=4):\n    \"\"\"\n    This function loads image and mask filenames, and then creates a data generator for lazy loading.\n    \"\"\"\n    # List all .npy files in the image and mask folders\n    image_files = sorted([f for f in os.listdir(folder) if f.startswith('image')])\n    mask_files = sorted([f for f in os.listdir(folder) if f.startswith('mask')])\n\n    # Create a data generator\n    return data_generator(image_files, mask_files, folder, batch_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n\n# Define a simple debug callback to ensure logs are not None\n# class DebugCallback(tf.keras.callbacks.Callback):\n#     def on_epoch_end(self, epoch, logs=None):\n#         if logs is None:\n#             print(f\"Logs at the end of epoch {epoch} is None.\")\n#         else:\n#             print(f\"Logs at the end of epoch {epoch}: {logs}\")\n# # Train the model\n# history = model_1.fit(train_dataset, validation_data=test_dataset, epochs=500, callbacks=[early_stopping, reduce_lr,DebugCallback()])\n\n# Using the generator for training a model\ndef train_model(model, folder, batch_size=4, epochs=500):\n    # Create the data generator\n    train_generator = load_data(folder, batch_size)\n    # Train the model using the generator\n    history = model.fit(train_generator, epochs=epochs, steps_per_epoch=50, callbacks=[early_stopping, reduce_lr])\n    return history\n\nhistory = train_model(model_1,\"/kaggle/input/processed-brats2020-image-mask-npy/data\", 4, 500)\n\n# Example usage with a model\n# model = your_model_function()  # Replace with your actual model\n# image_folder = 'path/to/images'  # Path to images folder\n# mask_folder = 'path/to/masks'    # Path to masks folder\n\n# train_model(model, image_folder, mask_folder, batch_size=4, epochs=10)\n\n# # Evaluate on test set\n# test_metrics = model_1.evaluate(X_test, y_test, batch_size=2)\n# print(f\"Test IoU: {test_metrics[1]:.4f}, Test Pixel Accuracy: {test_metrics[2]:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\ndef data_generator(file_names, folder, batch_size=4):\n    \"\"\"\n    A generator that loads and yields batches of images and masks from .npy files.\n    This ensures that only the current batch is in memory at any time, helping with memory usage.\n    \"\"\"\n    while True:  # Loop indefinitely so the generator can be used in training\n        # Shuffle the file names at the start of each epoch\n        indices = np.random.permutation(len(file_names))\n        file_names = [file_names[i] for i in indices]\n        \n        for start in range(0, len(file_names), batch_size):\n            end = min(start + batch_size, len(file_names))\n            \n            batch_images = []\n            batch_masks = []\n            \n            # Load the batch of images and masks\n            for i in range(start, end):\n                # Get image and mask file paths\n                img_path = os.path.join(folder, f'image_{file_names[i]}.npy')\n                mask_path = os.path.join(folder, f'mask_{file_names[i]}.npy')\n                \n                # Load the .npy files\n                image = np.load(img_path)\n                mask = np.load(mask_path)\n                \n                batch_images.append(image)\n                batch_masks.append(mask)\n            \n            # Convert the batch list to numpy arrays\n            yield np.array(batch_images), np.array(batch_masks)\n\n# Example: loading data in batches using this generator\ndef load_data(folder, batch_size=4):\n    \"\"\"\n    This function loads image and mask filenames, and then creates a data generator for lazy loading.\n    \"\"\"\n    # List all image and mask files (assuming images are named image_num.npy and masks are mask_num.npy)\n    image_files = sorted([f.split('_')[1].replace('.npy', '') for f in os.listdir(folder) if 'image' in f])\n    \n    # Create a data generator\n    return data_generator(image_files, folder, batch_size)\n\n# Using the generator for training a model\ndef train_model(model, folder, batch_size=4, epochs=10):\n    # Create the data generator\n    train_generator = load_data(folder, batch_size)\n    \n    # Define callbacks\n    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n    \n    # Train the model using the generator\n    history = model.fit(\n        train_generator, \n        epochs=epochs, \n        steps_per_epoch=100,  # Adjust this based on your dataset size\n        callbacks=[early_stopping, reduce_lr]\n    )\n    return history\n\n# Example usage with a model\nfolder = '/kaggle/input/processed-brats2020-image-mask-npy/data'  # Path to your folder with image_num.npy and mask_num.npy files\n\n# Instantiate the model\nmodel_1 = swin_3d_unetr(input_shape=(128, 128, 64, 4), num_classes=4)\nmodel_1.compile(optimizer='adamw', loss=combined_loss, metrics=[iou_metric, pixel_accuracy])\n#print(model_1.summary())\n\n# Start training the model\ntrain_model(model_1, folder, batch_size=4, epochs=10)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\n\ndef parse_file(file_name, folder):\n    \"\"\"\n    Parse a single image and mask file from a given folder.\n    \"\"\"\n    # Extract the number from the filename\n    num = file_name.split('_')[1].split('.')[0]\n\n    # Construct file paths for the image and corresponding mask\n    img_path = tf.strings.join([folder, f'image_{num}.npy'])\n    mask_path = tf.strings.join([folder, f'mask_{num}.npy'])\n\n    # Load and return the image and mask\n    def load_npy(file_path):\n        return np.load(file_path.numpy())\n\n    image = tf.numpy_function(load_npy, [img_path], tf.float32)\n    mask = tf.numpy_function(load_npy, [mask_path], tf.uint8)\n\n    # Ensure the shapes are static\n    image = tf.ensure_shape(image, (128, 128, 64, 4))\n    mask = tf.ensure_shape(mask, (128, 128, 64, 1))\n\n    return image, mask\n\ndef load_dataset(folder, batch_size):\n    \"\"\"\n    Create a TensorFlow Dataset for lazy loading.\n    \"\"\"\n    # Get all unique image files based on 'image_' pattern\n    file_names = [f for f in os.listdir(folder) if f.startswith('image_')]\n\n    # Create a dataset from the file names\n    dataset = tf.data.Dataset.from_tensor_slices(file_names)\n\n    # Map the parse_file function to load images and masks\n    dataset = dataset.map(\n        lambda file_name: parse_file(file_name, folder),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n\n    # Batch, shuffle, and prefetch for optimal performance\n    dataset = dataset.batch(batch_size).shuffle(buffer_size=100).prefetch(tf.data.AUTOTUNE)\n    \n    return dataset\n\n\n# Training pipeline\ndef train_model(folder, batch_size=4, epochs=10):\n    # Create the dataset\n    train_dataset = load_dataset(folder, batch_size=batch_size)\n    model_1.compile(optimizer='adamw', loss=combined_loss, metrics=[iou_metric, pixel_accuracy])\n    # Define callbacks\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n\n    # Train the model\n    history = model_1.fit(\n        train_dataset,\n        epochs=epochs,\n        steps_per_epoch=10,  # Adjust based on dataset size\n        callbacks=[early_stopping, reduce_lr]\n    )\n    \n    return history\n\n# Folder containing image and mask .npy files\nfolder = '/kaggle/input/processed-brats2020-image-mask-npy/data'\n\n# Train the model\ntrain_model(folder, batch_size=4, epochs=500)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef shifted_attention(x, embed_dim, num_heads, window_size, shift=False):\n    \"\"\"\n    Computes Shifted Window Multi-Head Self-Attention (SW-MSA) or W-MSA.\n    \"\"\"\n    if shift:\n        # Manual shift using slicing and concatenation (Keras-compatible)\n        shift_dim = window_size // 2\n        shifted_x = Concatenate(axis=1)([x[:, -shift_dim:], x[:, :-shift_dim]])\n    else:\n        shifted_x = x  # No shift for W-MSA\n\n    # Apply Multi-Head Attention\n    attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(shifted_x, shifted_x)\n    return attention\n\n\ndef swin_transformer_block(x, embed_dim, num_heads, window_size, shift=False):\n    \"\"\"\n    Swin Transformer Block: Includes W-MSA, SW-MSA, MLP, and skip connections.\n    \"\"\"\n    # Step 1: Layer Normalization and W-MSA or SW-MSA\n    x_norm1 = LayerNormalization()(x)\n    attention_output = shifted_attention(x_norm1, embed_dim, num_heads, window_size, shift)\n    x_skip1 = Add()([x, attention_output])  # First skip connection\n\n    # Step 2: Layer Normalization and MLP\n    x_norm2 = LayerNormalization()(x_skip1)\n    mlp1 = Dense(embed_dim * 4, activation=\"gelu\")(x_norm2)\n    mlp1 = Dense(embed_dim)(mlp1)\n    x_skip2 = Add()([x_skip1, mlp1])  # Second skip connection\n\n    return x_skip2\n\n\ndef swin_3d_unetr(input_shape, num_classes):\n    \"\"\"\n    Swin 3D UNETR model with hierarchical Swin Transformer embedding blocks and decoder.\n    \"\"\"\n    inputs = Input(input_shape)\n\n    # Encoder with Swin Transformer Blocks\n    embed_1 = patch_embedding(inputs, embed_dim=48)  # Stage 1\n    swin_1 = swin_transformer_block(embed_1, embed_dim=48, num_heads=3, window_size=7, shift=False)\n\n    embed_2 = patch_embedding(swin_1, embed_dim=96)  # Stage 2\n    swin_2 = swin_transformer_block(embed_2, embed_dim=96, num_heads=6, window_size=7, shift=True)\n\n    embed_3 = patch_embedding(swin_2, embed_dim=192)  # Stage 3\n    swin_3 = swin_transformer_block(embed_3, embed_dim=192, num_heads=12, window_size=7, shift=False)\n\n    embed_4 = patch_embedding(swin_3, embed_dim=384)  # Stage 4\n    swin_4 = swin_transformer_block(embed_4, embed_dim=384, num_heads=24, window_size=7, shift=True)\n\n    # Decoder\n    dec_4 = decoder_block(swin_4, swin_3, embed_dim=192)\n    dec_3 = decoder_block(dec_4, swin_2, embed_dim=96)\n    dec_2 = decoder_block(dec_3, swin_1, embed_dim=48)\n    dec_1 = decoder_block(dec_2, inputs, embed_dim=48)  # Final decoder with direct input\n\n    # Output\n    outputs = Conv3D(num_classes, kernel_size=1, activation=\"softmax\")(dec_1)\n\n    return tf.keras.Model(inputs, outputs)\n\n# Utility Functions: Patch Embedding and Decoder Block\ndef patch_embedding(x, embed_dim):\n    \"\"\"\n    Patch partitioning for downsampling.\n    \"\"\"\n    x = Conv3D(embed_dim, kernel_size=3, strides=2, padding=\"same\")(x)\n    return x\n\ndef decoder_block(x, skip, embed_dim):\n    \"\"\"\n    Decoder block with upsampling and skip connections.\n    \"\"\"\n    x = UpSampling3D(size=(2, 2, 2))(x)\n    x = Concatenate()([x, skip])\n    x = Conv3D(embed_dim, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n    return x\n\n# Instantiate the Model\ninput_shape = (128, 128, 128, 4)  # Input volume dimensions\nnum_classes = 3  # Number of segmentation classes\nmodel = swin_3d_unetr(input_shape, num_classes)\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"3DSwinUnetr.h5\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def swin_transformer_block(x, embed_dim, num_heads, window_size, shift=False):\n    \"\"\"\n    Swin Transformer Block: Implements the correct sequence with concatenated skip connections.\n\n    Sequence:\n    1) Layer Norm\n    2) W-MSA\n    3) Skip connection (concat with input before Layer Norm)\n    4) Layer Norm\n    5) MLP\n    6) Skip connection (concat with input before Layer Norm)\n    7) Layer Norm\n    8) SW-MSA\n    9) Skip connection (concat with input before Layer Norm)\n    10) Layer Norm\n    11) MLP\n    12) Skip connection (concat with input before Layer Norm)\n    \"\"\"\n\n    # Step 1: Layer Normalization + W-MSA\n    ln1 = LayerNormalization()(x)\n    w_msa = shifted_attention(ln1, embed_dim, num_heads, window_size, shift=False)\n    skip1 = Concatenate()([x, w_msa])  # Skip connection with input\n\n    # Step 2: Layer Normalization + MLP\n    ln2 = LayerNormalization()(skip1)\n    mlp1 = Dense(embed_dim * 4, activation=\"gelu\")(ln2)\n    mlp1 = Dense(embed_dim)(mlp1)\n    skip2 = Concatenate()([skip1, mlp1])  # Skip connection with input\n\n    # Step 3: Layer Normalization + SW-MSA\n    ln3 = LayerNormalization()(skip2)\n    sw_msa = shifted_attention(ln3, embed_dim, num_heads, window_size, shift=True)\n    skip3 = Concatenate()([skip2, sw_msa])  # Skip connection with input\n\n    # Step 4: Layer Normalization + MLP\n    ln4 = LayerNormalization()(skip3)\n    mlp2 = Dense(embed_dim * 4, activation=\"gelu\")(ln4)\n    mlp2 = Dense(embed_dim)(mlp2)\n    skip4 = Concatenate()([skip3, mlp2])  # Skip connection with input\n\n    return skip4\n\n\ndef shifted_attention(x, embed_dim, num_heads, window_size, shift=False):\n    \"\"\"\n    Computes Shifted Window Multi-Head Self-Attention (SW-MSA) or W-MSA.\n    \"\"\"\n    if shift:\n        # Manual shift using slicing and concatenation (Keras-compatible)\n        shift_dim = window_size // 2\n        shifted_x = Concatenate(axis=1)([x[:, -shift_dim:], x[:, :-shift_dim]])\n    else:\n        shifted_x = x  # No shift for W-MSA\n\n    # Apply Multi-Head Attention\n    attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(shifted_x, shifted_x)\n    return attention\n\n\ndef patch_embedding(x, embed_dim):\n    \"\"\"\n    Patch partitioning for downsampling.\n    \"\"\"\n    x = Conv3D(embed_dim, kernel_size=3, strides=2, padding=\"same\")(x)\n    return x\n\n\ndef swin_3d_unetr(input_shape, num_classes):\n    \"\"\"\n    Swin 3D UNETR model with hierarchical Swin Transformer embedding blocks and decoder.\n    \"\"\"\n    inputs = Input(input_shape)\n\n    # Encoder with Swin Transformer Blocks\n    embed_1 = patch_embedding(inputs, embed_dim=48)  # Stage 1\n    swin_1 = swin_transformer_block(embed_1, embed_dim=48, num_heads=3, window_size=7, shift=False)\n\n    embed_2 = patch_embedding(swin_1, embed_dim=96)  # Stage 2\n    swin_2 = swin_transformer_block(embed_2, embed_dim=96, num_heads=6, window_size=7, shift=True)\n\n    embed_3 = patch_embedding(swin_2, embed_dim=192)  # Stage 3\n    swin_3 = swin_transformer_block(embed_3, embed_dim=192, num_heads=12, window_size=7, shift=False)\n\n    embed_4 = patch_embedding(swin_3, embed_dim=384)  # Stage 4\n    swin_4 = swin_transformer_block(embed_4, embed_dim=384, num_heads=24, window_size=7, shift=True)\n\n    # Decoder\n    dec_4 = decoder_block(swin_4, swin_3, embed_dim=192)\n    dec_3 = decoder_block(dec_4, swin_2, embed_dim=96)\n    dec_2 = decoder_block(dec_3, swin_1, embed_dim=48)\n    dec_1 = decoder_block(dec_2, inputs, embed_dim=48)  # Final decoder with direct input\n\n    # Output\n    outputs = Conv3D(num_classes, kernel_size=1, activation=\"softmax\")(dec_1)\n\n    return tf.keras.Model(inputs, outputs)\n\n\ndef decoder_block(x, skip, embed_dim):\n    \"\"\"\n    Decoder block with upsampling and skip connections.\n    \"\"\"\n    x = UpSampling3D(size=(2, 2, 2))(x)\n    x = Concatenate()([x, skip])\n    x = Conv3D(embed_dim, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n    return x\n\n\n# Define input shape and number of classes\ninput_shape = (128, 128, 128, 4)  # Example 3D volume dimensions\nnum_classes = 3  # Example number of segmentation classes\n\n# Instantiate the model\nmodel = swin_3d_unetr(input_shape, num_classes)\nmodel.summary()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"3DSwinUnetr_2.h5\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"swin_3d_unetr_final.h5\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport nibabel as nib\n\ndef resize_3d_image(img, target_shape):\n    \"\"\"\n    Resize a 3D numpy image using linear interpolation along each axis.\n    Args:\n    - img: 3D numpy array (H, W, D)\n    - target_shape: Desired target shape (H, W, D)\n    Returns:\n    - Resized 3D numpy array\n    \"\"\"\n    orig_shape = np.array(img.shape)\n    target_shape = np.array(target_shape)\n\n    # Create coordinate grids for original and target shapes\n    coords_orig = [np.linspace(0, orig_shape[i] - 1, orig_shape[i]) for i in range(3)]\n    coords_target = [np.linspace(0, orig_shape[i] - 1, target_shape[i]) for i in range(3)]\n\n    # Create a meshgrid for target shape\n    meshgrids = np.meshgrid(*coords_target, indexing='ij')\n\n    # Interpolate each axis individually\n    resized_img = np.zeros(target_shape, dtype=img.dtype)\n    for i in range(target_shape[0]):\n        for j in range(target_shape[1]):\n            for k in range(target_shape[2]):\n                # Get the original coordinates for the interpolation\n                orig_x = np.interp(meshgrids[0][i, j, k], coords_orig[0], np.arange(orig_shape[0]))\n                orig_y = np.interp(meshgrids[1][i, j, k], coords_orig[1], np.arange(orig_shape[1]))\n                orig_z = np.interp(meshgrids[2][i, j, k], coords_orig[2], np.arange(orig_shape[2]))\n\n                # Use the interpolated coordinates to get the pixel values\n                resized_img[i, j, k] = img[int(orig_x), int(orig_y), int(orig_z)]\n\n    return resized_img\n\n\ndef load_nii_image(file_path, normalize=True, roi_slice=(slice(30, 210), slice(30, 210), slice(20, 130))):\n    \"\"\"\n    Load a NIfTI image, extract ROI, normalize and resize to (128, 128, 64).\n    \"\"\"\n    img = nib.load(file_path).get_fdata()  # Load the image\n    img = img[roi_slice]  # Extract ROI\n    \n    # Normalize image\n    if normalize:\n        img = (img - np.min(img)) / (np.max(img) - np.min(img))  # Normalize to [0, 1]\n        \n    img_resized = resize_3d_image(img, target_shape=(128, 128, 64))  # Resize the image\n\n    return img_resized\n\ndef load_nii_mask(file_path, roi_slice=(slice(30, 210), slice(30, 210), slice(20, 130))):\n    \"\"\"\n    Load a NIfTI mask, extract ROI, resize to (128, 128, 64), and convert to categorical.\n    \"\"\"\n    mask = nib.load(file_path).get_fdata()  # Load the mask\n    mask = mask[roi_slice]  # Extract ROI\n    \n    # Resize the mask\n    mask_resized = resize_3d_image(mask, target_shape=(128, 128, 64))\n    mask = mask_resized.astype(np.uint8)\n    mask[mask==4]=3\n    mask = to_categorical(mask, num_classes=4)  # Convert to categorical\n    \n    return mask","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_nii_image(file_path, normalize=True, roi_slice=(slice(30, 210), slice(30, 210), slice(20, 130))):\n    \"\"\"\n    Load a NIfTI file, normalize its pixel values, extract ROI, and resize to (128, 128, 64).\n    \"\"\"\n    img = nib.load(file_path).get_fdata()  # Load the image\n    img = img[roi_slice]  # Extract ROI\n    if normalize:\n        img = (img - np.min(img)) / (np.max(img) - np.min(img))  # Normalize to [0, 1]\n    img = tf.image.resize(img, size=(128, 128)).numpy()  # Resize height and width\n    img = tf.image.resize(img, size=(64,), method=\"nearest\", preserve_aspect_ratio=False, axis=2).numpy()  # Resize depth\n    return img\n\n\ndef load_nii_mask(file_path, roi_slice=(slice(30, 210), slice(30, 210), slice(20, 130))):\n    \"\"\"\n    Load a NIfTI mask, extract ROI, resize to (128, 128, 64), and convert to categorical.\n    \"\"\"\n    mask = nib.load(file_path).get_fdata()  # Load the mask\n    mask = mask[roi_slice]  # Extract ROI\n    mask = tf.image.resize(mask, size=(128, 128), method='nearest').numpy()  # Resize height and width\n    mask = tf.image.resize(mask, size=(64,), method=\"nearest\", preserve_aspect_ratio=False, axis=2).numpy()  # Resize depth\n    mask = mask.astype(np.uint8)\n    mask[mask==4]=3\n    mask = to_categorical(mask, num_classes=4)  # Convert to categorical\n    return mask","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_data(t1_list, t2_list, t1ce_list, flair_list, mask_list):\n    \"\"\"\n    Preprocess all images and masks.\n    \"\"\"\n    images = []\n    masks = []\n    \n    for t1, t2, t1ce, flair, mask in zip(t1_list, t2_list, t1ce_list, flair_list, mask_list):\n        t1_img = load_nii_image(t1)\n        t2_img = load_nii_image(t2)\n        t1ce_img = load_nii_image(t1ce)\n        flair_img = load_nii_image(flair)\n        mask_img = load_nii_mask(mask)\n\n        # Combine modalities into a single 4D image\n        combined_img = np.stack([t1_img, t2_img, t1ce_img, flair_img], axis=-1)\n        \n        images.append(combined_img)\n        masks.append(mask_img)\n    \n    return np.array(images), np.array(masks)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def split_dataset(images, masks, train_ratio=0.65, val_ratio=0.15, test_ratio=0.20):\n    \"\"\"\n    Split images and masks into training, validation, and test sets.\n    \"\"\"\n    X_train, X_temp, y_train, y_temp = train_test_split(images, masks, test_size=1 - train_ratio, random_state=42)\n    val_split = val_ratio / (val_ratio + test_ratio)\n    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=1 - val_split, random_state=42)\n    return X_train, X_val, X_test, y_train, y_val, y_test","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def remove_folder_contents(folder):\n    for the_file in os.listdir(folder):\n        file_path = os.path.join(folder, the_file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                remove_folder_contents(file_path)\n                os.rmdir(file_path)\n        except Exception as e:\n            print(e)\n\nfolder_path = '/kaggle/working/'\nremove_folder_contents(folder_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Path to the directory containing your .npy files\ndata_dir = \"/kaggle/working/\"  # Adjust this path if needed\noutput_zip = \"/kaggle/working/preprocessed_data.zip\"  # Path for the output zip file\n\n# Create a zip archive of the entire folder\nshutil.make_archive(base_name=output_zip.replace(\".zip\", \"\"), format='zip', root_dir=data_dir)\n\nprint(f\"Data zipped successfully at {output_zip}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nimport tensorflow as tf\n\ndef load_all_npy_files_from_single_folder(folder):\n    \"\"\"\n    Load all .npy files from a folder, assuming image and mask files have corresponding names with prefixes 'image_' and 'mask_'.\n\n    Parameters:\n    - folder: str, path to the folder containing both image and mask files with corresponding names.\n\n    Returns:\n    - images: numpy array, loaded images (shape: [num_samples, height, width, depth, num_modalities]).\n    - masks: numpy array, loaded masks (shape: [num_samples, height, width, depth, num_classes]).\n    \"\"\"\n    # List all .npy files in the directory\n    files = sorted([f for f in os.listdir(folder) if f.endswith('.npy')])\n\n    # Separate files into images and masks based on the prefix\n    image_files = sorted([f for f in files if f.startswith('image_')])\n    mask_files = sorted([f for f in files if f.startswith('mask_')])\n\n    images = []\n    masks = []\n\n    # Process each pair of image and mask files\n    for img_file, mask_file in zip(image_files, mask_files):\n        img_path = os.path.join(folder, img_file)\n        mask_path = os.path.join(folder, mask_file)\n\n        # Load the image and mask directly into memory\n        image = np.load(img_path)\n        mask = np.load(mask_path)\n\n        # Append to the list\n        images.append(image)\n        masks.append(mask)\n\n    # Convert the lists into numpy arrays\n    images = np.array(images)\n    masks = np.array(masks)\n    masks = masks.astype(np.uint8)\n\n    return images, masks\n\n# Define the folder containing both images and masks\nfolder = \"/kaggle/input/processed-brats2020-image-mask-npy/data\"\n\n# Load all .npy files into RAM\nimages, masks = load_all_npy_files_from_single_folder(folder)\n\n# Check the shape of the loaded data\nprint(\"Images shape:\", images.shape)\nprint(\"Masks shape:\", masks.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images.dtype","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.unique(masks[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nplt.figure(figsize=(12,8))\n# plt.subplot(231)\nmask = masks[0,:,:,30]\nplt.imshow(mask)\n# plt.subplot(232)\n# plt.imshow(masks[0,:,:,30,1],cmap='gray')\n# plt.subplot(233)\n# plt.imshow(masks[0,:,:,30,2],cmap='gray')\n# plt.subplot(234)\n# plt.imshow(masks[0,:,:,30,3],cmap='gray')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\n\ndef load_images_and_masks(image_paths, mask_paths):\n    # Memory-mapping images and masks to avoid loading all into memory\n    images = [np.load(img_path, mmap_mode='r') for img_path in image_paths]\n    masks = [np.load(mask_path, mmap_mode='r') for mask_path in mask_paths]\n    \n    return images, masks\n\n# Folder containing the .npy files\nfolder = '/kaggle/input/processed-brats2020-image-mask-npy/data'\n\n# Assuming the files are named like 'image_0.npy', 'mask_0.npy' etc.\nimage_files = sorted([f for f in os.listdir(folder) if f.startswith('image_')])\nmask_files = sorted([f for f in os.listdir(folder) if f.startswith('mask_')])\n\n# Ensure you have the correct number of files\nassert len(image_files) == len(mask_files), \"Mismatch between number of image and mask files!\"\n\n# Get full paths to the files\nimage_paths = [os.path.join(folder, f) for f in image_files]\nmask_paths = [os.path.join(folder, f) for f in mask_files]\n\n# Load images and masks with memory-mapping\nimages, masks = load_images_and_masks(image_paths, mask_paths)\n\n# Example usage: access the first image and mask\nfirst_image = images[0]  # This is a memory-mapped array, it is not loaded fully into RAM\nfirst_mask = masks[0]\n\nprint(f\"Shape of first image: {first_image.shape}\")\nprint(f\"Shape of first mask: {first_mask.shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(masks)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, LayerNormalization, Conv3D, UpSampling3D, Concatenate, Add\nimport numpy as np\nimport nibabel as nib\nimport glob\nimport matplotlib.pyplot as plt\nfrom tifffile import imsave\nfrom tqdm import tqdm\nimport os\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom skimage.transform import resize\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T02:33:57.482888Z","iopub.execute_input":"2024-11-26T02:33:57.483534Z","iopub.status.idle":"2024-11-26T02:33:57.489726Z","shell.execute_reply.started":"2024-11-26T02:33:57.483503Z","shell.execute_reply":"2024-11-26T02:33:57.488883Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import tensorflow as tf\n\n# Get the list of available GPUs\ngpus = tf.config.list_physical_devices('GPU')\n\n# If there are GPUs available, set memory growth and limit memory usage\nif gpus:\n    try:\n        # Set memory growth to True (if you want dynamic memory allocation)\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n\n        # # Set memory limit (for example, limit to 4GB)\n        # memory_limit = 12288  # in MB\n        # tf.config.experimental.set_virtual_device_configuration(\n        #     gpus[0],\n        #     [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit)]\n        # )\n        # print(f\"GPU memory cap set to {memory_limit}MB\")\n        \n    except RuntimeError as e:\n        # Catch and print the error if GPU configuration fails\n        print(f\"Error: {e}\")\nelse:\n    print(\"No GPU found\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T02:34:02.289087Z","iopub.execute_input":"2024-11-26T02:34:02.289476Z","iopub.status.idle":"2024-11-26T02:34:02.294999Z","shell.execute_reply.started":"2024-11-26T02:34:02.289445Z","shell.execute_reply":"2024-11-26T02:34:02.294140Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def swin_transformer_block(x, embed_dim, num_heads, window_size, shift=True):\n    \"\"\"\n    Swin Transformer Block with residual connections.\n    \"\"\"\n    ln1 = LayerNormalization()(x)\n    sw_msa = shifted_attention(ln1, embed_dim, num_heads, window_size, shift=True)\n    skip1 = Add()([x, sw_msa])  # Residual connection\n\n    ln2 = LayerNormalization()(skip1)\n    mlp1 = Dense(embed_dim , activation=\"relu\")(ln2)\n    skip2 = Add()([skip1, mlp1])  # Residual connection\n\n    return skip1\n\n\ndef shifted_attention(x, embed_dim, num_heads, window_size, shift=False):\n    \"\"\"\n    Computes Shifted Window Multi-Head Self-Attention (SW-MSA) or W-MSA.\n    \"\"\"\n    if shift:\n        shift_dim = window_size // 2\n        shifted_x = Concatenate(axis=1)([x[:, -shift_dim:], x[:, :-shift_dim]])\n    else:\n        shifted_x = x\n\n    attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(shifted_x, shifted_x)\n    return attention\n\n\ndef patch_embedding(x, embed_dim):\n    \"\"\"\n    Patch partitioning for downsampling.\n    \"\"\"\n    x = Conv3D(embed_dim, kernel_size=3, strides=2, padding=\"same\")(x)\n    return x\n\n\ndef decoder_block(x, skip, embed_dim, is_bottleneck=False):\n    \"\"\"\n    Decoder block with upsampling and skip connections.\n    If `is_bottleneck=True`, no concatenation is done, only conv layers.\n    \"\"\"\n    # First, upsample `x` to match the spatial size of `skip`\n    if x.shape[1] != skip.shape[1] or x.shape[2] != skip.shape[2] or x.shape[3] != skip.shape[3]:\n        x = UpSampling3D(size=(2, 2, 2))(x)  # Upsample by a factor of 2\n    \n    # Ensure the dimensions match before concatenation (if necessary, apply Conv3D)\n    if x.shape[1] != skip.shape[1] or x.shape[2] != skip.shape[2] or x.shape[3] != skip.shape[3]:\n        x = Conv3D(embed_dim, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n\n    # Concatenate the upsampled `x` with the `skip` connection\n    x = Concatenate()([x, skip])\n\n    # Apply a Conv3D layer after concatenation to refine the features\n    x = Conv3D(embed_dim, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n    \n    return x\n\n\ndef swin_3d_unetr(num_classes):\n    \"\"\"\n    Swin 3D UNETR model with hierarchical Swin Transformer embedding blocks and decoder.\n    \"\"\"\n    inputs = Input(shape=(32,32, 16, 4))  # Fix spatial dimensions and number of channels\n\n    # Encoder\n    enc1 = inputs  # Level 1: Input image\n    embed_1 = patch_embedding(enc1, embed_dim=4)  # Level 2\n    swin_1 = swin_transformer_block(embed_1, embed_dim=4, num_heads=7, window_size=4, shift=True)  # Level 3\n\n    embed_2 = patch_embedding(swin_1, embed_dim=4)  # Level 4\n    swin_2 = swin_transformer_block(embed_2, embed_dim=4, num_heads=14, window_size=4, shift=True)  # Level 5\n\n    embed_3 = patch_embedding(swin_2, embed_dim=4)  # Level 6\n    swin_3 = swin_transformer_block(embed_3, embed_dim=4, num_heads=21, window_size=4, shift=True)  # Level 7\n\n\n    # Decoder\n    # dec_6 = decoder_block(swin_5, swin_4, embed_dim=768, is_bottleneck=True)  # Bottleneck decoder (no concat)\n    # dec_5 = decoder_block(dec_6, swin_3, embed_dim=384)\n    # dec_4 = decoder_block(dec_5, swin_2, embed_dim=192)\n    # dec_3 = decoder_block(dec_4, swin_1, embed_dim=96)\n    # dec_2 = decoder_block(dec_3, embed_1, embed_dim=48)\n    # dec_1 = decoder_block(dec_2, enc1, embed_dim=48)  # Level 1\n    dec_4 = decoder_block(swin_3, swin_2, embed_dim=4, is_bottleneck=True)\n    dec_3 = decoder_block(dec_4, swin_1, embed_dim=4)\n    dec_2 = decoder_block(dec_3, embed_1, embed_dim=4)\n    dec_1 = decoder_block(dec_2, enc1, embed_dim=4)  # Level 1\n\n    # Segmentation Head\n    # Output shape is (128, 128, 64) with num_classes\n    outputs = Conv3D(num_classes, kernel_size=1, activation=\"softmax\")(dec_1)\n\n    return tf.keras.Model(inputs, outputs)\n\n\n# Define input shape and number of classes\nnum_classes = 4  # Adjust for the number of classes\n\n# Instantiate the model\nmodel_1 = swin_3d_unetr(num_classes)\nprint(model_1.summary())\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T02:34:03.664443Z","iopub.execute_input":"2024-11-26T02:34:03.665275Z","iopub.status.idle":"2024-11-26T02:34:04.183964Z","shell.execute_reply.started":"2024-11-26T02:34:03.665242Z","shell.execute_reply":"2024-11-26T02:34:04.183152Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_5\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n\n input_layer_2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,              \u001b[38;5;34m0\u001b[0m  -                 \n (\u001b[38;5;33mInputLayer\u001b[0m)         \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m4\u001b[0m)                                           \n\n conv3d_16 (\u001b[38;5;33mConv3D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m8\u001b[0m,         \u001b[38;5;34m436\u001b[0m  input_layer_2[\u001b[38;5;34m0\u001b[0m] \n                      \u001b[38;5;34m4\u001b[0m)                                               \n\n layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m8\u001b[0m,           \u001b[38;5;34m8\u001b[0m  conv3d_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n (\u001b[38;5;33mLayerNormalizatio\u001b[0m  \u001b[38;5;34m4\u001b[0m)                                               \n\n get_item_12          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m8\u001b[0m,            \u001b[38;5;34m0\u001b[0m  layer_normalizat \n (\u001b[38;5;33mGetItem\u001b[0m)            \u001b[38;5;34m4\u001b[0m)                                               \n\n get_item_13          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m8\u001b[0m,           \u001b[38;5;34m0\u001b[0m  layer_normalizat \n (\u001b[38;5;33mGetItem\u001b[0m)            \u001b[38;5;34m4\u001b[0m)                                               \n\n concatenate_14       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m8\u001b[0m,           \u001b[38;5;34m0\u001b[0m  get_item_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)        \u001b[38;5;34m4\u001b[0m)                             get_item_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n\n multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m8\u001b[0m,         \u001b[38;5;34m536\u001b[0m  concatenate_14[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mMultiHeadAttentio\u001b[0m  \u001b[38;5;34m4\u001b[0m)                             concatenate_14[\u001b[38;5;34m0\u001b[0m \n\n add_12 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m8\u001b[0m,           \u001b[38;5;34m0\u001b[0m  conv3d_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  \n                      \u001b[38;5;34m4\u001b[0m)                             multi_head_atten \n\n conv3d_17 (\u001b[38;5;33mConv3D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m4\u001b[0m,           \u001b[38;5;34m436\u001b[0m  add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n                      \u001b[38;5;34m4\u001b[0m)                                               \n\n layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m4\u001b[0m,             \u001b[38;5;34m8\u001b[0m  conv3d_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n (\u001b[38;5;33mLayerNormalizatio\u001b[0m  \u001b[38;5;34m4\u001b[0m)                                               \n\n get_item_14          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m4\u001b[0m,             \u001b[38;5;34m0\u001b[0m  layer_normalizat \n (\u001b[38;5;33mGetItem\u001b[0m)            \u001b[38;5;34m4\u001b[0m)                                               \n\n get_item_15          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m4\u001b[0m,             \u001b[38;5;34m0\u001b[0m  layer_normalizat \n (\u001b[38;5;33mGetItem\u001b[0m)            \u001b[38;5;34m4\u001b[0m)                                               \n\n concatenate_15       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m4\u001b[0m,             \u001b[38;5;34m0\u001b[0m  get_item_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)        \u001b[38;5;34m4\u001b[0m)                             get_item_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n\n multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m4\u001b[0m,         \u001b[38;5;34m1,068\u001b[0m  concatenate_15[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mMultiHeadAttentio\u001b[0m  \u001b[38;5;34m4\u001b[0m)                             concatenate_15[\u001b[38;5;34m0\u001b[0m \n\n add_14 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m4\u001b[0m,             \u001b[38;5;34m0\u001b[0m  conv3d_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  \n                      \u001b[38;5;34m4\u001b[0m)                             multi_head_atten \n\n conv3d_18 (\u001b[38;5;33mConv3D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2\u001b[0m,           \u001b[38;5;34m436\u001b[0m  add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n                      \u001b[38;5;34m4\u001b[0m)                                               \n\n layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2\u001b[0m,             \u001b[38;5;34m8\u001b[0m  conv3d_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n (\u001b[38;5;33mLayerNormalizatio\u001b[0m  \u001b[38;5;34m4\u001b[0m)                                               \n\n get_item_16          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2\u001b[0m,             \u001b[38;5;34m0\u001b[0m  layer_normalizat \n (\u001b[38;5;33mGetItem\u001b[0m)            \u001b[38;5;34m4\u001b[0m)                                               \n\n get_item_17          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2\u001b[0m,             \u001b[38;5;34m0\u001b[0m  layer_normalizat \n (\u001b[38;5;33mGetItem\u001b[0m)            \u001b[38;5;34m4\u001b[0m)                                               \n\n concatenate_16       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2\u001b[0m,             \u001b[38;5;34m0\u001b[0m  get_item_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)        \u001b[38;5;34m4\u001b[0m)                             get_item_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n\n multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2\u001b[0m,         \u001b[38;5;34m1,600\u001b[0m  concatenate_16[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mMultiHeadAttentio\u001b[0m  \u001b[38;5;34m4\u001b[0m)                             concatenate_16[\u001b[38;5;34m0\u001b[0m \n\n add_16 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2\u001b[0m,             \u001b[38;5;34m0\u001b[0m  conv3d_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  \n                      \u001b[38;5;34m4\u001b[0m)                             multi_head_atten \n\n up_sampling3d_6      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m4\u001b[0m,             \u001b[38;5;34m0\u001b[0m  add_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n (\u001b[38;5;33mUpSampling3D\u001b[0m)       \u001b[38;5;34m4\u001b[0m)                                               \n\n concatenate_17       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m4\u001b[0m,             \u001b[38;5;34m0\u001b[0m  up_sampling3d_6[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)        \u001b[38;5;34m8\u001b[0m)                             add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n conv3d_19 (\u001b[38;5;33mConv3D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m4\u001b[0m,           \u001b[38;5;34m868\u001b[0m  concatenate_17[\u001b[38;5;34m0\u001b[0m \n                      \u001b[38;5;34m4\u001b[0m)                                               \n\n up_sampling3d_7      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m8\u001b[0m,           \u001b[38;5;34m0\u001b[0m  conv3d_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n (\u001b[38;5;33mUpSampling3D\u001b[0m)       \u001b[38;5;34m4\u001b[0m)                                               \n\n concatenate_18       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m8\u001b[0m,           \u001b[38;5;34m0\u001b[0m  up_sampling3d_7[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)        \u001b[38;5;34m8\u001b[0m)                             add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n conv3d_20 (\u001b[38;5;33mConv3D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m8\u001b[0m,         \u001b[38;5;34m868\u001b[0m  concatenate_18[\u001b[38;5;34m0\u001b[0m \n                      \u001b[38;5;34m4\u001b[0m)                                               \n\n concatenate_19       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m8\u001b[0m,           \u001b[38;5;34m0\u001b[0m  conv3d_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  \n (\u001b[38;5;33mConcatenate\u001b[0m)        \u001b[38;5;34m8\u001b[0m)                             conv3d_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv3d_21 (\u001b[38;5;33mConv3D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m8\u001b[0m,         \u001b[38;5;34m868\u001b[0m  concatenate_19[\u001b[38;5;34m0\u001b[0m \n                      \u001b[38;5;34m4\u001b[0m)                                               \n\n up_sampling3d_8      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,              \u001b[38;5;34m0\u001b[0m  conv3d_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n (\u001b[38;5;33mUpSampling3D\u001b[0m)       \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m4\u001b[0m)                                           \n\n concatenate_20       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,              \u001b[38;5;34m0\u001b[0m  up_sampling3d_8[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)        \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m8\u001b[0m)                         input_layer_2[\u001b[38;5;34m0\u001b[0m] \n\n conv3d_22 (\u001b[38;5;33mConv3D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,            \u001b[38;5;34m868\u001b[0m  concatenate_20[\u001b[38;5;34m0\u001b[0m \n                      \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m4\u001b[0m)                                           \n\n conv3d_23 (\u001b[38;5;33mConv3D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,             \u001b[38;5;34m20\u001b[0m  conv3d_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n                      \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m4\u001b[0m)                                           \n\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n\n input_layer_2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                           \n\n conv3d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,         <span style=\"color: #00af00; text-decoration-color: #00af00\">436</span>  input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                               \n\n layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,           <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>  conv3d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>  <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                               \n\n get_item_12          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer_normalizat \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                               \n\n get_item_13          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer_normalizat \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                               \n\n concatenate_14       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  get_item_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                             get_item_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n\n multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,         <span style=\"color: #00af00; text-decoration-color: #00af00\">536</span>  concatenate_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>  <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                             concatenate_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n add_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv3d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  \n                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                             multi_head_atten \n\n conv3d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,           <span style=\"color: #00af00; text-decoration-color: #00af00\">436</span>  add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                               \n\n layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,             <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>  conv3d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>  <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                               \n\n get_item_14          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer_normalizat \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                               \n\n get_item_15          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer_normalizat \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                               \n\n concatenate_15       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  get_item_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                             get_item_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n\n multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,068</span>  concatenate_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>  <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                             concatenate_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n add_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv3d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  \n                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                             multi_head_atten \n\n conv3d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>,           <span style=\"color: #00af00; text-decoration-color: #00af00\">436</span>  add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                               \n\n layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>,             <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>  conv3d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>  <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                               \n\n get_item_16          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>,             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer_normalizat \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                               \n\n get_item_17          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>,             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer_normalizat \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                               \n\n concatenate_16       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>,             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  get_item_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                             get_item_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n\n multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>,         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span>  concatenate_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>  <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                             concatenate_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n add_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>,             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv3d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  \n                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                             multi_head_atten \n\n up_sampling3d_6      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling3D</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                               \n\n concatenate_17       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  up_sampling3d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                             add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n conv3d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,           <span style=\"color: #00af00; text-decoration-color: #00af00\">868</span>  concatenate_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                               \n\n up_sampling3d_7      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv3d_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling3D</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                               \n\n concatenate_18       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  up_sampling3d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                             add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n conv3d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,         <span style=\"color: #00af00; text-decoration-color: #00af00\">868</span>  concatenate_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                               \n\n concatenate_19       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv3d_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                             conv3d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv3d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,         <span style=\"color: #00af00; text-decoration-color: #00af00\">868</span>  concatenate_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                               \n\n up_sampling3d_8      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv3d_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling3D</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                           \n\n concatenate_20       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  up_sampling3d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                         input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n\n conv3d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,            <span style=\"color: #00af00; text-decoration-color: #00af00\">868</span>  concatenate_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                      <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                           \n\n conv3d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,             <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>  conv3d_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n                      <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                           \n\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,028\u001b[0m (31.36 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,028</span> (31.36 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,028\u001b[0m (31.36 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,028</span> (31.36 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"6348"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"model_1.save(\"model_1.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T02:24:20.806393Z","iopub.execute_input":"2024-11-26T02:24:20.806778Z","iopub.status.idle":"2024-11-26T02:24:20.870931Z","shell.execute_reply.started":"2024-11-26T02:24:20.806738Z","shell.execute_reply":"2024-11-26T02:24:20.870227Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def dice_loss(y_true, y_pred):\n    \"\"\"\n    Calculate Dice Loss with float16 precision.\n    \"\"\"\n    smooth = tf.cast(1e-6, tf.float32)\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    intersection = tf.reduce_sum(y_true * y_pred)\n    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n    dice = (2. * intersection + smooth) / (union + smooth)\n    return tf.cast(1 - dice, tf.float32)\n\n\ndef iou_metric(y_true, y_pred):\n    \"\"\"\n    Calculate Intersection over Union (IoU) with float32 precision.\n    \"\"\"\n    y_true = tf.cast(y_true, tf.float32)  # Ensure float32 for calculations\n    y_pred = tf.cast(y_pred > 0.5, tf.float32)  # Threshold and cast to float32\n    intersection = tf.reduce_sum(y_true * y_pred)\n    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n    iou = tf.divide(intersection, union + tf.keras.backend.epsilon())  # Smooth division\n    return tf.cast(iou, tf.float32)  # Keep the result in float32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T02:34:08.893756Z","iopub.execute_input":"2024-11-26T02:34:08.894551Z","iopub.status.idle":"2024-11-26T02:34:08.900415Z","shell.execute_reply.started":"2024-11-26T02:34:08.894520Z","shell.execute_reply":"2024-11-26T02:34:08.899568Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T02:24:20.879517Z","iopub.execute_input":"2024-11-26T02:24:20.879853Z","iopub.status.idle":"2024-11-26T02:24:21.106872Z","shell.execute_reply.started":"2024-11-26T02:24:20.879819Z","shell.execute_reply":"2024-11-26T02:24:21.106013Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\n# Function to load all npy files\ndef load_data(folder_path):\n    image_files = sorted([f for f in os.listdir(folder_path) if f.startswith('image')])\n    mask_files = sorted([f for f in os.listdir(folder_path) if f.startswith('mask')])\n    \n    images = []\n    masks = []\n    \n    for img_file, msk_file in zip(image_files, mask_files):\n        img = np.load(os.path.join(folder_path, img_file))\n        msk = np.load(os.path.join(folder_path, msk_file))\n        \n        images.append(img)\n        \n        masks.append(msk)\n    \n    return np.array(images), np.array(masks)\n\n# Path to the folder containing the .npy files\nfolder_path = '/kaggle/input/image-mask-32-32-16/image_mask_32_32_16/'\n\n# Load all data\nimages, masks = load_data(folder_path)\n\n# Split data into train, validation, and test sets\nX_train, X_temp, y_train, y_temp = train_test_split(\n    images, masks, test_size=0.3496, random_state=42\n)\n\n# Step 2: Split Temporary into Test and Validation\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.5836, random_state=42\n)\n\nprint(f\"Train data shape: {X_train.shape}, Test data shape: {X_test.shape}, Validation data shape: {X_val.shape}\")\n\n# Data generator function (with the above split)\ndef data_generator(X, y, batch_size):\n    while True:  # Infinite loop to yield batches\n        for start in range(0, len(X), batch_size):\n            end = min(start + batch_size, len(X))\n            batch_X = X[start:end]\n            batch_y = y[start:end]\n            \n            yield batch_X, batch_y\n\n# Set batch size\nbatch_size = 2  # You can adjust this depending on your GPU memory capacity\n\n# Create generators for training, validation, and testing\ntrain_gen = data_generator(X_train, y_train, batch_size)\nval_gen = data_generator(X_val, y_val, batch_size)\ntest_gen = data_generator(X_test, y_test, batch_size)\n\n# Define model (assuming your model is already defined)\nmodel = swin_3d_unetr(num_classes=4)\n\n# Compile model with mixed precision support\nmodel.compile(optimizer='adamw', loss=dice_loss, metrics=[iou_metric])\n\n# Train model using the generators\nhistory = model.fit(\n    train_gen,\n    steps_per_epoch=len(X_train) // batch_size,\n    epochs=100,\n    validation_data=val_gen,\n    validation_steps=len(X_val) // batch_size,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True),\n        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5),\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T02:35:43.080426Z","iopub.execute_input":"2024-11-26T02:35:43.080706Z","iopub.status.idle":"2024-11-26T02:38:23.450122Z","shell.execute_reply.started":"2024-11-26T02:35:43.080680Z","shell.execute_reply":"2024-11-26T02:38:23.449349Z"}},"outputs":[{"name":"stdout","text":"Train data shape: (239, 32, 32, 16, 4), Test data shape: (76, 32, 32, 16, 4), Validation data shape: (54, 32, 32, 16, 4)\nEpoch 1/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 30ms/step - iou_metric: 0.3336 - loss: 0.5018 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 0.0010\nEpoch 2/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - iou_metric: 0.9713 - loss: 0.0144 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 0.0010\nEpoch 3/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9715 - loss: 0.0144 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 0.0010\nEpoch 4/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9715 - loss: 0.0144 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 0.0010\nEpoch 5/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9720 - loss: 0.0141 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 0.0010\nEpoch 6/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - iou_metric: 0.9713 - loss: 0.0145 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 0.0010\nEpoch 7/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9718 - loss: 0.0142 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 5.0000e-04\nEpoch 8/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9718 - loss: 0.0142 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 5.0000e-04\nEpoch 9/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9715 - loss: 0.0144 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 5.0000e-04\nEpoch 10/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9720 - loss: 0.0141 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 5.0000e-04\nEpoch 11/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9722 - loss: 0.0140 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 5.0000e-04\nEpoch 12/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9723 - loss: 0.0140 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 2.5000e-04\nEpoch 13/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9718 - loss: 0.0142 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 2.5000e-04\nEpoch 14/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9720 - loss: 0.0141 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 2.5000e-04\nEpoch 15/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9728 - loss: 0.0137 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 2.5000e-04\nEpoch 16/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9728 - loss: 0.0137 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 2.5000e-04\nEpoch 17/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9732 - loss: 0.0135 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 1.2500e-04\nEpoch 18/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9724 - loss: 0.0139 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 1.2500e-04\nEpoch 19/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9716 - loss: 0.0143 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 1.2500e-04\nEpoch 20/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9710 - loss: 0.0147 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 1.2500e-04\nEpoch 21/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9715 - loss: 0.0144 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 1.2500e-04\nEpoch 22/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9716 - loss: 0.0144 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 6.2500e-05\nEpoch 23/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9722 - loss: 0.0141 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 6.2500e-05\nEpoch 24/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9721 - loss: 0.0141 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 6.2500e-05\nEpoch 25/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9722 - loss: 0.0141 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 6.2500e-05\nEpoch 26/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9714 - loss: 0.0145 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 6.2500e-05\nEpoch 27/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9717 - loss: 0.0143 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 3.1250e-05\nEpoch 28/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9716 - loss: 0.0144 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 3.1250e-05\nEpoch 29/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9712 - loss: 0.0146 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 3.1250e-05\nEpoch 30/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - iou_metric: 0.9712 - loss: 0.0146 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 3.1250e-05\nEpoch 31/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9711 - loss: 0.0146 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 3.1250e-05\nEpoch 32/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9718 - loss: 0.0143 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 1.5625e-05\nEpoch 33/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - iou_metric: 0.9716 - loss: 0.0144 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 1.5625e-05\nEpoch 34/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - iou_metric: 0.9715 - loss: 0.0145 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 1.5625e-05\nEpoch 35/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9716 - loss: 0.0144 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 1.5625e-05\nEpoch 36/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9717 - loss: 0.0143 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 1.5625e-05\nEpoch 37/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - iou_metric: 0.9717 - loss: 0.0143 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 7.8125e-06\nEpoch 38/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9720 - loss: 0.0142 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 7.8125e-06\nEpoch 39/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9724 - loss: 0.0140 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 7.8125e-06\nEpoch 40/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9728 - loss: 0.0138 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 7.8125e-06\nEpoch 41/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9725 - loss: 0.0139 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 7.8125e-06\nEpoch 42/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9716 - loss: 0.0144 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 3.9063e-06\nEpoch 43/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9719 - loss: 0.0142 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 3.9063e-06\nEpoch 44/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9717 - loss: 0.0143 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 3.9063e-06\nEpoch 45/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9716 - loss: 0.0144 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 3.9063e-06\nEpoch 46/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9711 - loss: 0.0147 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 3.9063e-06\nEpoch 47/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9708 - loss: 0.0148 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 1.9531e-06\nEpoch 48/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9706 - loss: 0.0149 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 1.9531e-06\nEpoch 49/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9710 - loss: 0.0147 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 1.9531e-06\nEpoch 50/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9714 - loss: 0.0145 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 1.9531e-06\nEpoch 51/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9722 - loss: 0.0141 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 1.9531e-06\nEpoch 52/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9710 - loss: 0.0147 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 9.7656e-07\nEpoch 53/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9714 - loss: 0.0145 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 9.7656e-07\nEpoch 54/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9711 - loss: 0.0147 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 9.7656e-07\nEpoch 55/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9716 - loss: 0.0144 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 9.7656e-07\nEpoch 56/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9721 - loss: 0.0142 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 9.7656e-07\nEpoch 57/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9727 - loss: 0.0138 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 4.8828e-07\nEpoch 58/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9718 - loss: 0.0143 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 4.8828e-07\nEpoch 59/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9712 - loss: 0.0146 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 4.8828e-07\nEpoch 60/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9710 - loss: 0.0147 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 4.8828e-07\nEpoch 61/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9717 - loss: 0.0144 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 4.8828e-07\nEpoch 62/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9718 - loss: 0.0143 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 2.4414e-07\nEpoch 63/100\n\u001b[1m119/119\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - iou_metric: 0.9718 - loss: 0.0143 - val_iou_metric: 0.9728 - val_loss: 0.0138 - learning_rate: 2.4414e-07\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"test_loss, test_iou = model.evaluate(test_gen, steps=len(X_test) // batch_size)\nprint(f\"Test loss: {test_loss}, Test IoU: {test_iou}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T02:38:23.451630Z","iopub.execute_input":"2024-11-26T02:38:23.451908Z","iopub.status.idle":"2024-11-26T02:38:23.743701Z","shell.execute_reply.started":"2024-11-26T02:38:23.451880Z","shell.execute_reply":"2024-11-26T02:38:23.742841Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m38/38\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - iou_metric: 0.9673 - loss: 0.0166\nTest loss: 0.01637481153011322, Test IoU: 0.9678250551223755\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"model_1.save(\"model_trained.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T02:38:23.744757Z","iopub.execute_input":"2024-11-26T02:38:23.745030Z","iopub.status.idle":"2024-11-26T02:38:23.797399Z","shell.execute_reply.started":"2024-11-26T02:38:23.745005Z","shell.execute_reply":"2024-11-26T02:38:23.796437Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Visualization of training history\ndef plot_history(history):\n    epochs = range(1, len(history.history['loss']) + 1)  # Generate epoch indices\n\n    plt.figure(figsize=(10, 10))\n    \n    # Plot training & validation loss values\n    plt.plot(epochs, history.history['loss'], label='Train Loss')\n    plt.plot(epochs, history.history['val_loss'], label='Validation Loss')\n    plt.title('Model Loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.xticks(epochs)  # Set x-axis ticks to every epoch\n    plt.legend(loc='upper right')\n\n    plt.show()\n    plt.figure(figsize=(10, 10))\n    # Plot training & validation accuracy values\n    plt.plot(epochs, history.history['iou_metric'], label='Train Accuracy')\n    plt.plot(epochs, history.history['val_iou_metric'], label='Validation Accuracy')\n    plt.title('Model Accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.xticks(epochs)  # Set x-axis ticks to every epoch\n    plt.legend(loc='upper left')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Call the plotting function\nplot_history(history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T02:57:59.200590Z","iopub.execute_input":"2024-11-26T02:57:59.200914Z","iopub.status.idle":"2024-11-26T02:58:00.758645Z","shell.execute_reply.started":"2024-11-26T02:57:59.200889Z","shell.execute_reply":"2024-11-26T02:58:00.757773Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x1000 with 1 Axes>","image/png":"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"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x1000 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA94AAAPdCAYAAAB8+bCFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACVcUlEQVR4nOzdeZzUdf0H8Pfs7LIsCKiAIIYCnmmo5UGamhaJR6RmHpip5JGmlaKVlgdqaZoZWR7VD7DMK9PMyiPEzMwzzdQ088wDQdEEBIGd4/fH7szuwnIssjvf+e7z+XBk9zvf78xn9js7O695f9+fb6ZYLBYDAAAA6BQ1lR4AAAAApJngDQAAAJ1I8AYAAIBOJHgDAABAJxK8AQAAoBMJ3gAAANCJBG8AAADoRII3AAAAdCLBGwAAADqR4A0AVSCTycTEiRM7vN1LL70UmUwmrrzyytU+JgBg5QjeALCSrrzyyshkMpHJZOLee+9d6vpisRhDhw6NTCYTn/70pyswwtXj1ltvjUwmE0OGDIlCoVDp4QBA1RO8AaCDevbsGddcc81Sy//yl7/Eq6++GvX19RUY1epz9dVXx7Bhw+L111+Pu+66q9LDAYCqJ3gDQAfttddeccMNN0Qul2uz/JprroltttkmBg8eXKGRvX/z58+P3/3udzFhwoT48Ic/HFdffXWlh7RM8+fPr/QQAGClCN4A0EHjxo2Lt956K6ZNm1Zetnjx4vjNb34ThxxySLvbzJ8/P04++eQYOnRo1NfXx6abbhoXXXRRFIvFNustWrQoTjrppBg4cGD06dMnPvOZz8Srr77a7m2+9tpr8cUvfjEGDRoU9fX1scUWW8SUKVPe12P77W9/G++9914ccMABcfDBB8dNN90UCxcuXGq9hQsXxsSJE2OTTTaJnj17xrrrrhuf/exn4/nnny+vUygU4kc/+lGMHDkyevbsGQMHDow99tgj/v73v0fE8vvPl+xpnzhxYmQymXjqqafikEMOibXWWit22mmniIh4/PHH44gjjogRI0ZEz549Y/DgwfHFL34x3nrrrXZ/ZkceeWQMGTIk6uvrY/jw4XHcccfF4sWL44UXXohMJhM//OEPl9ruvvvui0wmE9dee21Hf6QAELWVHgAAVJthw4bFDjvsENdee23sueeeERFx2223xZw5c+Lggw+OSy65pM36xWIxPvOZz8Sf//znOPLII2PrrbeOO+64I77+9a/Ha6+91iboHXXUUfGrX/0qDjnkkNhxxx3jrrvuir333nupMcyaNSs++tGPRiaTiRNOOCEGDhwYt912Wxx55JExd+7cOPHEE1fpsV199dWx2267xeDBg+Pggw+OU089NX7/+9/HAQccUF4nn8/Hpz/96Zg+fXocfPDB8bWvfS3mzZsX06ZNiyeffDI23HDDiIg48sgj48orr4w999wzjjrqqMjlcvHXv/41Hnjggdh2221XaXwHHHBAbLzxxnHeeeeVP7SYNm1avPDCCzF+/PgYPHhw/Otf/4qf/exn8a9//SseeOCByGQyERExY8aM2H777eOdd96JY445JjbbbLN47bXX4je/+U0sWLAgRowYER/72Mfi6quvjpNOOmmpn0ufPn1in332WaVxA9DNFQGAlTJ16tRiRBQffvjh4k9+8pNinz59igsWLCgWi8XiAQccUNxtt92KxWKxuMEGGxT33nvv8nY333xzMSKK3/nOd9rc3uc+97liJpMpPvfcc8VisVh87LHHihFR/PKXv9xmvUMOOaQYEcWzzjqrvOzII48srrvuusXZs2e3Wffggw8u9uvXrzyuF198sRgRxalTp67w8c2aNatYW1tb/PnPf15etuOOOxb32WefNutNmTKlGBHFiy++eKnbKBQKxWKxWLzrrruKEVH86le/usx1lje2JR/vWWedVYyI4rhx45Zat/RYW7v22muLEVG85557yssOO+ywYk1NTfHhhx9e5ph++tOfFiOi+PTTT5evW7x4cXHAgAHFww8/fKntAGBlONQcAFbBgQceGO+991784Q9/iHnz5sUf/vCHZR5mfuutt0Y2m42vfvWrbZaffPLJUSwW47bbbiuvFxFLrbdk9bpYLMaNN94YY8eOjWKxGLNnzy5fxowZE3PmzIlHH320w4/puuuui5qamth///3Ly8aNGxe33XZb/O9//ysvu/HGG2PAgAHxla98ZanbKFWXb7zxxshkMnHWWWctc51Vceyxxy61rKGhofz1woULY/bs2fHRj340IqL8cygUCnHzzTfH2LFj2622l8Z04IEHRs+ePdv0tt9xxx0xe/bsOPTQQ1d53AB0b4I3AKyCgQMHxujRo+Oaa66Jm266KfL5fHzuc59rd93//ve/MWTIkOjTp0+b5R/84AfL15f+rampKR+qXbLpppu2+f7NN9+Md955J372s5/FwIED21zGjx8fERFvvPFGhx/Tr371q9h+++3jrbfeiueeey6ee+65+PCHPxyLFy+OG264obze888/H5tuumnU1i67Y+3555+PIUOGxNprr93hcSzP8OHDl1r29ttvx9e+9rUYNGhQNDQ0xMCBA8vrzZkzJyKafmZz586ND33oQ8u9/TXXXDPGjh3bZtb6q6++OtZbb734xCc+sRofCQDdiR5vAFhFhxxySBx99NExc+bM2HPPPWPNNdfskvstnVv70EMPjcMPP7zddbbccssO3eazzz4bDz/8cEREbLzxxktdf/XVV8cxxxzTwZEu37Iq3/l8fpnbtK5ulxx44IFx3333xde//vXYeuutY4011ohCoRB77LHHKp2H/LDDDosbbrgh7rvvvhg5cmTccsst8eUvfzlqatQrAFg1gjcArKL99tsvvvSlL8UDDzwQ119//TLX22CDDeLOO++MefPmtal6//vf/y5fX/q3UCiUK8olzzzzTJvbK814ns/nY/To0avlsVx99dVRV1cXV111VWSz2TbX3XvvvXHJJZfEyy+/HOuvv35suOGG8eCDD0ZjY2PU1dW1e3sbbrhh3HHHHfH2228vs+q91lprRUTEO++802Z56QiAlfG///0vpk+fHmeffXaceeaZ5eXPPvtsm/UGDhwYffv2jSeffHKFt7nHHnvEwIED4+qrr45Ro0bFggUL4gtf+MJKjwkAluSjWwBYRWussUZcfvnlMXHixBg7duwy19trr70in8/HT37ykzbLf/jDH0YmkynPjF76d8lZ0SdNmtTm+2w2G/vvv3/ceOON7QbJN998s8OP5eqrr46dd945DjrooPjc5z7X5vL1r389IqJ8Kq39998/Zs+evdTjiYjyTOP7779/FIvFOPvss5e5Tt++fWPAgAFxzz33tLn+sssuW+lxlz4kKC5xWrYlf2Y1NTWx7777xu9///vy6czaG1NERG1tbYwbNy5+/etfx5VXXhkjR47s8BEEANCaijcAvA/LOtS7tbFjx8Zuu+0W3/72t+Oll16KrbbaKv70pz/F7373uzjxxBPLPd1bb711jBs3Li677LKYM2dO7LjjjjF9+vR47rnnlrrN733ve/HnP/85Ro0aFUcffXRsvvnm8fbbb8ejjz4ad955Z7z99tsr/RgefPDBeO655+KEE05o9/r11lsvPvKRj8TVV18d3/zmN+Owww6LX/7ylzFhwoR46KGHYuedd4758+fHnXfeGV/+8pdjn332id122y2+8IUvxCWXXBLPPvts+bDvv/71r7HbbruV7+uoo46K733ve3HUUUfFtttuG/fcc0/85z//Wemx9+3bN3bZZZe48MILo7GxMdZbb73405/+FC+++OJS65533nnxpz/9KT7+8Y/HMcccEx/84Afj9ddfjxtuuCHuvffeNq0Chx12WFxyySXx5z//OS644IKVHg8AtEfwBoBOVlNTE7fcckuceeaZcf3118fUqVNj2LBh8f3vfz9OPvnkNutOmTKlfJjzzTffHJ/4xCfij3/8YwwdOrTNeoMGDYqHHnoozjnnnLjpppvisssui/79+8cWW2zR4aBYmsF7eVX7sWPHxsSJE+Pxxx+PLbfcMm699db47ne/G9dcc03ceOON0b9//9hpp51i5MiR5W2mTp0aW265ZUyePDm+/vWvR79+/WLbbbeNHXfcsbzOmWeeGW+++Wb85je/iV//+tex5557xm233RbrrLPOSo//mmuuia985Stx6aWXRrFYjN133z1uu+22GDJkSJv11ltvvXjwwQfjjDPOiKuvvjrmzp0b6623Xuy5557Rq1evNutus802scUWW8TTTz8dn//851d6LADQnkxxyWOzAACID3/4w7H22mvH9OnTKz0UAKqcHm8AgCX8/e9/j8ceeywOO+ywSg8FgBRQ8QYAaPbkk0/GI488Ej/4wQ9i9uzZ8cILL0TPnj0rPSwAqpyKNwBAs9/85jcxfvz4aGxsjGuvvVboBmC1UPEGAACATqTiDQAAAJ3I6cTaUSgUYsaMGdGnT5/IZDKVHg4AAAAJUywWY968eTFkyJCoqVl+TVvwbseMGTOWOl8qAAAALOmVV16JD3zgA8tdR/BuR58+fSKi6QfYt2/fCo8GAACApJk7d24MHTq0nB+XR/BuR+nw8r59+wreAAAALNPKtCebXA0AAAA6keANAAAAnUjwBgAAgE6kx/t9yOfz0djYWOlhwGpXV1cX2Wy20sMAAIBUELxXQbFYjJkzZ8Y777xT6aFAp1lzzTVj8ODBzmUPAADvk+C9Ckqhe5111olevXoJJqRKsViMBQsWxBtvvBEREeuuu26FRwQAANVN8O6gfD5fDt39+/ev9HCgUzQ0NERExBtvvBHrrLOOw84BAOB9MLlaB5V6unv16lXhkUDnKj3HzWMAAADvj+C9ihxeTtp5jgMAwOoheAMAAEAnErwBAACgEwnerLJhw4bFpEmTKj0MAACARBO8u4FMJrPcy8SJE1fpdh9++OE45phjVssYr7322shms3H88cevltsDAABICsG7G3j99dfLl0mTJkXfvn3bLDvllFPK6xaLxcjlcit1uwMHDlxts7tPnjw5vvGNb8S1114bCxcuXC23uaoWL15c0fsHAADSRfBeDYrFYixYnOvyS7FYXKnxDR48uHzp169fZDKZ8vf//ve/o0+fPnHbbbfFNttsE/X19XHvvffG888/H/vss08MGjQo1lhjjdhuu+3izjvvbHO7Sx5qnslk4v/+7/9iv/32i169esXGG28ct9xyywrH9+KLL8Z9990Xp556amyyySZx0003LbXOlClTYosttoj6+vpYd91144QTTihf984778SXvvSlGDRoUPTs2TM+9KEPxR/+8IeIiJg4cWJsvfXWbW5r0qRJMWzYsPL3RxxxROy7777x3e9+N4YMGRKbbrppRERcddVVse2220afPn1i8ODBccghh8Qbb7zR5rb+9a9/xac//eno27dv9OnTJ3beeed4/vnn45577om6urqYOXNmm/VPPPHE2HnnnVf4MwEAANKjttIDSIP3GvOx+Zl3dPn9PnXOmOjVY/XswlNPPTUuuuiiGDFiRKy11lrxyiuvxF577RXf/e53o76+Pn75y1/G2LFj45lnnon1119/mbdz9tlnx4UXXhjf//7348c//nF8/vOfj//+97+x9tprL3ObqVOnxt577x39+vWLQw89NCZPnhyHHHJI+frLL788JkyYEN/73vdizz33jDlz5sTf/va3iIgoFAqx5557xrx58+JXv/pVbLjhhvHUU09FNpvt0OOfPn169O3bN6ZNm1Ze1tjYGOeee25suumm8cYbb8SECRPiiCOOiFtvvTUiIl577bXYZZddYtddd4277ror+vbtG3/7298il8vFLrvsEiNGjIirrroqvv71r5dv7+qrr44LL7ywQ2MDAACqm+BNREScc8458alPfar8/dprrx1bbbVV+ftzzz03fvvb38Ytt9zSptq8pCOOOCLGjRsXERHnnXdeXHLJJfHQQw/FHnvs0e76hUIhrrzyyvjxj38cEREHH3xwnHzyyfHiiy/G8OHDIyLiO9/5Tpx88snxta99rbzddtttFxERd955Zzz00EPx9NNPxyabbBIRESNGjOjw4+/du3f83//9X/To0aO87Itf/GL56xEjRsQll1wS2223Xbz77ruxxhprxKWXXhr9+vWL6667Lurq6iIiymOIiDjyyCNj6tSp5eD9+9//PhYuXBgHHnhgh8cHAABUL8F7NWioy8ZT54ypyP2uLttuu22b7999992YOHFi/PGPf4zXX389crlcvPfee/Hyyy8v93a23HLL8te9e/eOvn37LnV4dmvTpk2L+fPnx1577RUREQMGDIhPfepTMWXKlDj33HPjjTfeiBkzZsQnP/nJdrd/7LHH4gMf+ECbwLsqRo4c2SZ0R0Q88sgjMXHixPjnP/8Z//vf/6JQKERExMsvvxybb755PPbYY7HzzjuXQ/eSjjjiiDj99NPjgQceiI9+9KNx5ZVXxoEHHhi9e/d+X2MFAACqi+C9GmQymdV2yHelLBkGTznllJg2bVpcdNFFsdFGG0VDQ0N87nOfW+HEY0uG0EwmUw6s7Zk8eXK8/fbb0dDQUF5WKBTi8ccfj7PPPrvN8vas6PqampqleuEbGxuXWm/Jxz9//vwYM2ZMjBkzJq6++uoYOHBgvPzyyzFmzJjyz2BF973OOuvE2LFjY+rUqTF8+PC47bbb4u67717uNgAAQPpUd1qk0/ztb3+LI444Ivbbb7+IaKqAv/TSS6v1Pt5666343e9+F9ddd11sscUW5eX5fD522mmn+NOf/hR77LFHDBs2LKZPnx677bbbUrex5ZZbxquvvhr/+c9/2q16Dxw4MGbOnBnFYjEymUxENFXJV+Tf//53vPXWW/G9730vhg4dGhERf//735e671/84hfR2Ni4zKr3UUcdFePGjYsPfOADseGGG8bHPvaxFd43AACQLmY1p10bb7xx3HTTTfHYY4/FP//5zzjkkEOWW7leFVdddVX0798/DjzwwPjQhz5Uvmy11Vax1157xeTJkyOiaWbyH/zgB3HJJZfEs88+G48++mi5J/zjH/947LLLLrH//vvHtGnT4sUXX4zbbrstbr/99oiI2HXXXePNN9+MCy+8MJ5//vm49NJL47bbblvh2NZff/3o0aNH/PjHP44XXnghbrnlljj33HPbrHPCCSfE3Llz4+CDD46///3v8eyzz8ZVV10VzzzzTHmdMWPGRN++feM73/lOjB8/fnX96AAAgCoieNOuiy++ONZaa63YcccdY+zYsTFmzJj4yEc+slrvY8qUKbHffvuVK9Gt7b///nHLLbfE7Nmz4/DDD49JkybFZZddFltssUV8+tOfjmeffba87o033hjbbbddjBs3LjbffPP4xje+Efl8PiIiPvjBD8Zll10Wl156aWy11Vbx0EMPtTlv+bIMHDgwrrzyyrjhhhti8803j+9973tx0UUXtVmnf//+cdddd8W7774bH//4x2ObbbaJn//8522q3zU1NXHEEUdEPp+Pww47bFV/VAAAQBXLFFf2ZNDdyNy5c6Nfv34xZ86c6Nu3b5vrFi5cWJ5xu2fPnhUaIdXkyCOPjDfffHOlzmmeJJ7rAACwbMvLjUvS4w2dZM6cOfHEE0/ENddcU3WhGwAAWH0Eb+gk++yzTzz00ENx7LHHtjlHOgAA0L0I3tBJnDoMAACIMLkaAAAAdCrBGwAAADqR4A0AAACdSPAGAACATiR4AwAAQCcSvAEAAKATCd6stF133TVOPPHE8vfDhg2LSZMmLXebTCYTN9988/u+79V1OwAAAF1N8O4Gxo4dG3vssUe71/31r3+NTCYTjz/+eIdv9+GHH45jjjnm/Q6vjYkTJ8bWW2+91PLXX3899txzz9V6X8vy3nvvxdprrx0DBgyIRYsWdcl9AgAA6SV4dwNHHnlkTJs2LV599dWlrps6dWpsu+22seWWW3b4dgcOHBi9evVaHUNcocGDB0d9fX2X3NeNN94YW2yxRWy22WYVr7IXi8XI5XIVHQMAAPD+CN6rQ7EYsXh+11+KxZUa3qc//ekYOHBgXHnllW2Wv/vuu3HDDTfEkUceGW+99VaMGzcu1ltvvejVq1eMHDkyrr322uXe7pKHmj/77LOxyy67RM+ePWPzzTePadOmLbXNN7/5zdhkk02iV69eMWLEiDjjjDOisbExIiKuvPLKOPvss+Of//xnZDKZyGQy5TEveaj5E088EZ/4xCeioaEh+vfvH8ccc0y8++675euPOOKI2HfffeOiiy6KddddN/r37x/HH398+b6WZ/LkyXHooYfGoYceGpMnT17q+n/961/x6U9/Ovr27Rt9+vSJnXfeOZ5//vny9VOmTIktttgi6uvrY911140TTjghIiJeeumlyGQy8dhjj5XXfeeddyKTycTdd98dERF33313ZDKZuO2222KbbbaJ+vr6uPfee+P555+PffbZJwYNGhRrrLFGbLfddnHnnXe2GdeiRYvim9/8ZgwdOjTq6+tjo402ismTJ0exWIyNNtooLrroojbrP/bYY5HJZOK5555b4c8EAABYdbWVHkAqNC6IOG9I19/vqa9E9Oi9wtVqazJx2BcOjSuvvDK+fdqpkclkIiLihuuvj3w+H+MOOjDefffd2OYjH45vfv2U6Nu3b/zx1lvjC1/4Qmw4fFhsv/32zbdUbAr7hXzLjRcLEYV8FAqF+OxnPxuDBq0TD95/X8yZMydOnHBy0zqFQnmbPmv0jiunTI4hQ4bEE088EUd/6djos0bv+MbXvx4HHfC5ePKJx+P2O/4Ud/7pjoiI6NevX8v9Nd/O/PnzY8yYMbHDRz8aDz/4QLzxxhtx1DFfihOOPz6unDqleVzF+POf/xzrDh4cf55+Zzz33HNx0LhDYustt4yjjz5qmT+r559/Pu6///646Tc3RLFYjJNOOin+++ILscEGG0RExGuvvRa77LJL7Prxj8ddd06Lvn37xt/+dl/kFi+KKOTj8suviAmnnBLfO/+82HOPPWLOnDnxt/vua3oM5cexxNetlzV/f+qpp8ZFF14QI0aMiLXWWiteeeWV2GuPPeK7554T9fX18curroqxY8fGM08/Feuvv35ERBz2hS/E/Q88EJdMmhRbbbVlvPjiizF79luRKRbii+OPiKlTp8YpE04qP9apU6bELrvsHBuNGN52n5YU8k37d/GCiJp2rgcAgM5W1yuiOb9UM8G7ms16MqKuYaVW/eLYHeP7F/0g/vLbqbHrjttGRMTUn18a+++5W/R777/RLxtxyuc/1bz2u/GVz+0Sd9yyQ/z6ystj+/V7Ni1ePD9i/psRM5v7wfOLI+bOiJj5eNz5l/vj3//+d9zxix/EkEGZiEFrxnknfzH2PPQrEe+8VN7m9CPHNt/H3Bi23QZxyjHj4rprfhnf+MKYaIiINWJ+1BYbY3C80bTanDci5jRv0nw711x9Uyx8b3788sJTonevQsSAAfGTs0+KsUecGBdM+EIMGtg/4r3/xVp9e8dPvn1UZLOLY7Nt14+9P/GxmH7rTXH02NIHCUub8uOfxJ677RhrLXolIiLGfPyjMfUnF8TEk4+NiIhLL/xx9FujIa774alRV1cXEQtjkz0/EhGLImY+Ht/5ztlx8jGfj68duGtELIzoWx/bHbRb0+N/c0bTncx+NmJm88Emc+Y1/fv2CxEz1276NyLOOfGI+NTIQRHRdHTD2oMysdU+H42IQkS8F+d++XPx299cH7dcfUWcMP7g+M/z/41f33BDTLv28hi9w4iIeDdGbDEwIgZGzHw8jthz+zjzrInx0G3XxPYf/lA0NjbGNVdfFRedcVLL/lxSrhgx582IWw+KePeVZf7MAACg03xrxkoVG5NO8F4d6no1PSG6SiHfFLpre670JpttNDx23HarmHLd72LXHbeN5158Of764D/inBuOi4iIfD4f510yJX79h2nx2sw3YvHixli0uDF6NaxcsH/62Rdj6JBBMWTwwPKyHbZZum/8+t/dEZdMuS6e/++r8e78BZHL56PvGh37RXr62Rdjqw9uEr17tYztY9ttFYVCIZ55/qWm4B0RW2yyYWSz2fI66w4aEE88/ewybzefz8cvbvhD/Oicr5eXHfrZveKUc38YZ550TNTU1MRjT/0ndt7+w82hu603Zr8dM2a+GZ/cadnBfmVtu+Xmbb5/d/6CmPiDn8Yfp/81Xn9jduRy+Xhv4aJ4+bWZERHx2L+eiWw2Gx/f4SPt3t6QwQNj70/uFFOu+11s/+EPxe+n3ROLFjfGAWNHv++xAgAAyyd4rw6ZTNd+ClMsRgwd1eHNjvzSCfGVr30tLu09PKb+8dex4YYbxsf3Gx+RycT3L7ggfjT11zHp4otj5MgPRe/evePEkybE4praiMHNAbpH74jeA1u+z/aI6Duk6fu+dzd9P7hV2G5oLlWvOSxi8JZx//33x+e/cnqcPfGs+NSndo8+ffvG9ddfH5MmTYr3+m8RhWLEop4Do5DtGQvW3iJqajJRk4nyofHF5tuJ3gMi6l9t/77W3rBpecNaUbdGps06md4Do1D3WtvtWrnj1lvjtZlvxEHHndpmeT6fjz89+WZ8cvToqO+7ThR69o7cOiOjJpOJTCaidOBLQ+/m6vXaI9q9j5rFazY9jv4bla9vzL7Zdpu1346IiN7Dt41Yc83ytqcc9+WYduff4qILL4qNNtowGhoa4nMHHhSL69aMGLxlNKz736YVB28Z0c6HAhERR375xDjs8CPiosunxpSbz4gDDjwoatffLhY2TxVQeiyZTCYyEZFftDCK79ZH8Zi/RPRc+Q95qkGxWIz3GvOxYHE+FjYWYsHifCxYnIv3GvPRmC80dVQUi1EoNq1bbP669bJCsdjU/VAsRnuzLbR3QFSm1WFSmYjoWZeNhvps9KrNRkOP2mjokY3e9TXRUNv0dY/a7jkNR2O+EAsb87EwV4jFuUIsbGz6fnEuH4tyhViYy0djrhDZmpqor6uJ+mw26mtror6u9G8memSz0bOuJuprs5GteX+Hp5X2f65QiHyhGLlCMQrN/+aLxcgXipHPN11fKDYtz+WLUZPJRG020/RvTSayNZmoqYmmfzOZqM1koqam5brS8o7KRETN+3yMKyOXL0R+JecWWVJNJhPZ5sdL5yoUirE4X2i65ArRmC/E4lwxGvOFWFT+vun6xnwhGpu/XpQrRCYiGnpkm16b6pr+7VmXjV49mn6fSsvrsh17bWrvd6ik9d+diNLfokybo0pLy4rR9PvWmG/6N1csNP0u5qPp97FQKP/+Nf2OFiJfWOLvW6bp+dj6+9Lt19Q0/5tZ8qjWTJuxtF3S8tpek4noUZuJ2pqa6FFbEz2yNVGXrXnfr0GdrVAoxryFuZizsDHmLGiM/723OOa+1/T1vEW5qK9teg70rs9Grx610btHbfSur43e9U3LeveojV49sm3+xq1I035seg7m8sVoLBSiGBE9ml/Xe2RroraDz7NKKr2vmLcoF+8uzMW8hbmYtygXCxblotD8utn6ed30/Ct9FW3eT0am6TlY+ruQbX79LP09ydbURE1NRG1NTWRrIrI1Nc3XR9TXNv+u1mbT9Xpb1zWTOXc2wbsaZTIRmeyK11vCfgccGF858aT4/hVXxuQrr4qDDvti/Gvm/IiIuP2ue2KX0XvG1p/cJ4oRMb9QiCeffiZGbLJpPPF606Rl8xfl4635i+OpmfMjk4lozBfjjXmL4z9vLIg+6w6PV155Je7714sxaPDgyETEPXfdExERM+YuimfeWBC//dNfYt0PDI3PHPHVKEYxGiPi8WdeiEKxGM++uaDpPnKZeG9xYzw3e8FS4//v2+/Fk6+/G/2GjIh/XPmL+MdLb8Yaa/SOmkwm7r7zz1FTUxP1AzaI52a/F3MX5eLdxfn4z5sLoliMKEYx3l7QGAsWF+KpmfOjGMVo/q8pNBUjLr7s57HHZz4bR33l5Db3+38//kFMuuxnMeRDH411R2wat/zm2vjnK/8rV71LbxhqMplYb+j6cf3v74ghW4wqf2hQejFdWLNGREQ8+sx/o+/QzaIYEffe81BERLw6Z1E8++aCePWdptOXPTd7QfRt7NE0gGLEn++5Nz69/7jY/GO7R0TE3PnvxgsvvhRbbbdjPPPGgug1ZKMoFArxq9/dETvsvFvT4yqFxmj6d+jWO0ePnr1i4oWXxB133BFTfvPH+Pes+ct8vhRzi+ONdxbF0b99ON5cUIy6bCbqapveRDS9mcg0fd16WW3TstLy1uuVlhWa/9guzrf80S29MVzc/CawvKx5nXyh2OqPUPMfmuY38a3/bfmj1PRHqxSu31ucj/mLc/He4ubvG/MrOzdhRdXWZKJXj6Y3Or16ZKOhRzZqazLNYa9pv+YLxfIHAuWvmwNhodj0hiqi6WdXegNYm81EXU1N87JM1GZbvs7W1ERd8x/7fKEYjYVi5Mr7qemNbGOu6U1SrrwPS8ub3ji1vKlt+h2oKb3RzbT9fSn9jhSLxViYawrXi3JN+3t1/xxLwbwp2Lb6IKVQbH6NKH2w0voDl5YPW5KutibT9DvX/HvX+uv62pqlrqvL1kS+UIzFuabAtThXiEX5QixqzJcDW2l509f51fZzyJZ/b1v9Hi/xu9z6uVF6PYtoei0v7a8oLy+2ur7lda90fevvy6/7zRuUlrcex1IflLT6gKT0+pKtyZSfy23fSLd67kfTi3/r70shr/QBS+sPcNv7HSnd9uJW4bi0T5q+z7d8Xw7Tnf+Era3JREOPphBeX1cThUKUA3Up/C75IVV3Vnp9rWv+fWz9t7P0vGr9AUA5hLV6PtUs8dwqBa/abMvztbam5TW+tvnvZem6umwmcoViU7BesDjeaQ7W/1uwOOa81/i+f78zmYheddnoXV8ba9TXlt8n5pr/lpf+RpT+pqzM/WVrMk2vYc1BvL75w9TWy3rU1rT5wHLJ7N/eByQRUf67VH7PUJOJbGlZ69ejmpb1isWmDyjmLszFvIWNzeG6+d+FudX+t+v9av0zK31wVgrlpa/ra7NN78nzxba/w/nmD5FbfV+6PlcoRKHQ8oFaMUp/N1tebwut/oZG6d9oeV2raX7el94LtH5PUH7dbV53540HxsTPbFHZH+ZqInh3IzU9GmLM2P3iR+efHfPfnRdjPzeu/IswdNiGceetv4tHH34g+vZbM676+WXx1uw3YsTGm5bfoEQ0vUnJFQrl73KFYizM5eMjO+wS64/YKE4+/piYcPrZMX/evPj+d8+OiCi/afvABsNj5muvxq2/+018aKuPxD13/Snuuv0PERFRl2164Rw2bFjMeOXleP7fT8Y6g9eLhl69o7ZHj/L9F4rF2GPfz8WlF50f3/jKl+LYk74Z/3v7rTj71FPi0589KBrWXDsWLM5FvvnT7oWN+TbbFovFVuNv8fZbs+Mvd94el0y+JjberO1h3mM/d3CcdPQXYs7//hfjjjgmrp36s/jm8UfGkcefFGv07RuPP/pwjNx6mxi24cZx7EmnxndOmxD91h4QO+02Oha8+2784+8PxiHjj4moqYstP7JdXPajH0T/dT8Qb8+eHT8875yIiFiUK8R7jflYlG8a28LGQvRoNfYPDBsRd9x6S3zsk2Mik4m49PvnRaHQVH1alMvHwHU/EGM/Ny6+ddLx8c2zL4hNNv9QvP7aK/H27DdjzNj9IiIim83GPgeMi0suOCfWH75hbL3tqDZvGotLvCFtral6EtH0v3RpaK7mNPRo+rf0R7wUGFtCYkulpKYmllqnPcv7E1woRixcnI8FjbmWDwcWlaruTVvmCsWY2/xHvrvqUVsTPWtrypW30huFHrU1kSsUm4Jic0gsBfdFuXyb8JErFCO3OB/zO+H5my2/4c20+bomkymH+3xhiUvzstWp9BgXVMHvaL5QjHwUI5I/1FQofQDaY6kPYLLNX2fKy4oRsbAxH+81FsqvT+8tbvrdWrA4Vw5LueYK6bwKvzbVZKIlbDYHztZhszbbFKAiWo5OKgWCUkhoLzgUlnjfE9Hqw58lPrRp/UJfCiVLfvhR+t1f2FiIeav/x7Da9OqRjTUb6qJfrx6xVq+6WLNXXfSpr4tFuXy8u6jpb9T8xbl4d1Eu5i/KxYJF+Xh3ca7885zf/Dr7xrxFHb7vUuhq/dqYLxTjvULTh+XVoiYT0adnXfTpWdv0b31t1NREmw8Mo/k5F7H0e6/WhZN8sfnojWLbD7HKR1yV/p7km/5tXOKDt9KRL/Oiut9DbLxOn0oPYbURvLuRYrEY+x18aPz2uqtijz33jF223qR83fe/OzGOeePVOP4Ln4tevXrFkUcdHb322TfmzJ0Tmw3uGxFNh5+t2atHbDyoT0SxGLU1NTFgjR4xYkDvKEbEDTf8Jk447ktx6NjRsf4GG8T53784Dth3bAzqWx8bDlwjNj70wHjpX4/G98/6ZixatCj22nvvmHjWmTFx4sT44LpN9zHsi4fGfdNviy8eMDbeeeedmDp1ahx++OERETF07YbYbHCfKBT7xB9uuzW+PmFCHDr2k9HQq1eM3Wff+O73vh99+vSKTGRijZ61kV+YjREDepc/4ezXUBcNPbKxyaCmX+DWh/lMuvb/Yo3eveOIA8ZGXV1deXkmE7HpgZ+Jb3+1If7x59/HV7/61bj7z3fF17/xjTjqwE9HNpuNrbbaOj675ydj+KA+cfLxR0ffumL85Mc/ih9+54zo339AfGa//WLo2r0iExE/+/nP46tfPjYO2Xu32GjjTeLc754f+43dKwb37RnDBvSOV/s1HdI9rH+vWHPNlvaFSyb9ML78paPjiP3GRP/+A+KkU06J/KIF0a+hLjYc2FRJn/Lzn8bEM0+PC878erz91lsxdP314xvfPDU2G9y3/On5N7725fi/n1wcXzrqizFyvX7LfJ4UI+K99xZG7fyeccOxO0Zk69pUphvzrQ5TzBfLhyqWDmFsbK6Etjmcsfn7mkym/El/XfMbwvrW37d6I1haVqq+tj68t1BoqfbmWn2dL0TkC02fpjfUNQXq3s2Hcfda4uuGumQeitWYbzn8vRTKFzRX7YvFYmRKh+0uUTUsfWKcbf1JcvORei2fVhfKn2yXDvErvVksHcKZyzdVrGqbq+S1rY5aKFXLS5XyHuXrm6otNZlM+U1syxvclje8hUJL1bJ0fUS0CdWlT+Z7ZGtWef+0VHOb2glK/xaKxbafrLdTlW/v39bBulRZat0K01GlN/xtnsP5YrTfuLDix9qYLzb/vuVbKthLVEIXt6psN+YKUdv8u9ejtqWK1PR1q2XN35eWrcohs8VoOkFC6QOH1h9GFIpL/u6WjjpofShy+4cdlyvNS34f7R/CWdpXbSrSpZ9hO2Nr/SFJe68/hVZBrKUqv2T1veX5HtFS9Wn9/C8UW45Mavq+JQhG83VLHcVQt/T+WrIKWFq2qs/RJRWLTc+z95qPGnqvsem1aWEu33QobKtKa9PvTE1k23yfafN9JtP2rKiln1XL121/viV1rYJ2El+/I5qONGostP37WD7sP9901FDrv5ltjtRo9ZxpHcpaP0dKr6el1+98q9f1ZX2fyxejpiYTazY0Beo1e/WINRvqYq3eTf/2baiLnnUdP5qydIh1UxhvCufvLmoKeqUqf9Oh96W/J81HwtU0/b0v/Z0pvba0fu0uvY6VPlBd8rWttE7r37klx1b+us0VLUczLfl+ovR7XyzGUq9TNZlMS6DuWVv+um+rZR095H51yxeK5b93CxvzzZemFq3yh9ONLdcvyhWa3ze0f/RE05FwbY+mKB+htMSRGku2arR39E6x+WfepoWv0PaIs9LRaKWv1+rVY7mPuZpkiu2Vt7q5uXPnRr9+/WLOnDnRt2/fNtctXLgwXnzxxRg+fHj0rLK+13cWLI6X314Qa9TXxojmsEb38te//jU++clPxiuvvBKDBg1a7rrV/FwHAIDOtrzcuCQV726k9BFLJT+JozIWLVoUb775ZkycODEOOOCAFYZuAABg9ame6QJ530qHNojd3c+1114bG2ywQbzzzjtx4YUXVno4AADQrQje3Yiugu7riCOOiHw+H4888kist956lR4OAAB0K4J3N+RIcwAAgK4jeK+iQjunpEq6co+3g81ZCdX4HAcAgCQyuVoH9ejRI2pqamLGjBkxcODA6NGjR9VMVrZ48eIo5hZHvrEYCxf6zIX2FYvFWLx4cbz55ptRU1MTPXqk5zQOAABQCYJ3B9XU1MTw4cPj9ddfjxkzZlR6OB0yb2FjzHkvF/N7ZGPh/4Qplq9Xr16x/vrrR02ND2kAAOD9ELxXQY8ePWL99dePXC4X+Xy+0sNZab964KWY+rcZsdfIdePk3YdXejgkWDabjdra2qo5mgMAAJJM8F5FmUwm6urqoq6urtJDWWnv5mritXn5eK9QEz179qz0cAAAALoFx5B2I/lC0+xqtQ4dBgAA6DISWDfSmG8K3tkahw8DAAB0FcG7G8k3nx6qVvAGAADoMoJ3N5IrqHgDAAB0NcG7G2np8Ra8AQAAuorg3Y2UKt61WbsdAACgq0hg3Uje5GoAAABdTvDuRhpNrgYAANDlBO9uJG9yNQAAgC4neHcjOZOrAQAAdDnBuxsp93ibXA0AAKDLSGDdSKniXafiDQAA0GUE724k3zy5mh5vAACAriN4dyMt5/EWvAEAALqK4N2N5Mrn8bbbAQAAuooE1o3kzWoOAADQ5QTvbiTX3OMteAMAAHQdwbsbyevxBgAA6HKCdzdSmlxNjzcAAEDXkcC6kdLkag41BwAA6DqCdzeScx5vAACALid4dyNmNQcAAOh6gnc3kitPrma3AwAAdBUJrBtR8QYAAOh6gnc30jKrueANAADQVQTvbiSXb5pcTcUbAACg6wje3YiKNwAAQNcTvLuRlh5vux0AAKCrSGDdSMus5ireAAAAXUXw7kbMag4AAND1BO9uolgsloO3Hm8AAICuI3h3E6XDzCP0eAMAAHQlCaybyLcK3lk93gAAAF1G8O4m2la8BW8AAICuInh3E/m84A0AAFAJgnc3kSsUyl+bXA0AAKDrCN7dROsZzTMZwRsAAKCrCN7dRKNTiQEAAFSE4N1NlHq89XcDAAB0LcG7myj1eKt4AwAAdC3Bu5so9XjXZe1yAACAriSFdRM5Pd4AAAAVIXh3E6WKtx5vAACAriV4dxONeT3eAAAAlSB4dxMq3gAAAJUheHcTerwBAAAqo+LB+9JLL41hw4ZFz549Y9SoUfHQQw8tc93GxsY455xzYsMNN4yePXvGVlttFbfffnubdSZOnBiZTKbNZbPNNuvsh5F4ZjUHAACojIqmsOuvvz4mTJgQZ511Vjz66KOx1VZbxZgxY+KNN95od/3TTz89fvrTn8aPf/zjeOqpp+LYY4+N/fbbL/7xj3+0WW+LLbaI119/vXy59957u+LhJJqKNwAAQGVUNHhffPHFcfTRR8f48eNj8803jyuuuCJ69eoVU6ZMaXf9q666Kr71rW/FXnvtFSNGjIjjjjsu9tprr/jBD37QZr3a2toYPHhw+TJgwIDljmPRokUxd+7cNpe0yReaJlfT4w0AANC1Kha8Fy9eHI888kiMHj26ZTA1NTF69Oi4//77291m0aJF0bNnzzbLGhoalqpoP/vsszFkyJAYMWJEfP7zn4+XX355uWM5//zzo1+/fuXL0KFDV/FRJVdjXsUbAACgEioWvGfPnh35fD4GDRrUZvmgQYNi5syZ7W4zZsyYuPjii+PZZ5+NQqEQ06ZNi5tuuilef/318jqjRo2KK6+8Mm6//fa4/PLL48UXX4ydd9455s2bt8yxnHbaaTFnzpzy5ZVXXlk9DzJBWmY11+MNAADQlWorPYCO+NGPfhRHH310bLbZZpHJZGLDDTeM8ePHtzk0fc899yx/veWWW8aoUaNigw02iF//+tdx5JFHtnu79fX1UV9f3+njryQ93gAAAJVRsfLngAEDIpvNxqxZs9osnzVrVgwePLjdbQYOHBg333xzzJ8/P/773//Gv//971hjjTVixIgRy7yfNddcMzbZZJN47rnnVuv4q025xzsreAMAAHSligXvHj16xDbbbBPTp08vLysUCjF9+vTYYYcdlrttz549Y7311otcLhc33nhj7LPPPstc9913343nn38+1l133dU29mqUy5cONRe8AQAAulJFG34nTJgQP//5z+MXv/hFPP3003HcccfF/PnzY/z48RERcdhhh8Vpp51WXv/BBx+Mm266KV544YX461//GnvssUcUCoX4xje+UV7nlFNOib/85S/x0ksvxX333Rf77bdfZLPZGDduXJc/viTJlw811+MNAADQlSra433QQQfFm2++GWeeeWbMnDkztt5667j99tvLE669/PLLUdMqKC5cuDBOP/30eOGFF2KNNdaIvfbaK6666qpYc801y+u8+uqrMW7cuHjrrbdi4MCBsdNOO8UDDzwQAwcO7OqHlyiNBRVvAACASsgUi8VipQeRNHPnzo1+/frFnDlzom/fvpUezmpx5d9ejIm/fyr23nLduPSQj1R6OAAAAFWtI7nRccfdRE7FGwAAoCIE727CebwBAAAqQwrrJlS8AQAAKkPw7ibKs5o7jzcAAECXEry7iVy+EBEq3gAAAF1N8O4mcuXzeAveAAAAXUnw7ibyerwBAAAqQvDuJsqTq2XtcgAAgK4khXUTKt4AAACVIXh3E7lC0+RqerwBAAC6luDdTeTyKt4AAACVIHh3Ey2zmtvlAAAAXUkK6yb0eAMAAFSG4N1NtMxqLngDAAB0JcG7m8g3T66m4g0AANC1BO9uojS5mh5vAACAriWFdRM5Pd4AAAAVIXh3Ey2zmgveAAAAXUnw7ibKPd4mVwMAAOhSgnc3UerxrtXjDQAA0KWksG4i71BzAACAihC8uwmTqwEAAFSG4N1N5Jp7vLN6vAEAALqU4N1NtPR4C94AAABdSfDuJvR4AwAAVIbg3U2Ugndd1i4HAADoSlJYN5FT8QYAAKgIwbubyJvVHAAAoCIE726iMd88q7ngDQAA0KUE726ipeJtlwMAAHQlKayb0OMNAABQGYJ3N9Eyq7ngDQAA0JUE724iV9DjDQAAUAmCdzehxxsAAKAypLBuoFgsRmNejzcAAEAlCN7dQHOxOyKcxxsAAKCrCd7dQKm/OyIia3I1AACALiV4dwP5ViXvOj3eAAAAXUoK6wZyrYK3Hm8AAICuJXh3A/l8S/DW4w0AANC1BO9uoLG5xzuTiagRvAEAALqU4N0NtJzDW+gGAADoaoJ3N5BzDm8AAICKEby7gVLF24zmAAAAXU8S6wZKs5o7hzcAAEDXE7y7AT3eAAAAlSN4dwON+aZZzfV4AwAAdD3BuxtoqXjb3QAAAF1NEusGyj3eKt4AAABdTvDuBsoVb5OrAQAAdDnBuxvIFZp6vE2uBgAA0PUE724gly8dam53AwAAdDVJrBtwOjEAAIDKEby7AZOrAQAAVI7g3Q3k9XgDAABUjODdDeTMag4AAFAxgnc30NLjbXcDAAB0NUmsG2jM6/EGAACoFMG7G9DjDQAAUDmCdzdgVnMAAIDKEby7gbzJ1QAAACpG8O4GcnmTqwEAAFSKJNYNtMxqruINAADQ1QTvbqCxeXI1Pd4AAABdT/DuBvJ5Pd4AAACVInh3A2Y1BwAAqBzBuxto6fG2uwEAALqaJNYN5EyuBgAAUDGCdzeQL02upscbAACgywne3UBjXsUbAACgUgTvbiBfnlzN7gYAAOhqklg3oMcbAACgcgTvbqDU4+083gAAAF1P8O4GVLwBAAAqR/DuBvR4AwAAVI4k1g3kzGoOAABQMYJ3N5Arncdb8AYAAOhygnc3kNfjDQAAUDGCdzdQnlwta3cDAAB0NUmsG1DxBgAAqBzBuxsoTa6mxxsAAKDrCd7dQGlyNRVvAACArid4dwO5goo3AABApQje3UC5xzsreAMAAHQ1wbsbKPV419bY3QAAAF1NEusGzGoOAABQOYJ3N1CaXE2PNwAAQNcTvLuBnB5vAACAihG8u4GW83jb3QAAAF1NEusG9HgDAABUjuDdDTjUHAAAoHIE724g3zy5moo3AABA1xO8u4FSxVuPNwAAQNeTxLqB0uRqKt4AAABdT/DuBvLlirfgDQAA0NUE724gp8cbAACgYgTvlCsUitFc8I7arN0NAADQ1SSxlMsXi+WvHWoOAADQ9QTvlCv1d0c41BwAAKASBO+Ua8wXyl+reAMAAHQ9wTvlVLwBAAAqS/BOuVxBjzcAAEAlCd4pV6p419ZkIpMRvAEAALqa4J1ypYq3ajcAAEBlCN4pl8+3VLwBAADoeoJ3yjUWmmY1V/EGAACoDME75co93lm7GgAAoBKksZTL5fV4AwAAVJLgnXKlined4A0AAFARgnfK5Uo93lnBGwAAoBIE75RrOY+3XQ0AAFAJ0ljKNerxBgAAqCjBO+VaKt6CNwAAQCUI3imXcx5vAACAihK8U855vAEAACpLGku5nEPNAQAAKkrwTrlSxduh5gAAAJUheKdcY76px1vFGwAAoDIE75RT8QYAAKgswTvl9HgDAABUluCdcmY1BwAAqCxpLOVUvAEAACpL8E65fPPkanq8AQAAKkPwTjkVbwAAgMoSvFMuV57V3K4GAACoBGks5fIq3gAAABUleKdcLl+a1VzwBgAAqATBO+XyhabJ1VS8AQAAKqPiwfvSSy+NYcOGRc+ePWPUqFHx0EMPLXPdxsbGOOecc2LDDTeMnj17xlZbbRW33377+7rNtNPjDQAAUFkVTWPXX399TJgwIc4666x49NFHY6uttooxY8bEG2+80e76p59+evz0pz+NH//4x/HUU0/FscceG/vtt1/84x//WOXbTLvyrOYONQcAAKiIigbviy++OI4++ugYP358bL755nHFFVdEr169YsqUKe2uf9VVV8W3vvWt2GuvvWLEiBFx3HHHxV577RU/+MEPVvk2067U4+083gAAAJVRseC9ePHieOSRR2L06NEtg6mpidGjR8f999/f7jaLFi2Knj17tlnW0NAQ99577yrfZul2586d2+aSFnq8AQAAKqtiwXv27NmRz+dj0KBBbZYPGjQoZs6c2e42Y8aMiYsvvjieffbZKBQKMW3atLjpppvi9ddfX+XbjIg4//zzo1+/fuXL0KFD3+ejS47yoeZ6vAEAACqiqtLYj370o9h4441js802ix49esQJJ5wQ48ePj5r3GSpPO+20mDNnTvnyyiuvrKYRV15ejzcAAEBFVSx4DxgwILLZbMyaNavN8lmzZsXgwYPb3WbgwIFx8803x/z58+O///1v/Pvf/4411lgjRowYscq3GRFRX18fffv2bXNJi5ZZzQVvAACASqhY8O7Ro0dss802MX369PKyQqEQ06dPjx122GG52/bs2TPWW2+9yOVyceONN8Y+++zzvm8zrXJ5Pd4AAACVVFvJO58wYUIcfvjhse2228b2228fkyZNivnz58f48eMjIuKwww6L9dZbL84///yIiHjwwQfjtddei6233jpee+21mDhxYhQKhfjGN76x0rfZ3ah4AwAAVFZFg/dBBx0Ub775Zpx55pkxc+bM2HrrreP2228vT4728ssvt+nfXrhwYZx++unxwgsvxBprrBF77bVXXHXVVbHmmmuu9G12N+Ueb8EbAACgIjLFYrFY6UEkzdy5c6Nfv34xZ86cqu/3PvqXf49pT82K8z87MsZtv36lhwMAAJAKHcmNVTWrOR2Xd6g5AABARQneKZdzqDkAAEBFCd4pV5rVXMUbAACgMgTvlGupeNvVAAAAlSCNpZwebwAAgMoSvFOuVPGuywreAAAAlSB4p1y+oMcbAACgkgTvlMvl9XgDAABUkjSWcjk93gAAABUleKdcaXK1Wj3eAAAAFSF4p1xOjzcAAEBFCd4pl2/u8a7T4w0AAFAR0ljK6fEGAACoLME75fR4AwAAVJbgnXKNeT3eAAAAlSR4p1y54i14AwAAVITgnXJ6vAEAACpL8E65UsW7LmtXAwAAVII0lmLFYlHFGwAAoMIE7xRrztwRoccbAACgUgTvFCvNaB6h4g0AAFApgneK5VuVvGtr7GoAAIBKkMZSLNcqeKt4AwAAVIbgnWJtK96CNwAAQCUI3imWKzT1eNdkImoEbwAAgIoQvFOsVPHW3w0AAFA5ElmK5fLO4Q0AAFBpgneK5coVb8EbAACgUgTvFMs393hns4I3AABApQjeKZbT4w0AAFBxElmKlXq8HWoOAABQOYJ3ipVmNTe5GgAAQOUI3ilWOo93rR5vAACAihG8U8zpxAAAACpP8E6xvNOJAQAAVJzgnWJmNQcAAKg8iSzFyhVvPd4AAAAVI3inWM6s5gAAABUneKdYLt88q7ngDQAAUDGCd4qpeAMAAFSe4J1ipR7vuqzdDAAAUCkSWYqpeAMAAFSe4J1i+YIebwAAgEoTvFNMxRsAAKDyBO8Uy+Wbz+NdYzcDAABUikSWYireAAAAlSd4p1i5xzsreAMAAFSK4J1ipYq3ydUAAAAqR/BOsXy+dKi53QwAAFApElmKNap4AwAAVJzgnWKlHm+TqwEAAFSO4J1ierwBAAAqT/BOsVKPd23WbgYAAKgUiSzFVLwBAAAqT/BOsXyhNKu54A0AAFApgneK5ZonV1PxBgAAqBzBO8VypfN4ZwVvAACAShG8UyyvxxsAAKDiBO8Ua5lczW4GAACoFIksxcoVb4eaAwAAVIzgnWKlydXMag4AAFA5gneKlSZX0+MNAABQOYJ3iuXK5/G2mwEAACpFIksxs5oDAABUnuCdYqUeb5OrAQAAVI7gnWIq3gAAAJUneKeYHm8AAIDKk8hSzKzmAAAAlSd4p1hLxVvwBgAAqBTBO8XypcnVBG8AAICKEbxTrFTxrs3azQAAAJUikaVY3qHmAAAAFSd4p5jJ1QAAACpP8E6xXHOPt4o3AABA5QjeKZYv93gL3gAAAJUieKdYeXI1FW8AAICKEbxTLF/u8babAQAAKkUiS7GcWc0BAAAqTvBOMT3eAAAAlSd4p1ijWc0BAAAqTvBOqUKhGMWmgrcebwAAgAqSyFKq1N8doeINAABQSYJ3SuVbBe86Pd4AAAAVI3inVK65vztCxRsAAKCSBO+Ual3x1uMNAABQORJZSjXmW4K3gjcAAEDlCN4pVT6Hd00mMhnJGwAAoFIE75TKOYc3AABAIgjeKVWqeNdl7WIAAIBKkspSqnQebxVvAACAyhK8U6p1jzcAAACVI3inVGNejzcAAEASCN4ppeINAACQDIJ3SpV7vLOCNwAAQCUJ3ilVntW8xi4GAACoJKkspXJ5s5oDAAAkgeCdUnmnEwMAAEgEwTulGgtNs5rX6vEGAACoKME7pfLlQ83tYgAAgEqSylIq53RiAAAAiSB4p5TzeAMAACSD4J1SOT3eAAAAiSB4p1TLrOZ2MQAAQCVJZSlVOo+3Q80BAAAqS/BOqZzzeAMAACSC4J1S+VKPt+ANAABQUYJ3SpVPJ5a1iwEAACpJKksppxMDAABIBsE7pfR4AwAAJIPgnVK5vB5vAACAJBC8U0rFGwAAIBkE75TS4w0AAJAMgndKmdUcAAAgGaSylFLxBgAASAbBO6VyeT3eAAAASSB4p1SuYFZzAACAJBC8U6plVnO7GAAAoJKkspTK50uTq6l4AwAAVJLgnVI5k6sBAAAkguCdUvnmHm+TqwEAAFSW4J1SKt4AAADJIHinVPl0Ylm7GAAAoJKkspRS8QYAAEgGwTul9HgDAAAkg+CdUqWKd53TiQEAAFSU4J1S+ebgna2xiwEAACpJKkspPd4AAADJIHinVC6vxxsAACAJBO+Uyqt4AwAAJELFg/ell14aw4YNi549e8aoUaPioYceWu76kyZNik033TQaGhpi6NChcdJJJ8XChQvL10+cODEymUyby2abbdbZDyNxcuUeb8EbAACgkmoreefXX399TJgwIa644ooYNWpUTJo0KcaMGRPPPPNMrLPOOkutf80118Spp54aU6ZMiR133DH+85//xBFHHBGZTCYuvvji8npbbLFF3HnnneXva2sr+jArIl+e1bzin60AAAB0axVNpBdffHEcffTRMX78+IiIuOKKK+KPf/xjTJkyJU499dSl1r/vvvviYx/7WBxyyCERETFs2LAYN25cPPjgg23Wq62tjcGDB6/0OBYtWhSLFi0qfz937txVeTiJouINAACQDBUrhy5evDgeeeSRGD16dMtgampi9OjRcf/997e7zY477hiPPPJI+XD0F154IW699dbYa6+92qz37LPPxpAhQ2LEiBHx+c9/Pl5++eXljuX888+Pfv36lS9Dhw59n4+u8vR4AwAAJEPFgvfs2bMjn8/HoEGD2iwfNGhQzJw5s91tDjnkkDjnnHNip512irq6uthwww1j1113jW9961vldUaNGhVXXnll3H777XH55ZfHiy++GDvvvHPMmzdvmWM57bTTYs6cOeXLK6+8snoeZAU1mtUcAAAgEaqqAfjuu++O8847Ly677LJ49NFH46abboo//vGPce6555bX2XPPPeOAAw6ILbfcMsaMGRO33nprvPPOO/HrX/96mbdbX18fffv2bXOpdi0V76raxQAAAKlTsR7vAQMGRDabjVmzZrVZPmvWrGX2Z59xxhnxhS98IY466qiIiBg5cmTMnz8/jjnmmPj2t78dNe2EzDXXXDM22WSTeO6551b/g0gwPd4AAADJULFyaI8ePWKbbbaJ6dOnl5cVCoWYPn167LDDDu1us2DBgqXCdTabjYiIYrHY7jbvvvtuPP/887HuuuuuppFXh5ZZzQVvAACASqrorOYTJkyIww8/PLbddtvYfvvtY9KkSTF//vzyLOeHHXZYrLfeenH++edHRMTYsWPj4osvjg9/+MMxatSoeO655+KMM86IsWPHlgP4KaecEmPHjo0NNtggZsyYEWeddVZks9kYN25cxR5nJeT0eAMAACRCRYP3QQcdFG+++WaceeaZMXPmzNh6663j9ttvL0+49vLLL7epcJ9++umRyWTi9NNPj9deey0GDhwYY8eOje9+97vldV599dUYN25cvPXWWzFw4MDYaaed4oEHHoiBAwd2+eOrJD3eAAAAyZApLusY7W5s7ty50a9fv5gzZ07VTrS2yem3xeJcIf526idivTUbKj0cAACAVOlIblQOTSnn8QYAAEgGwTuFisViOXjr8QYAAKgswTuFSqE7IqJOjzcAAEBFSWUplGsVvLNOJwYAAFBRgncKta546/EGAACoLME7hXL5VhVvwRsAAKCiBO8UyhUK5a+zGcEbAACgkgTvFCodal6TiahR8QYAAKgowTuFSpOr1WbtXgAAgEqTzFKoVPE2sRoAAEDlCd4pVKp4m1gNAACg8gTvFMrlmyZXU/EGAACoPME7hVoq3nYvAABApUlmKVTq8a7LqngDAABUmuCdQnq8AQAAkkPwTqF8QY83AABAUgjeKZTLq3gDAAAkheCdQrnyebztXgAAgEqTzFJIjzcAAEByCN4pVOrxNqs5AABA5QneKaTHGwAAIDkE7xTK6/EGAABIDMkshfR4AwAAJIfgnUK50nm89XgDAABUnOCdQnq8AQAAkkPwTiE93gAAAMkhmaVQrhy8VbwBAAAqTfBOoVLFO6vHGwAAoOIE7xRS8QYAAEgOwTuFcvmmWc1NrgYAAFB5gncKqXgDAAAkh+CdQuVZzbN2LwAAQKV1OJkNGzYszjnnnHj55Zc7YzysBireAAAAydHh4H3iiSfGTTfdFCNGjIhPfepTcd1118WiRYs6Y2ysonxBjzcAAEBSrFLwfuyxx+Khhx6KD37wg/GVr3wl1l133TjhhBPi0Ucf7Ywx0kEq3gAAAMmxyk3AH/nIR+KSSy6JGTNmxFlnnRX/93//F9ttt11svfXWMWXKlCgWi6tznHRALt98Hu8aPd4AAACVVruqGzY2NsZvf/vbmDp1akybNi0++tGPxpFHHhmvvvpqfOtb34o777wzrrnmmtU5VlZSXsUbAAAgMTocvB999NGYOnVqXHvttVFTUxOHHXZY/PCHP4zNNtusvM5+++0X22233WodKCsv19zjXZsVvAEAACqtw8F7u+22i0996lNx+eWXx7777ht1dXVLrTN8+PA4+OCDV8sA6TgVbwAAgOTocPB+4YUXYoMNNljuOr17946pU6eu8qB4f/R4AwAAJEeHk9kbb7wRDz744FLLH3zwwfj73/++WgbF+6PiDQAAkBwdDt7HH398vPLKK0stf+211+L4449fLYPi/WkslCregjcAAECldTh4P/XUU/GRj3xkqeUf/vCH46mnnlotg+L9yZtcDQAAIDE6HLzr6+tj1qxZSy1//fXXo7Z2lc9OxmpU6vGu1eMNAABQcR1OZrvvvnucdtppMWfOnPKyd955J771rW/Fpz71qdU6OFaNHm8AAIDk6HCJ+qKLLopddtklNthgg/jwhz8cERGPPfZYDBo0KK666qrVPkA6LqfHGwAAIDE6HLzXW2+9ePzxx+Pqq6+Of/7zn9HQ0BDjx4+PcePGtXtOb7peTo83AABAYqxSU3bv3r3jmGOOWd1jYTVpOY+34A0AAFBpqzwb2lNPPRUvv/xyLF68uM3yz3zmM+97ULw/erwBAACSo8PB+4UXXoj99tsvnnjiichkMlEsNoW8TKYp5OXz+dU7QjosVzCrOQAAQFJ0OJl97Wtfi+HDh8cbb7wRvXr1in/9619xzz33xLbbbht33313JwyRjipVvLN6vAEAACquwxXv+++/P+66664YMGBA1NTURE1NTey0005x/vnnx1e/+tX4xz/+0RnjpANyDjUHAABIjA5XvPP5fPTp0yciIgYMGBAzZsyIiIgNNtggnnnmmdU7OlZJLt80q7nJ1QAAACqvwxXvD33oQ/HPf/4zhg8fHqNGjYoLL7wwevToET/72c9ixIgRnTFGOiivxxsAACAxOhy8Tz/99Jg/f35ERJxzzjnx6U9/Onbeeefo379/XH/99at9gHRc6VBzFW8AAIDK63DwHjNmTPnrjTbaKP7973/H22+/HWuttVZ5ZnMqq1TxrjO5GgAAQMV16FjkxsbGqK2tjSeffLLN8rXXXlvoTpBcQY83AABAUnQoeNfV1cX666/vXN0Jp8cbAAAgOTqczL797W/Ht771rXj77bc7YzysBo15Pd4AAABJ0eEe75/85Cfx3HPPxZAhQ2KDDTaI3r17t7n+0UcfXW2DY9XknccbAAAgMTocvPfdd99OGAarkx5vAACA5Ohw8D7rrLM6YxysRi2zmuvxBgAAqDTJLIWcxxsAACA5OlzxrqmpWe6pw8x4XlmFQjGKTblbjzcAAEACdDh4//a3v23zfWNjY/zjH/+IX/ziF3H22WevtoGxahqb+7sjIrJZwRsAAKDSOhy899lnn6WWfe5zn4stttgirr/++jjyyCNXy8BYNaX+7ggVbwAAgCRYbT3eH/3oR2P69Omr6+ZYRblWwVuPNwAAQOWtluD93nvvxSWXXBLrrbfe6rg53od8viV419WYOw8AAKDSOnyo+VprrdVmcrVisRjz5s2LXr16xa9+9avVOjg6rlTxzmQialS8AQAAKq7DwfuHP/xhm+BdU1MTAwcOjFGjRsVaa621WgdHx5V6vPV3AwAAJEOHg/cRRxzRCcNgdWnMN81qrr8bAAAgGTrcBDx16tS44YYbllp+ww03xC9+8YvVMihWXUvFW383AABAEnQ4nZ1//vkxYMCApZavs846cd55562WQbHqSj3eKt4AAADJ0OHg/fLLL8fw4cOXWr7BBhvEyy+/vFoGxaorVbzrsoI3AABAEnQ4eK+zzjrx+OOPL7X8n//8Z/Tv33+1DIpVlyvo8QYAAEiSDgfvcePGxVe/+tX485//HPl8PvL5fNx1113xta99LQ4++ODOGCMdoMcbAAAgWTo8q/m5554bL730Unzyk5+M2tqmzQuFQhx22GF6vBOgMa/HGwAAIEk6HLx79OgR119/fXznO9+Jxx57LBoaGmLkyJGxwQYbdMb46CDn8QYAAEiWDgfvko033jg23njj1TkWVgM93gAAAMnS4Ubg/fffPy644IKlll944YVxwAEHrJZBserKFe+sHm8AAIAk6HA6u+eee2KvvfZaavmee+4Z99xzz2oZFKsu51BzAACAROlw8H733XejR48eSy2vq6uLuXPnrpZBseryJlcDAABIlA4H75EjR8b111+/1PLrrrsuNt9889UyKFZdqcdbxRsAACAZOjy52hlnnBGf/exn4/nnn49PfOITERExffr0uOaaa+I3v/nNah8gHVM61FzFGwAAIBk6HLzHjh0bN998c5x33nnxm9/8JhoaGmKrrbaKu+66K9Zee+3OGCMd0DK5muANAACQBKt0OrG999479t5774iImDt3blx77bVxyimnxCOPPBL5fH61DpCOyeVLk6uZ1RwAACAJVjmd3XPPPXH44YfHkCFD4gc/+EF84hOfiAceeGB1jo1VkDerOQAAQKJ0qOI9c+bMuPLKK2Py5Mkxd+7cOPDAA2PRokVx8803m1gtIfR4AwAAJMtKV7zHjh0bm266aTz++OMxadKkmDFjRvz4xz/uzLGxCsqzmuvxBgAASISVrnjfdttt8dWvfjWOO+642HjjjTtzTLwPufJ5vPV4AwAAJMFKp7N777035s2bF9tss02MGjUqfvKTn8Ts2bM7c2ysAj3eAAAAybLSwfujH/1o/PznP4/XX389vvSlL8V1110XQ4YMiUKhENOmTYt58+Z15jhZSTnBGwAAIFE6fDxy796944tf/GLce++98cQTT8TJJ58c3/ve92KdddaJz3zmM50xRjogr8cbAAAgUd5XI/Cmm24aF154Ybz66qtx7bXXrq4x8T6Y1RwAACBZVssMXNlsNvbdd9+45ZZbVsfN8T6UJlerNbkaAABAIkhnKaPiDQAAkCyCd8qUe7wFbwAAgEQQvFOmPKu5ydUAAAASQfBOmXz5UHO7FgAAIAmks5RxHm8AAIBkEbxTJpdv6vE2uRoAAEAyCN4po+INAACQLIJ3yuSdTgwAACBRBO+UKVW867J2LQAAQBJIZymTz6t4AwAAJIngnTJ6vAEAAJJF8E6ZXMGs5gAAAEkieKdMaXK12qzgDQAAkASCd8rkyj3edi0AAEASSGcpU6p41znUHAAAIBEE75TR4w0AAJAsgnfK6PEGAABIFsE7ZRr1eAMAACSKdJYyeefxBgAASBTBO2X0eAMAACSL4J0y5VnN9XgDAAAkguCdMrmCHm8AAIAkkc5SRo83AABAsgjeKdMyq7ngDQAAkASCd8rkmydXU/EGAABIBsE7ZUo93rVZuxYAACAJpLOU0eMNAACQLIJ3yrTMai54AwAAJIHgnTIq3gAAAMkieKdIsVgsB28VbwAAgGQQvFOkdJh5RERtjV0LAACQBNJZiuRbB++sijcAAEASCN4p0rri7VBzAACAZBC8UySfb32oueANAACQBBUP3pdeemkMGzYsevbsGaNGjYqHHnpouetPmjQpNt1002hoaIihQ4fGSSedFAsXLnxft5kWuUKh/LWKNwAAQDJUNHhff/31MWHChDjrrLPi0Ucfja222irGjBkTb7zxRrvrX3PNNXHqqafGWWedFU8//XRMnjw5rr/++vjWt761yreZJq3P4Z3JCN4AAABJUNHgffHFF8fRRx8d48ePj8033zyuuOKK6NWrV0yZMqXd9e+777742Mc+FoccckgMGzYsdt999xg3blybinZHbzMiYtGiRTF37tw2l2qUcyoxAACAxKlY8F68eHE88sgjMXr06JbB1NTE6NGj4/777293mx133DEeeeSRctB+4YUX4tZbb4299tprlW8zIuL888+Pfv36lS9Dhw5dHQ+xy5V6vOsEbwAAgMSoWPCePXt25PP5GDRoUJvlgwYNipkzZ7a7zSGHHBLnnHNO7LTTTlFXVxcbbrhh7LrrruVDzVflNiMiTjvttJgzZ0758sorr7zPR1cZpR5vFW8AAIDkqPjkah1x9913x3nnnReXXXZZPProo3HTTTfFH//4xzj33HPf1+3W19dH375921yqUek83rXZqtqtAAAAqVZbqTseMGBAZLPZmDVrVpvls2bNisGDB7e7zRlnnBFf+MIX4qijjoqIiJEjR8b8+fPjmGOOiW9/+9urdJtposcbAAAgeSpWGu3Ro0dss802MX369PKyQqEQ06dPjx122KHdbRYsWBA1NW2HnM1mIyKiWCyu0m2mSa65x9s5vAEAAJKjYhXviIgJEybE4YcfHttuu21sv/32MWnSpJg/f36MHz8+IiIOO+ywWG+99eL888+PiIixY8fGxRdfHB/+8Idj1KhR8dxzz8UZZ5wRY8eOLQfwFd1mmunxBgAASJ6KBu+DDjoo3nzzzTjzzDNj5syZsfXWW8ftt99enhzt5ZdfblPhPv300yOTycTpp58er732WgwcODDGjh0b3/3ud1f6NtOs1ONdp8cbAAAgMTLFYrFY6UEkzdy5c6Nfv34xZ86cqppo7YEX3oqDf/ZAbLTOGnHnhI9XejgAAACp1ZHcqDSaIuVZzR1qDgAAkBiCd4qY1RwAACB5BO8UyeWbJldT8QYAAEgOwTtFVLwBAACSR/BOkXKPt1nNAQAAEkNCS5GcydUAAAASR/BOkXyhqcfboeYAAADJIXinSC6v4g0AAJA0gneKtEyuZrcCAAAkhYSWInq8AQAAkkfwTpF86TzeWcEbAAAgKQTvFFHxBgAASB7BO0XyerwBAAASR0JLERVvAACA5BG8U6R0OrGsHm8AAIDEELxTJF9onlxNxRsAACAxBO8UaTnU3G4FAABICgktRUqTqzmdGAAAQHII3imSK89qLngDAAAkheCdInmzmgMAACSO4J0ijfmmydVUvAEAAJJD8E4RFW8AAIDkEbxTpDyredZuBQAASAoJLUVUvAEAAJJH8E4Rs5oDAAAkj+CdIvlC0+RqKt4AAADJIXinSGO+VPG2WwEAAJJCQksRPd4AAADJI3inSMus5oI3AABAUgjeKVLq8Ta5GgAAQHII3imSy5cONbdbAQAAkkJCS5G804kBAAAkjuCdIo0mVwMAAEgcwTtFyj3eJlcDAABIDME7RUo93nV6vAEAABJDQksRPd4AAADJI3inSN55vAEAABJH8E6RnIo3AABA4gjeKZLLN02uZlZzAACA5BC8U0TFGwAAIHkE7xQp9XjXZe1WAACApJDQUkTFGwAAIHkE7xQpz2oueAMAACSG4J0iuULT5Goq3gAAAMkheKdILl+qeNutAAAASSGhpUSxWNTjDQAAkECCd0o0Z+6IiKjLCt4AAABJIXinRKm/O0LFGwAAIEkE75TItyp56/EGAABIDgktJRrzLcFbxRsAACA5BO+UaFvxFrwBAACSQvBOiVKPdyYTUSN4AwAAJIbgnRKlined/m4AAIBEkdJSIpd3Dm8AAIAkErxTolTx1t8NAACQLIJ3SpR6vLNZwRsAACBJBO+UyKl4AwAAJJLgnRJ6vAEAAJJJ8E6Jlh5vuxQAACBJpLSUKB9qrscbAAAgUQTvlChVvB1qDgAAkCyCd0rk8k2zmptcDQAAIFkE75TIlSvedikAAECSSGkpkXc6MQAAgEQSvFPC5GoAAADJJHinRL6gxxsAACCJBO+UyJnVHAAAIJEE75TI5Us93nYpAABAkkhpKaHiDQAAkEyCd0ro8QYAAEgmwTslzGoOAACQTIJ3SrScx9suBQAASBIpLSVKk6vp8QYAAEgWwTslcnq8AQAAEknwTgmzmgMAACST4J0S+bzJ1QAAAJJI8E6JnMnVAAAAEklKS4m8Q80BAAASSfBOiZaKt+ANAACQJIJ3SuTyTbOaZ/V4AwAAJIrgnRIq3gAAAMkkeKdES4+3XQoAAJAkUlpKlCredSreAAAAiSJ4p0S+oMcbAAAgiQTvlNDjDQAAkEyCd0rk8nq8AQAAkkhKS4m8ijcAAEAiCd4pkSv1eAveAAAAiSJ4p0Sp4l1ncjUAAIBEEbxTIuc83gAAAIkkpaWEHm8AAIBkErxTojGvxxsAACCJBO+UUPEGAABIJsE7JUo93rVZuxQAACBJpLSUUPEGAABIJsE7JXL50qzmgjcAAECSCN4poeINAACQTIJ3SjQWzGoOAACQRIJ3SpQr3lnBGwAAIEkE75Qo9XjX1tilAAAASSKlpUSp4u1QcwAAgGQRvFMi51BzAACARBK8UyLfPLmaWc0BAACSRfBOiZbzeNulAAAASSKlpUTOebwBAAASSfBOCacTAwAASCbBOyVyzT3eZjUHAABIFsE7BQqFYjQXvJ3HGwAAIGGktBTIF4vlr1W8AQAAkkXwToHSjOYRJlcDAABIGsE7BUr93REq3gAAAEkjeKdAaUbziIi6rF0KAACQJFJaCuRaBW8FbwAAgGQRvFOgfA7vmkxkMpI3AABAkgjeKVCqeOvvBgAASB7BOwVy+abJ1cxoDgAAkDyCdwqoeAMAACSX4J0CpR5vM5oDAAAkj6SWArm8ijcAAEBSCd4p0HpWcwAAAJJF8E6BXKFpcrVsVvAGAABIGsE7BXLlirfdCQAAkDSSWgro8QYAAEguwTsF9HgDAAAkl+CdAqUe71o93gAAAIkjeKdAqeKd1eMNAACQOJJaCuQcag4AAJBYgncKmFwNAAAguRIRvC+99NIYNmxY9OzZM0aNGhUPPfTQMtfdddddI5PJLHXZe++9y+scccQRS12/xx57dMVDqYhyj7fgDQAAkDi1lR7A9ddfHxMmTIgrrrgiRo0aFZMmTYoxY8bEM888E+uss85S6990002xePHi8vdvvfVWbLXVVnHAAQe0WW+PPfaIqVOnlr+vr6/vvAdRYeVZzbOJ+BwFAACAViqe1C6++OI4+uijY/z48bH55pvHFVdcEb169YopU6a0u/7aa68dgwcPLl+mTZsWvXr1Wip419fXt1lvrbXW6oqHUxF6vAEAAJKrosF78eLF8cgjj8To0aPLy2pqamL06NFx//33r9RtTJ48OQ4++ODo3bt3m+V33313rLPOOrHpppvGcccdF2+99dYyb2PRokUxd+7cNpdq0jKrueANAACQNBUN3rNnz458Ph+DBg1qs3zQoEExc+bMFW7/0EMPxZNPPhlHHXVUm+V77LFH/PKXv4zp06fHBRdcEH/5y19izz33jHw+3+7tnH/++dGvX7/yZejQoav+oCpAxRsAACC5Kt7j/X5Mnjw5Ro4cGdtvv32b5QcffHD565EjR8aWW24ZG264Ydx9993xyU9+cqnbOe2002LChAnl7+fOnVtV4TuXb5pcTcUbAAAgeSpa8R4wYEBks9mYNWtWm+WzZs2KwYMHL3fb+fPnx3XXXRdHHnnkCu9nxIgRMWDAgHjuuefavb6+vj769u3b5lJN8ireAAAAiVXR4N2jR4/YZpttYvr06eVlhUIhpk+fHjvssMNyt73hhhti0aJFceihh67wfl599dV46623Yt11133fY06inFnNAQAAEqviSW3ChAnx85//PH7xi1/E008/Hccdd1zMnz8/xo8fHxERhx12WJx22mlLbTd58uTYd999o3///m2Wv/vuu/H1r389HnjggXjppZdi+vTpsc8++8RGG20UY8aM6ZLH1NVUvAEAAJKr4j3eBx10ULz55ptx5plnxsyZM2PrrbeO22+/vTzh2ssvvxw1NW0/H3jmmWfi3nvvjT/96U9L3V42m43HH388fvGLX8Q777wTQ4YMid133z3OPffc1J7LO5c3qzkAAEBSVTx4R0SccMIJccIJJ7R73d13373Usk033TSKxWK76zc0NMQdd9yxOoeXePlC0+RqKt4AAADJU/FDzXn/Gsvn8bY7AQAAkkZSS4Fyj3dWxRsAACBpBO8UKPV4O9QcAAAgeQTvFNDjDQAAkFyCdwrk9HgDAAAklqSWAnq8AQAAkkvwToFG5/EGAABILME7BfR4AwAAJJfgnQKlHm/BGwAAIHkE7xQo9Xhns3YnAABA0khqKaDiDQAAkFyCdwqUK96CNwAAQOII3inQmDe5GgAAQFIJ3img4g0AAJBcgncKlHq860yuBgAAkDiSWgqoeAMAACSX4J0CZjUHAABILsE7BfKFpsnVVLwBAACSR/BOgVy+VPG2OwEAAJJGUkuBnB5vAACAxBK8UyBfntVc8AYAAEgawTsFcnq8AQAAEkvwToG8Hm8AAIDEktRSQI83AABAcgneKVA+j7cebwAAgMQRvFMgl9fjDQAAkFSCdwqUZzXX4w0AAJA4kloKlHu8HWoOAACQOIJ3CpQq3rUONQcAAEgcwbvKFYtFs5oDAAAkmOBd5UrV7ggVbwAAgCQSvKtcrlXwVvEGAABIHsG7yrWueNdl7U4AAICkkdSqnIo3AABAsgneVa51xTubEbwBAACSRvCucrlCISIiajIRNSreAAAAiSN4V7lcvnQOb7sSAAAgiaS1Kpd3Dm8AAIBEE7yrXGlytdqs4A0AAJBEgneVyzf3eNeqeAMAACSS4F3lcuVDze1KAACAJJLWqlzL5Goq3gAAAEkkeFe5nMnVAAAAEk3wrnLlHm+TqwEAACSS4F3lHGoOAACQbIJ3lSudx7vW5GoAAACJJK1VOT3eAAAAySZ4V7mcHm8AAIBEE7yrXKnHW8UbAAAgmQTvKtfS4y14AwAAJJHgXeVyJlcDAABINGmtypUr3nq8AQAAEknwrnJmNQcAAEg2wbvK5fLNs5oL3gAAAIkkeFc5FW8AAIBkE7yrXN7kagAAAIkmrVW5nMnVAAAAEk3wrnL5QlOPt0PNAQAAkknwrnIt5/EWvAEAAJJI8K5yuXxpcjW7EgAAIImktSqn4g0AAJBsgneVK/V4m1wNAAAgmQTvKqfiDQAAkGyCd5XL6/EGAABINGmtyql4AwAAJJvgXeVyzuMNAACQaIJ3lcureAMAACSa4F3lSufxrs3alQAAAEkkrVU5FW8AAIBkE7yrXGlyNT3eAAAAySR4V7lyxTsreAMAACSR4F3lGvNmNQcAAEgywbvK6fEGAABINsG7yuXKwduuBAAASCJprcrp8QYAAEg2wbvK5Qp6vAEAAJJM8K5yerwBAACSTfCuco350nm87UoAAIAkktaqnIo3AABAsgneVS5ncjUAAIBEE7yrXN7kagAAAIkmeFc55/EGAABINmmtypV6vFW8AQAAkknwrnK5vMnVAAAAkkzwrnI5Pd4AAACJJnhXudKh5nVZuxIAACCJpLUql9PjDQAAkGiCd5XL6/EGAABINMG7yql4AwAAJJvgXeVKk6vVZgVvAACAJBK8q5yKNwAAQLIJ3lWsUChGsSl3R12NXQkAAJBE0loVK1W7IyKyDjUHAABIJMG7iuVbBW+zmgMAACST4F3FShOrRejxBgAASCrBu4rl8q0r3nYlAABAEklrVax1j7eCNwAAQDIJ3lWs1ONdl81EJiN5AwAAJJHgXcVKPd76uwEAAJJL8K5ipYq3/m4AAIDkktiqWKnHW8UbAAAguQTvKlaa1dw5vAEAAJJL8K5ierwBAACST/CuYi2zmtuNAAAASSWxVTE93gAAAMkneFexllnNBW8AAICkEryrWGlyNRVvAACA5BK8q5jJ1QAAAJJP8K5ipR7v2qzgDQAAkFSCdxXLl8/jbTcCAAAklcRWxXImVwMAAEg8wbuK5Z1ODAAAIPEE7ypWmlxNjzcAAEByCd5VrOV0YnYjAABAUklsVSyvxxsAACDxBO8qZnI1AACA5BO8q1hejzcAAEDiCd5VLFfQ4w0AAJB0ElsV0+MNAACQfIJ3FWvMO483AABA0gneVazc4y14AwAAJJbgXcXKs5qbXA0AACCxBO8q1tLjbTcCAAAklcRWxVpmNVfxBgAASCrBu4qZ1RwAACD5BO8q1phvmlxNxRsAACC5BO8qpuINAACQfIJ3FWuZ1dxuBAAASCqJrYrl8yZXAwAASLpEBO9LL700hg0bFj179oxRo0bFQw89tMx1d91118hkMktd9t577/I6xWIxzjzzzFh33XWjoaEhRo8eHc8++2xXPJQulXOoOQAAQOJVPHhff/31MWHChDjrrLPi0Ucfja222irGjBkTb7zxRrvr33TTTfH666+XL08++WRks9k44IADyutceOGFcckll8QVV1wRDz74YPTu3TvGjBkTCxcu7KqH1SXyBZOrAQAAJF3Fg/fFF18cRx99dIwfPz4233zzuOKKK6JXr14xZcqUdtdfe+21Y/DgweXLtGnTolevXuXgXSwWY9KkSXH66afHPvvsE1tuuWX88pe/jBkzZsTNN9/c7m0uWrQo5s6d2+ZSDRpVvAEAABKvosF78eLF8cgjj8To0aPLy2pqamL06NFx//33r9RtTJ48OQ4++ODo3bt3RES8+OKLMXPmzDa32a9fvxg1atQyb/P888+Pfv36lS9Dhw59H4+q65R7vE2uBgAAkFgVTWyzZ8+OfD4fgwYNarN80KBBMXPmzBVu/9BDD8WTTz4ZRx11VHlZabuO3OZpp50Wc+bMKV9eeeWVjj6Uiij1eNepeAMAACRWbaUH8H5Mnjw5Ro4cGdtvv/37up36+vqor69fTaPqOnq8AQAAkq+iFe8BAwZENpuNWbNmtVk+a9asGDx48HK3nT9/flx33XVx5JFHtlle2m5VbrPatJzHW/AGAABIqooG7x49esQ222wT06dPLy8rFAoxffr02GGHHZa77Q033BCLFi2KQw89tM3y4cOHx+DBg9vc5ty5c+PBBx9c4W1Wm3yhdB5vPd4AAABJVfFDzSdMmBCHH354bLvttrH99tvHpEmTYv78+TF+/PiIiDjssMNivfXWi/PPP7/NdpMnT4599903+vfv32Z5JpOJE088Mb7zne/ExhtvHMOHD48zzjgjhgwZEvvuu29XPawukcub1RwAACDpKh68DzrooHjzzTfjzDPPjJkzZ8bWW28dt99+e3lytJdffjlqlqjoPvPMM3HvvffGn/70p3Zv8xvf+EbMnz8/jjnmmHjnnXdip512ittvvz169uzZ6Y+nK+X0eAMAACReplgsFis9iKSZO3du9OvXL+bMmRN9+/at9HCWaZ+f3Bv/fHVOTDli2/jEZoNWvAEAAACrRUdyo+bgKpbT4w0AAJB4ElsVK02upscbAAAguQTvKtZS8Ra8AQAAkkrwrmK5fNPkaireAAAAySV4VzEVbwAAgOQTvKtYqce7Lms3AgAAJJXEVsVUvAEAAJJP8K5iZjUHAABIPsG7ipUmV1PxBgAASC7Bu4rlyhVvuxEAACCpJLYqVu7xzqp4AwAAJJXgXcXKs5o71BwAACCxBO8qVSwWy8FbjzcAAEByCd5VqhS6I/R4AwAAJJnEVqVyrYK3Hm8AAIDkEryrVK5NxVvwBgAASCrBu0rl860q3oI3AABAYgneVSpXKJS/VvEGAABILsG7SrWe0TyTEbwBAACSSvCuUjmnEgMAAKgKgneVKlW8HWYOAACQbIJ3lWrMN/V4q3gDAAAkm+BdpVS8AQAAqoPgXaVKPd61WbsQAAAgyaS2KqXiDQAAUB0E7yplVnMAAIDqIHhXqXyhaXI1FW8AAIBkE7yrVGNexRsAAKAaCN5VqqXH2y4EAABIMqmtSrXMaq7iDQAAkGSCd5XS4w0AAFAdBO8qldPjDQAAUBUE7yqlxxsAAKA6SG1VqtF5vAEAAKqC4F2lyj3eJlcDAABINMG7SpV6vE2uBgAAkGyCd5XKlw81twsBAACSTGqrUuXzeKt4AwAAJJrgXaXKFW893gAAAIkmeFepxnzz5Goq3gAAAIkmeFcp5/EGAACoDlJbldLjDQAAUB0E7yqlxxsAAKA6CN5VSsUbAACgOgjeVSrXPLlaVvAGAABINMG7SuVVvAEAAKqC4F2lyoeaZ+1CAACAJJPaqpSKNwAAQHUQvKtUrqDHGwAAoBoI3lVKxRsAAKA6CN5VqjHffB7vGrsQAAAgyaS2KqXiDQAAUB0E7yrVMqu54A0AAJBktZUeAKtmm/XXjGKxGMMH9K70UAAAAFiOTLFYLFZ6EEkzd+7c6NevX8yZMyf69u1b6eEAAACQMB3JjQ41BwAAgE4keAMAAEAnErwBAACgEwneAAAA0IkEbwAAAOhEgjcAAAB0IsEbAAAAOpHgDQAAAJ1I8AYAAIBOJHgDAABAJxK8AQAAoBMJ3gAAANCJBG8AAADoRII3AAAAdCLBGwAAADqR4A0AAACdSPAGAACATiR4AwAAQCcSvAEAAKATCd4AAADQiQRvAAAA6ESCNwAAAHQiwRsAAAA6keANAAAAnUjwBgAAgE4keAMAAEAnErwBAACgEwneAAAA0IkEbwAAAOhEgjcAAAB0IsEbAAAAOpHgDQAAAJ1I8AYAAIBOJHgDAABAJxK8AQAAoBMJ3gAAANCJBG8AAADoRII3AAAAdKLaSg8giYrFYkREzJ07t8IjAQAAIIlKebGUH5dH8G7HvHnzIiJi6NChFR4JAAAASTZv3rzo16/fctfJFFcmnnczhUIhZsyYEX369IlMJlPp4bRr7ty5MXTo0HjllVeib9++nbpdV21jfNUzvq68L+MzPuMzviRtY3zG113G15X3ZXzGtzq360rFYjHmzZsXQ4YMiZqa5Xdxq3i3o6amJj7wgQ9UehgrpW/fvqv0RFyV7bpqm668L+Ornvsyvq7fpivvy/i6fpuuvC/j6/ptuvK+jK/rt0nrfRlf12/TlffVlePrSiuqdJeYXA0AAAA6keANAAAAnUjwrlL19fVx1llnRX19fadv11XbGF/1jK8r78v4jM/4jC9J2xif8XWX8XXlfRmf8a3O7ZLK5GoAAADQiVS8AQAAoBMJ3gAAANCJBG8AAADoRII3AAAAdCLBuwrdc889MXbs2BgyZEhkMpm4+eabl7v++eefH9ttt1306dMn1llnndh3333jmWeeWeH9XH755bHllluWT1q/ww47xG233dahsX7ve9+LTCYTJ5544jLXmThxYmQymTaXzTbbbKVu/7XXXotDDz00+vfvHw0NDTFy5Mj4+9//vsz1hw0bttR9ZTKZOP7445e5TT6fjzPOOCOGDx8eDQ0NseGGG8a5554bKzMv4bx58+LEE0+MDTbYIBoaGmLHHXeMhx9+uHz9ivZlsViMM888M9Zdd91oaGiI0aNHx7PPPrvC7W666abYfffdo3///pHJZOKxxx5b7jaNjY3xzW9+M0aOHBm9e/eOIUOGxGGHHRY33XTTcu9n4sSJsdlmm0Xv3r1jrbXWitGjR8eDDz7YoefoscceG5lMJr7yla8sd5sjjjhiqf22/fbbr/B+nn766fjMZz4T/fr1i969e8d2220XN9xww3K3a+85kslkYvPNN1/mNu+++26ccMIJ8YEPfCAaGhpi8803j5NPPnm59zNr1qw44ogjYsiQIdGrV6/YY4894uSTT17h7+vChQvj+OOPj/79+8caa6wRH/rQh2Lrrbde7jY/+9nPYtddd42+fftGJpOJd955Z4WvDW+//XZ85StfiU033TQaGhpi/fXXjx133DE+8pGPLPe+vvSlL8WGG24YDQ0NMXDgwNhnn31iwoQJK/06VCwWY88994xMJhMbb7zxcrfZddddl9pP66yzzgrv5/77749PfOIT0bt37+jbt28MHz48ttlmm2Vu99JLLy3zedHQ0LDM+5o5c2Z84QtfiMGDB5d/tzbaaKPlju/555+P/fbbLwYOHBh9+/aNAw88MC644ILlvh4v+ZzYf//9V7hNe8+J5b3ut/d8+OpXvxpz5sxZ4d+L9p4TZ5111kr9jWn9fDj22GOXu017z4djjz12pf6eLfmc2GijjWLkyJHtbrO858MGG2ywzPtZ8vnwkY98JG688cYVjq+958SsWbPajL+9v7ftPS9ab9feNu09L5Z3P8t7XqxofO09L/79738vd5v2nhetX1vb22ZZz4sVjS9i6efFLrvsEu+991672yzveXHDDTcs836W9bxY0fiWfF5sscUWy30/taznw4reh7X3nFjeNst7Tqzovtp7Tpxwwgkr9T6x9XPi4IMPXu427T0ntt122xXeT3vPh29/+9vL3G55z4nl3deynhMr+vkt67ViRe+X23u/+de//nW527T3XnN597Os95ozZsxY4fiW9X6z2gjeVWj+/Pmx1VZbxaWXXrpS6//lL3+J448/Ph544IGYNm1aNDY2xu677x7z589f7nYf+MAH4nvf+1488sgj8fe//z0+8YlPxD777BP/+te/Vup+H3744fjpT38aW2655QrX3WKLLeL1118vX+69994VbvO///0vPvaxj0VdXV3cdttt8dRTT8UPfvCDWGuttZY7ptb3M23atIiIOOCAA5a5zQUXXBCXX355/OQnP4mnn346Lrjggrjwwgvjxz/+8QrHeNRRR8W0adPiqquuiieeeCJ23333GD16dLz22msRseJ9eeGFF8Yll1wSV1xxRTz44IPRu3fvGDNmTPzvf/9b7nbz58+PnXbaKS644II2y5a1zYIFC+LRRx+NM844Ix599NG46aab4plnnolvfvOby72fTTbZJH7yk5/EE088Effee28MGzYsdt9995gxY8ZKPUd/+9vfxgMPPBBDhgyJxYsXr3CbPfbYo83+O+WUU5a7zfPPPx877bRTbLbZZnH33XfH448/HmeccUbkcrnlbtf6Pl5//fWYMmVKRETstttuy9xmwoQJcfvtt8evfvWrePrpp+PEE0+MSZMmRUNDQ7vbFIvF2HfffeOFF16I3/3ud/GPf/wjNthgg7jsssviqKOOWu7v60knnRS///3v44Ybboi//OUv8eqrr8b8+fOXu82CBQtijz32iG9961vlZSt6bZgxY0bMmDEjLrroonjyySfjyiuvjH/+859RU1Oz3PvaZpttYurUqfH000/HHXfcEcViMS6//PI47rjjVup1aNKkSZHJZCIiYs8991zhNkcffXR5X+26665x9tlnL3eb+++/P/bYY4/Yfffd46GHHoqHH344+vXrF8cee+wytxs6dOhSz4uNNtoo6uvr45577lnmfR122GHxzDPPxC233BJPPPFE9O7dO1544YWYOnVqu9vMnz8/dt9998hkMnHXXXfF3/72t1i8eHFMnjw5zjvvvGW+Hi/5nJgxY0ZceeWVy30Nb+85sbzX/faeD7fffnsceeSRK/x70d5z4rLLLovvfve7K/wb0/r50L9//xX+XWr9fHj99dfjwgsvXOH42ntO7P//7Z17UBRX9sfPDAzDQ0EReQeUoOhqxBDRoK5uxKBo0ICCicTFR8X1gUs2iYKKpcbEuGtWdyubGDVK1kfwkQ2KMUoEH9mliISEh9lSHkrEIBGjAiIKCN/fH9ZMzQx9u3vcHzHG86maKum53zm3u7997rnTfccpU+jNN9+U1Ej5YfXq1WRvb08bNmwQxrH0Q0xMDMXFxVFzc7OwfyJPREVFUXt7OxGJx1spX8TExMhqpHxhQEoj5ws5ncgXERER1NbWplhDmPpCKY7IF0o6KV8kJiaSVquV1Ih80aVLF4qMjBTGEfmisLBQ2D8pX7S1tZGDgwNVV1dL1lNyfpCrw0SeEGmUPCEXS8oTO3fuVFUnWnpCSWPpiWeffVZWI/KDRqMR6kSe0Ol01L9/f2EskSdqamqEsUS5IjIyUrFetqw3dTodjRkzhrRarVBjWWs2NDTIxhHVmhMmTFDsn6jevHr1agcf/KIB81BDRMjIyLBKU1tbCyLCqVOnrI7XvXt3fPjhh4rtbt68iT59+uDYsWMYPXo0kpKShG1XrlyJ4OBgq/uSnJyMkSNHWq0zJSkpCY8//jja29uFbSZOnIjZs2ebbYuJiUF8fLzsZzc1NcHGxgafffaZ2faQkBAsX768Q3vLc9ne3g5PT0+sX7/euK2urg56vR7p6elCnSmVlZUgIhQWFsrGkiI/Px9EhIsXL6rW1NfXg4iQnZ2tGOuHH36Aj48PvvvuO/j7+2Pjxo2ymoSEBEyePFkYW0ozbdo0vPTSS7J9VrNfkydPxpgxY2Q1AwYMwBtvvGG2zfRcW2pKS0tBRPjuu++M29ra2tCzZ09s3brVuM3yeq2rq4NOp8P+/fuNbc6ePQsiQl5enqTGlBMnToCIcOPGjQ7vqckN+/btg52dHVpbW1VriouLQUSoqKhQjFVYWAgfHx/U1NR0OGZSGqX8IqUZNmwYUlNThRq1+zV48GCz3CClcXJywo4dO8x0rq6uxnNsqcnKyoJWq0V9fb2xfV1dHTQaDY4dO2b2OYZ8rMYTlhpT5Dwh0hiw9INanZQnpDRyfpDSKPlBpFPjCaV9svSDlEbJD1I6JU+Ixls5X+Tk5CiO0Za+sGZcN/WFNTqDL4qLi2U1Ur6QiyMXV04n8oU1+2TwhZxGzhcinZQvkpOTQUQdcgUg74c5c+aoqsNMPWFt7WbwxIoVK6zSGTzRv39/2XaWnpg2bZpsHKnzprRPIj9YeywGDx6MwYMHy2pEnoiKihLqRLmCiDBw4EBhLKl685VXXoFGozGrN0UYas2ZM2daXZcbas3Q0FCrdFL15sMA3/F+BDE8/uXq6qpa09bWRnv27KFbt25RWFiYYvuFCxfSxIkTaezYsao+v7y8nLy9vSkgIIDi4+OpqqpKUZOZmUlDhgyh2NhYcnd3pyeffJK2bt2qKh4RUUtLC+3atYtmz57d4VtzU4YPH045OTlUVlZGRETFxcX0n//8hyIjI2U//+7du9TW1kb29vZm2x0cHFTd0a+srKQff/zR7Bi6uLjQsGHDKC8vT1H/v1JfX08ajYa6deumqn1LSwtt2bKFXFxcKDg4WLZte3s7zZgxgxYvXkwDBgxQ3aeTJ0+Su7s7BQUF0fz58+natWuyMQ4fPkx9+/alcePGkbu7Ow0bNkxxaYYlV65cocOHD5vdvZFi+PDhlJmZSdXV1QSATpw4QWVlZRQRESHZvrm5mYjIzB9arZb0er2ZPyyv12+++YZaW1vNfNGvXz/y8/Mz+uJ+rnG1uvr6enJ2diZbW1tVmlu3blFaWhr17t2bHnvsMdlYTU1NNH36dHrvvffI09NTdf92795Nbm5uNHDgQFq6dCk1NTUJNbW1tXT69Glyd3en4cOHk4eHB40ePbrDNam0X9988w0VFRWZ+UJKM3z4cNq7dy9dv36d2tvbac+ePXTnzh363e9+J6lpbm4mjUZDer3e+Bn29vak1WqNfbTMx2o8YW0OV6ux9IManZQnpDRKfhDFkfODlE6NJ5T2ScoPUholP0jplDwhGm/lfJGcnGzVGE1k3bhu6gu1OlNfrF+/XqgR+UIpjsgXIp2cL9Tuk6kv5DRyvhDppHxhuA6jo6M71FNyfrh06dJ91WHWaAye0Gq1qnUGT3Tr1o0uXrwo1Ig8oRTH0hOtra1CjVKeULtPBk+EhITIakSe6NWrl1AnyhVERE5OTsJ6WarezMrKIm9vb1qxYoXqGvvUqVNW1+WGMTA0NFS1zpp68xfHg575M/8bZOUd77a2NkycOBEjRoxQ1b6kpAROTk6wsbGBi4sLDh8+rKhJT0/HwIEDcfv2bQDKdyA+//xz7Nu3D8XFxTh69CjCwsLg5+eHhoYG2Th6vR56vR5Lly7Ft99+i82bN8Pe3h4fffSRqn3bu3cvbGxsUF1dLduura0NycnJ0Gg0sLW1hUajwdq1a1XFCAsLw+jRo1FdXY27d+9i586d0Gq16Nu3b4e2lucyNzcXRITLly+btYuNjUVcXJxQZ8r93vG+ffs2QkJCMH36dEXNoUOH4OTkBI1GA29vb+Tn5yvGWrt2LZ599lnjkwZq7ninp6fj4MGDKCkpQUZGBvr374/Q0FDcvXtXUmP41tvR0REbNmxAYWEh3n77bWg0Gpw8eVL1sfjzn/+M7t27G/0s0ty5cwe///3vQUSwtbWFnZ0d/vnPfwo1LS0t8PPzQ2xsLK5fv47m5masW7cORISIiAgA0tfr7t27YWdn16GfoaGhWLJkieI1Lrq7qSY3XL16FX5+fli2bJmi5r333oOTkxOICEFBQWZ3NkW6uXPnYs6cOca/TY+ZSLN582YcPXoUJSUl2LVrF3x8fBAdHS3U5OXlgYjg6uqK7du349tvv8Urr7wCOzs7lJWVqT4W8+fPN7sDI9LcuHEDERERRl84OzsjKytLqKmtrYWzszOSkpJw69YtNDY2IjExEUSEqVOnSuZjOU/MmjVLMYdbekJt3rf0g5JOyhNyGpEf5DRyfhDp5DxhyG9Kx8LUD3L9k/ODSCfniTFjxgjHW5EvAgIC4ObmpjhGm/rCmnHd1BdqdJa++Nvf/iarkfLFq6++KqsR+UKufyJf2NjYoG/fvqqOhcEXSsdB5As5nZQvoqKiQESYMmVKh3pKLk9MnTpVVR1m6glrajdTT6jRWXpi27ZtshopT6SmpspqpDwRFhYm1Mjlia1bt6o+FgZPKB0HkSfkdHK5wsbGRlgvS9Wber0eWq0W/fr1U6yxDbWmnZ2dVXW5odbUarWqdEr15sMAT7wfcqydeM+bNw/+/v64dOmSqvbNzc0oLy9HQUEBUlJS4Obmhv/+97/C9lVVVXB3d0dxcbFxmzWP/gH3ko2zs7PiI+06nQ5hYWFm2xYtWoSnn35aVZyIiAg899xziu3S09Ph6+uL9PR0lJSUYMeOHXB1dVU1wa+oqMCoUaOMSS80NBTx8fHo169fh7a/lIl3S0sLoqKi8OSTT5o9riTSNDY2ory8HHl5eZg9ezZ69eqFK1euCHUFBQXw8PAw+8JDzcTbkvPnz5s9ZmSpqa6uBhHhxRdfNNNFRUXhhRdeUB0rKCgIiYmJZtukNOvXr0ffvn2RmZmJ4uJivPvuu+jSpYvxkT8pTUFBAYKDg43+GDduHCIjIzF+/HgA0ter0sRb6RoXTbyVdPX19Rg6dCjGjx+PlpYWRU1dXR3Kyspw6tQpREVFISQkxFg8SukOHjyIwMBA3Lx507jN9JipzV05OTnGR5ilNIbraunSpWa6J554AikpKapiNTU1wcXFBe+8845xm0iTmJiIoUOHIjs7G0VFRVi1ahVcXFxQUlIi1GRlZSEgIAAajQY2NjZ46aWXEBISgpdfflkyH8t54rXXXlPM4ZaeUJP3pfygpJPyRH19vaRGzg/WjEumfhDp5DyxePFixViWfpDrn5wf5HRSnhg4cCAcHByE462UL6qqqmBra4uZM2dKaqR8cebMGdXjuqkvzp8/r0pn6ouxY8fC1tbWrKA21Yh84eLiYlXdYfBFjx49hDopXxiOn+mSAlEsgy9SU1MVj4OUL7p27QpXV1dZnShXzJs3D4B5PaU0dpgiqsPklqWINFK5QkknN35YapTGDqX+GTDNFVIaNWOHUiypsUOkkcsVcjopT2g0Gnh4eJjpTOtlqXpTp9OhR48eZvWmqMY21Jq2traq63LTWlNtPa9Ubz4M8MT7IceaiffChQvh6+uLCxcu3He88PBwzJ07V/h+RkaGcRJheBGRMQEY7k4qMWTIkA6JzBI/Pz+zbzgB4P3334e3t7fi53///ffQarU4cOCAYltfX1/84x//MNu2Zs0aBAUFKWoNNDY2GhNaXFwcJkyY0KGN5bk0TCwtJ82jRo3CH//4R6HOFGsn3i0tLXj++ecxaNAg/PTTT6o0lgQGBpo9EWCp27hxo9EPph7RarXw9/e3Kpabmxs++OADSU1zczNsbW2xZs0aM82SJUswfPhwVfv15ZdfgohQVFRktt1S09TUBJ1O12E9/5w5czBu3DjFOHV1daitrQUADB06FAsWLBBer4biwLLw8fPzw4gRIxSvcanCSSk3NDQ0ICwsDOHh4cbix5p80tzcDEdHR3z88cdCXVJSktAX3t7eqmM1NjaCiBAVFSWpuXDhAogIO3fuNNseFxeH6dOnq9qvHTt2QKfTGc+ZSFNRUQEi83X8wL08OmDAAMU4V69eNZ4nDw8P/OUvf+nwOXPnzpX1xIYNGyQ1piit8bbUSPlBjc4UU09IaeT8MHr0aNVxDH44evSosH9KnlCKZekHURw5P/zhD38Q6kwx9YSLi4vseJudnd3hvBrGaK1WqzhGG3yxa9cuVeO6pS/upx7Yt2+fbP8SExMlfWF4qY1j8IVc/wzny9QXhn0y7YMolsEXH330kao4lr4YNGiQ6uMnlysM9ZQ1ecJUZ4pSrrDUqM0VcjWfKFcYNNbkCrk4olxh0FiTJ0SxlHKFQWNtrpCKZeoJrVbbYQ21ab0sVW/6+fnB09PTrN4U1diGWtPT01NVXW5Za95vPW9Zbz4M8BrvRwAAlJiYSBkZGXT8+HHq3bv3fX9We3u7cW2qFOHh4XTmzBkqKioyvoYMGULx8fFUVFRENjY2ijEaGxvp/Pnz5OXlJdtuxIgRHf4bnrKyMvL391eMkZaWRu7u7jRx4kTFtk1NTaTVml8qNjY2xl+TVYOTkxN5eXnRjRs3KCsriyZPnqyo6d27N3l6elJOTo5xW0NDA50+fVr1Gk1raG1tpbi4OCovL6fs7Gzq0aPHfX2OkkdmzJhBJSUlZh7x9vamxYsXU1ZWluo4P/zwA127dk3oEzs7OwoNDb1vjxARbdu2jZ566inFNUStra3U2tp63z5xcXGhnj17Unl5OX399dd06dIl4fX61FNPkU6nM/PFuXPnqKqqikpLS626xtXkhoaGBoqIiCA7OzvKzMwkvV5vdT4BQO3t7fThhx8KdSkpKR18QXTvOm9ra1Mdy/ArwF999ZWkplevXuTt7d3BF6WlpXTu3DlV+7Vt2zaaNGkSubm5yR4LwxpSU18AoLKyMqqsrFSM4+bmRt26daPjx49TbW0tTZo0yex9w7Um5YnS0lKqqqrqkCuUrk8pTDWWfrD8DQu1sXDvi/8O7xs0Ij9s3LiR0tLSVMcx6KTyhEEn8oRUrpCKZfBDz549Jftg0Ej5gUicJ6RimXqivr6eMjMzhePtkCFDOvjCsKZ+x44dqsfoUaNGKY7rUr64n3rgmWeeIb1eT6tXr5bULF++XNIXycnJdOTIEdVxDLpPPvlE2L+AgIAOvggPD6egoCCaM2eOYiyDL2JiYmSPg8gXbm5uNGXKFFXHT5QrTOspa/KE2jpMTqM2VyjFksoVphq1uUIpjlSuMNVYkydEseRyhanGmlwhimXqifb29g46035L1ZtDhw6lK1eumHlDqX4aPHiw4vGRqjXvt56/n/HsgfPg5vzM/XLz5k0UFhaisLAQRGRcv2r49WlL5s+fDxcXF5w8eRI1NTXGV1NTk2yclJQUnDp1CpWVlSgpKUFKSgo0Gg2++OILq/qr9MjXa6+9hpMnT6KyshK5ubkYO3Ys3NzchN8IGsjPz4etrS3eeustlJeXY/fu3XB0dMSuXbtkdW1tbfDz80NycrKq/ickJMDHxwefffYZKisr8emnn8LNza3Do1lSHD16FEeOHMGFCxfwxRdfIDg4GMOGDTM+bqV0LtetW4du3boZ1zZPnjwZvXv3xtWrV2V1165dQ2FhIQ4fPgwiwp49e5Cbm4tjx45JalpaWjBp0iT4+vqiqKjI6JGKigrk5+dLahobG7F06VLk5eXh+++/R0FBAWbNmgW9Xm/UqPWov78/1q1bJ9TcvHkTr7/+OvLy8lBZWYns7GyEhITg8ccfx+nTp4VxPv30U+h0OmzZsgXl5eV49913YWNjg6ysLMX+1dfXw9HREZs2bVJ1rkaPHo0BAwbgxIkTuHDhAtLS0oxrlkSaffv24cSJEzh//jwOHDgAf39/9O7dW/F6nTdvHvz8/HD8+HHjo/s2NjaympqaGhQWFmLr1q0gInz55ZeIjY2Fs7OzUFdfX49hw4bhiSeeQEVFBWpqapCQkABnZ2fk5ORIas6fP4+1a9eioKAAFy9eRG5uLqKioqDX62VjSUF0b42+SFNRUYE33ngDBQUFqKysxMGDB+Hs7Kx4LDZu3AhnZ2fs378f5eXlSE1NhY2NDbp27arYv/Lycmg0Ghw5ckQxt7a0tCAwMBC//e1vcfr0aVRUVCAsLAxEhHXr1gnjbN++HXl5eaioqMDOnTvh6uqK0NBQ2Xxs6YmwsDB4e3vLaqQ8MWvWLBw6dEhSI+UHw2vJkiXCWCJP2Nvb48CBA6rHGCJCTEyMMI6UHwICAjBq1CjF8UzkiY8//li2f6Z+AOTHTSk/vPPOO9BoNIiNjZXtn5QnXn311Q7HyHK8lfKF5SOdlhopXxQWFuLatWuSGjlfWN5tNtWJfOHq6mr2+KhSDUESTxOZauR8oXT8pHxhb29v9jiyVP8sfSEXR84Xlr8pYBnL0hf29vaIjY0V1lMiPyjVYVKemDFjBjIzMyU1cp7405/+JIwllysyMjJU14lEhMmTJwvjiDzh6+srexxEfpgzZ45iHWvpCbljLueJ6Oho2VhSuWL69OmK9bJlvWlYJrl69WqhxrLWfOutt2Bra4uUlBRJjajWPHLkiGz/5OpNy6cCfunwxPshxPCoj+UrISFBsr1UWyJCWlqabJzZs2fD398fdnZ26NmzJ8LDw62edAPKg+a0adPg5eUFOzs7+Pj4YNq0aR3+ixkRhw4dwsCBA6HX69GvXz9s2bJFUZOVlQUiQmlpqaoYDQ0NSEpKgp+fH+zt7REQEIDly5ejublZUbt3714EBATAzs4Onp6eWLhwIerq6ozvK53L9vZ2rFixAh4eHtDr9QgPD0dpaamiLi0tTXjepTSGx4Ss0dy+fRvR0dHw9vaGnZ0dvLy8MGnSJOTn51vtUX9/fyxcuFCoaWpqQkREBHr27AmdTgd/f3+8/PLL+Ne//qUYZ9u2bQgMDIS9vT2Cg4Nx4MABVf3bvHkzHBwcjOdLSVNTU4OZM2fC29sb9vb2CAoKwvz582U1f//73+Hr6wudTgc/Pz+kpqaqul5v376NBQsWoHv37nB0dFSlWblyperza9CJ9llOU11djcjISLi7u0On08HX1xfTp0+/rzykpKmqqsKoUaPg6uoKvV6PwMBA1XHefvtt+Pr6wtHR0TgZVqNbunQpHnvsMbS1tanSlJWVISYmBu7u7qrPVXJyMjw8PKDT6dCnTx/89a9/xaxZs2TzsaUnoqOj8eKLL8pqRJ7o0aOHpEbOD3FxccJYIk9MmTLFqjGGiBAeHi7USPlh8eLFqK+vVzWeWXpi4sSJihpTPwDK46alHwYNGoQdO3Yo6qQ8IfXfYFqOt1K+qKmpkdWIfGHqUVONnC8qKyuFsUS+OHfunGz/LCGSn3jL+ULp+AEdffHvf/9bUWPpC6U4Il8o6Sx9MXjwYNl6SuQHpTpM5Ilu3bpJauQ88dxzzwljiTwxYcIEq+pEIsLIkSOFGpEnYmJiFONI+UFNHWvpCSWNyBNKOlGuUKqXperNDz74QFYjqjXd3d0lNXK15tq1a4Wx5OrNhw0NABDDMAzDMAzDMAzDMJ0Cr/FmGIZhGIZhGIZhmE6EJ94MwzAMwzAMwzAM04nwxJthGIZhGIZhGIZhOhGeeDMMwzAMwzAMwzBMJ8ITb4ZhGIZhGIZhGIbpRHjizTAMwzAMwzAMwzCdCE+8GYZhGIZhGIZhGKYT4Yk3wzAMwzAMwzAMw3QiPPFmGIZhGKbT0Gg0dODAgQfdDYZhGIZ5oPDEm2EYhmF+pcycOZM0Gk2H1/jx4x901xiGYRjmkcL2QXeAYRiGYZjOY/z48ZSWlma2Ta/XP6DeMAzDMMyjCd/xZhiGYZhfMXq9njw9Pc1e3bt3J6J7j4Fv2rSJIiMjycHBgQICAuiTTz4x0585c4bGjBlDDg4O1KNHD5o7dy41Njaatdm+fTsNGDCA9Ho9eXl5UWJiotn7P/30E0VHR5OjoyP16dOHMjMzO3enGYZhGOYXBk+8GYZhGOYRZsWKFTRlyhQqLi6m+Ph4euGFF+js2bNERHTr1i0aN24cde/enb7++mvav38/ZWdnm02sN23aRAsXLqS5c+fSmTNnKDMzkwIDA81irF69muLi4qikpIQmTJhA8fHxdP369Z91PxmGYRjmQaIBgAfdCYZhGIZh/v+ZOXMm7dq1i+zt7c22L1u2jJYtW0YajYbmzZtHmzZtMr739NNPU0hICL3//vu0detWSk5OpkuXLpGTkxMREX3++ecUFRVFly9fJg8PD/Lx8aFZs2bRm2++KdkHjUZDqamptGbNGiK6N5nv0qULHTlyhNeaMwzDMI8MvMabYRiGYX7FPPPMM2YTayIiV1dX47/DwsLM3gsLC6OioiIiIjp79iwFBwcbJ91ERCNGjKD29nYqLS0ljUZDly9fpvDwcNk+DBo0yPhvJycncnZ2ptra2vvdJYZhGIZ56OCJN8MwDMP8inFycurw6Pf/Fw4ODqra6XQ6s781Gg21t7d3RpcYhmEY5hcJr/FmGIZhmEeYr776qsPf/fv3JyKi/v37U3FxMd26dcv4fm5uLmm1WgoKCqKuXbtSr169KCcn52ftM8MwDMM8bPAdb4ZhGIb5FdPc3Ew//vij2TZbW1tyc3MjIqL9+/fTkCFDaOTIkbR7927Kz8+nbdu2ERFRfHw8rVy5khISEmjVqlV09epVWrRoEc2YMYM8PDyIiGjVqlU0b948cnd3p8jISLp58ybl5ubSokWLft4dZRiGYZhfMDzxZhiGYZhfMUePHiUvLy+zbUFBQXTu3DkiuveL43v27KEFCxaQl5cXpaen029+8xsiInJ0dKSsrCxKSkqi0NBQcnR0pClTptCGDRuMn5WQkEB37tyhjRs30uuvv05ubm40derUn28HGYZhGOYhgH/VnGEYhmEeUTQaDWVkZNDzzz//oLvCMAzDML9qeI03wzAMwzAMwzAMw3QiPPFmGIZhGIZhGIZhmE6E13gzDMMwzCMKrzZjGIZhmJ8HvuPNMAzDMAzDMAzDMJ0IT7wZhmEYhmEYhmEYphPhiTfDMAzDMAzDMAzDdCI88WYYhmEYhmEYhmGYToQn3gzDMAzDMAzDMAzTifDEm2EYhmEYhmEYhmE6EZ54MwzDMAzDMAzDMEwnwhNvhmEYhmEYhmEYhulE/g83IOUj8yw+mQAAAABJRU5ErkJggg=="},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"try:\n    # Disable all GPUS\n    tf.config.set_visible_devices([], 'GPU')\n    visible_devices = tf.config.get_visible_devices()\n    for device in visible_devices:\n        assert device.device_type != 'GPU'\nexcept:\n    # Invalid device or cannot modify virtual devices once initialized.\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T02:21:37.360536Z","iopub.execute_input":"2024-11-26T02:21:37.361269Z","iopub.status.idle":"2024-11-26T02:21:37.365641Z","shell.execute_reply.started":"2024-11-26T02:21:37.361235Z","shell.execute_reply":"2024-11-26T02:21:37.364764Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"y_pred = model.predict(test_gen, steps=len(X_test) // batch_size)\ny_pred.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T02:26:55.801236Z","iopub.execute_input":"2024-11-26T02:26:55.802076Z","iopub.status.idle":"2024-11-26T02:26:56.078924Z","shell.execute_reply.started":"2024-11-26T02:26:55.802043Z","shell.execute_reply":"2024-11-26T02:26:56.078008Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m37/37\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(74, 32, 32, 16, 4)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"random_image = images[50]\nground_truth_mask = masks[50]\n\n# Add a batch dimension to the image (needed for model prediction)\nimage_batch = np.expand_dims(random_image, axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T03:15:05.852551Z","iopub.execute_input":"2024-11-26T03:15:05.853456Z","iopub.status.idle":"2024-11-26T03:15:05.857456Z","shell.execute_reply.started":"2024-11-26T03:15:05.853420Z","shell.execute_reply":"2024-11-26T03:15:05.856601Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"image_batch.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T03:15:16.971496Z","iopub.execute_input":"2024-11-26T03:15:16.972201Z","iopub.status.idle":"2024-11-26T03:15:16.977736Z","shell.execute_reply.started":"2024-11-26T03:15:16.972164Z","shell.execute_reply":"2024-11-26T03:15:16.976844Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"(1, 32, 32, 16, 4)"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"y_pred = model.predict(image_batch)\npredicted_mask = np.squeeze(y_pred, axis=0)\n# Convert probabilities to class labels\npredicted_mask = np.argmax(predicted_mask, axis=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T03:17:07.452039Z","iopub.execute_input":"2024-11-26T03:17:07.452393Z","iopub.status.idle":"2024-11-26T03:17:07.515834Z","shell.execute_reply.started":"2024-11-26T03:17:07.452364Z","shell.execute_reply":"2024-11-26T03:17:07.514919Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Select a specific slice to visualize (e.g., middle slice along z-axis)\nslice_index = random_image.shape[-2] // 2  # Middle slice along the z-axis\n\n# Plot the results\nfig, axes = plt.subplots(1, 2, figsize=(18, 6))\n\n# Display the ground truth mask (slice)\naxes[0].imshow(ground_truth_mask[:, :, 6], cmap=\"viridis\")\naxes[0].set_title(\"Ground Truth Mask (Slice)\")\naxes[0].axis(\"off\")\n\n# Display the predicted mask (slice)\naxes[1].imshow(predicted_mask[:, :, 6], cmap=\"viridis\")\naxes[1].set_title(\"Predicted Mask (Slice)\")\naxes[1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T03:19:37.860830Z","iopub.execute_input":"2024-11-26T03:19:37.861246Z","iopub.status.idle":"2024-11-26T03:19:38.204642Z","shell.execute_reply.started":"2024-11-26T03:19:37.861209Z","shell.execute_reply":"2024-11-26T03:19:38.203805Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1800x600 with 2 Axes>","image/png":"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"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}